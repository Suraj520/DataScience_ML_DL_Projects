{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### About\n",
    "Apache Spark using Python a.k.a PySpark\n",
    "> Basic Details\n",
    "1. PySpark helps in performing EDA while building machine learning pipelines.\n",
    "2. It helps in creating ETL pipelines.\n",
    "3. Big Data by definition means extremely large datasets that may be analyzed computationally to reveal patterns, trends.\n",
    "4. Big Data Analytics by definition means the process of examining large and varied datasets to uncover information including hidden patterns, market trends and customer preferences.\n",
    "5. Social media networks like Facebook who generated 4 peta byte data per day had to evolve this concept of BigData.\n",
    "6. The 5V's of Big Data - Velocity(Speed at which data is processed), Volume(Quantity of Data), Variety(Diff. Kind of data- Structured, Semi structured and unstructured), Veracity(Inconsistency of Data), Value(Previously unvaluable data to valuable data).\n",
    "7. Hadoop is a framework which is used to store Big Data a spectrum of devices. It is done to help one process big data in parallel.\n",
    "8. Three major components of a hadoop ecosystem are hdfs(hadoop distributed file system - Storage layer- Parallel), map reduce(processing layer - Parallel) and yarn(negotiator layer - yet another resource negotiator).\n",
    "9. Hadoop cluster consists of master and slave nodes. A map reduce submitted to master will automatically executed for slave nodes.\n",
    "10. In betweem the slaves and master, There is a resource negotiator i.e yarn. It keeps track of size etc resources on slaves. Suppose - Master requests yarn to give slaves that can divide 10 TB file into 5 Tb each and save. Yarn will return back the slave number 5, 7. It runs continuosly across slaves and check if they are functioning.\n",
    "11. Replicas of data blocks is maintained in master and slaves so that if one server goes down, Then also backup i.e fault tolerance is maintained.\n",
    "12. Map reduce is broken into 2 parts - Mapper function and reducer function. Hadoop will pass each line to mapper function and then the intermediate output will be read and aggregation(group by key) is done by reducer function.\n",
    "13. Master and slave nodes run daemons in background. Daemon can be referred as a computer program that runs a background process instead of being under direct control of an interactive user on a multitasking computer os.\n",
    "14. Data bricks is a platform where all requirements like hadoop, spark, hive come preinstalled over a n-node cluster.\n",
    "15. HDFs consist of Namenode(Runs on master, Keeps tracks of block) and Datanode(where actual data is stored).\n",
    "16. Map reduce has two major components ie. Map functions which converts one set of data into another where individual elements are broken down into key,value pairs whereas reduce function takes data from map function as input, aggregates and summarizes the results to yield the final output.\n",
    "17. Hadoop works inefficiently with small data, Only large data is preferred.\n",
    "18. Spark uses hadoop in two ways - One is storage and another is processing. Since spark has its own cluster management computation, it uses hadoop for storage purposes only !\n",
    "19. Apache Spark is a lightning fast cluster computing technology, designed for fast computation. It is based on Hadoop MapReduce and extends the mapreduce model to efficiently use it for more types of computations.\n",
    "20. Features of spark include in a) memory computation i.e no need to fetch data from disk every single time b) Fault tolerance - It implies very less data loss or nil c) Lazy evaluation i.e all transformations made in spark RDD involves creation of a new RDD.\n",
    "21.  All structured and unstructured data go into HDFS and input data is being fed to Spark, All intermediate output is saved in RAM and final output is written back to HDFS. Spark takes live data via Spark streaming whereas Hadoop map reduce doesn't have this functionality. It uses map reduce for optional processing and YARN for resource negotiator.\n",
    "22. Spark core is core layer for akl spark. Spark SQL is a distributed framework for structured data processing.\n",
    "23. Spark streaming is an add-on to core Spark APi which allows scalable, high throughput, fault tolerant stream processing of live data streams.\n",
    "24. Spark can access data from sources such as kafka, flume, kinesis or Tcp socket. The data can be visualised in live dashboards. Spark uses micro batching for Live streaming.\n",
    "25. Spark MLlib is a scalable machine learing library that contains sklearn, tensorflow etc. It's by default written in scala.\n",
    "26. Spark is a data analytics engine. It has read-eval-print-loop(repl) shell.\n",
    "27. Spark GraphX is library for manipulating graphs which provides analysis of graphs.\n",
    "28. Pyspark is an API written in Python to support Apache Spark.\n",
    "29. Pyspark comes to rescue in handling large dataset instead of pandas.\n",
    "30. Pyspark uses RDD(Resilient Distributed Database)\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Comparison of pandas with respect to pyspark.\n",
    "1. When dataset size increase beyond size of ram then pandas have to replaced with pyspark.\n",
    "2. pandas doesn't have parallelism like pyspark.\n",
    "3. Operations in pyspark are lazy but not in pyspark.\n",
    "4. In pyspark, we can't change the data since it uses RDD. We can transform the dataframe. In pandas, data frames are immutable hence are dynamic.\n",
    "5. dataframe access is slower in pyspark but processing is fast but in pandas's vice versa.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = sc.parallelize(['this','is','pyspark','demo','that','you','are','viewing']) # we can parallelise the data as list or tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this', 'is', 'pyspark', 'demo']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.take(4) # returns till 4 # we don't use collect method since it puts data into RAM and then it's of no use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to stop the spark context\n",
    "sc.stop()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To access GUI : Access localhost:4040\n",
    "> It stops when spark context is set to stop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('spark.driver.extraJavaOptions', '-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED'), ('spark.app.id', 'local-1678635364399'), ('spark.app.startTime', '1678635364329'), ('spark.driver.host', 'suraj'), ('spark.executor.id', 'driver'), ('spark.app.submitTime', '1678634749769'), ('spark.app.name', 'pyspark-shell'), ('spark.rdd.compress', 'True'), ('spark.executor.extraJavaOptions', '-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED'), ('spark.serializer.objectStreamReset', '100'), ('spark.driver.port', '33915'), ('spark.master', 'local[*]'), ('spark.submit.pyFiles', ''), ('spark.submit.deployMode', 'client'), ('spark.ui.showConsoleProgress', 'true')]\n"
     ]
    }
   ],
   "source": [
    "#second way to define a spark context\n",
    "sc = SparkContext()\n",
    "print(sc.getConf().getAll())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# third way to define a spark context\n",
    "# master and its name\n",
    "sc = SparkContext(\"local\",\"Master\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RDD\n",
    "1. RDD is like dataset\n",
    "2. RDD contains Transformation and Action. Transformation helps in creating a new RDD. Action gives us a value(Integer, String). Transformation is lazy, It creates Directed Acyclic Graph. Once action is applied, It executes.\n",
    "3. Transformation is divided into narrow and wide transformations.\n",
    "4. Various operations in Actions are Collect(), Count(), countByValue(), Take(), Top(), Reduce(), Fold(), Foreach(), saveAsTextFile() whereas various operations in Transformations are map(), flatmap(), filter(), distinct(), reduceByKey(), groupByKey(), mapValues(), flatMapValues(), sortByKey()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.RDD'>\n"
     ]
    }
   ],
   "source": [
    "# creating a rDD and their basic actions\n",
    "values = sc.parallelize([1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20])\n",
    "print(type(values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values.collect() # puts it to ram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {1: 1,\n",
       "             2: 1,\n",
       "             3: 1,\n",
       "             4: 1,\n",
       "             5: 1,\n",
       "             6: 1,\n",
       "             7: 1,\n",
       "             8: 1,\n",
       "             9: 1,\n",
       "             10: 1,\n",
       "             11: 1,\n",
       "             12: 1,\n",
       "             13: 1,\n",
       "             14: 1,\n",
       "             15: 1,\n",
       "             16: 1,\n",
       "             17: 1,\n",
       "             18: 1,\n",
       "             19: 1,\n",
       "             20: 1})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values.countByValue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "# for each\n",
    "def display_value(x):\n",
    "    print(x)\n",
    "a = values.foreach(lambda x: display_value(x))\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using glom - it transform each partition into a tupple. One tuple per parition. \n",
    "values.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using take over collect\n",
    "values.take(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.RDD'>\n"
     ]
    }
   ],
   "source": [
    "#creating multiple partitions\n",
    "values1 = sc.parallelize([1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20],21)\n",
    "print(type(values1)) # simple RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values1.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[],\n",
       " [1],\n",
       " [2],\n",
       " [3],\n",
       " [4],\n",
       " [5],\n",
       " [6],\n",
       " [7],\n",
       " [8],\n",
       " [9],\n",
       " [10],\n",
       " [11],\n",
       " [12],\n",
       " [13],\n",
       " [14],\n",
       " [15],\n",
       " [16],\n",
       " [17],\n",
       " [18],\n",
       " [19],\n",
       " [20]]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values1.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.PipelinedRDD"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pipelined RDD\n",
    "type(values1.glom())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "210"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reduce vs fold function - fold takes initial parameter\n",
    "values1.reduce(lambda a,b:a+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#max number\n",
    "values1.reduce(lambda x,y: x if x>y else y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-208"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#user define function\n",
    "def func(a,b):\n",
    "    return a -b\n",
    "\n",
    "values1.reduce(func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "212"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values.fold(1, lambda a,b : a+b) # adds 2 in 1st place, 4 in 2nd place\n",
    "#folds depend on the parallelise secondparam, too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values3 = sc.parallelize(range(1,20))\n",
    "values3.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading from text file\n",
    "firstnames = sc.textFile('first_names.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(firstnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Adfas'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "firstnames.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Adfas', 'fafsa', 'gaasf', 'fsafgasg', 'fsadsgasg']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "firstnames.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "firstnames.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sdbnbnhgfgsd',\n",
       " 'hnfdhgdhf',\n",
       " 'hbfgthnsdvbbdf',\n",
       " 'hbfdsdvhnsfdv',\n",
       " 'grthjtj',\n",
       " 'ghsfhrghsfvdhb']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "firstnames.top(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "firstnames.distinct().count()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Transformations\n",
    "It is a function that has I/O as RDD\n",
    "\n",
    "- Narrow transformation also known as pipelining where data are required to be in single note. \n",
    "- MAP, FLATMAP. MAP Partitions, Filter, Sample and Union are examples of this.\n",
    "- Wide transformation also known as shuffling where data may live in many partitions.\n",
    "- Intersection and Join, Distinct, Cartesian, Repartition, GroupByKey, ReduceByKey are examples of this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# multiply each row by 0.1\n",
    "values.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.1,\n",
       " 0.2,\n",
       " 0.30000000000000004,\n",
       " 0.4,\n",
       " 0.5,\n",
       " 0.6000000000000001,\n",
       " 0.7000000000000001,\n",
       " 0.8,\n",
       " 0.9,\n",
       " 1.0,\n",
       " 1.1,\n",
       " 1.2000000000000002,\n",
       " 1.3,\n",
       " 1.4000000000000001,\n",
       " 1.5,\n",
       " 1.6,\n",
       " 1.7000000000000002,\n",
       " 1.8,\n",
       " 1.9000000000000001,\n",
       " 2.0]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values.map(lambda a: a*0.1).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#flatmap for varying number of partittions\n",
    "values.flatMap(lambda x: range(1,x)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 10, 2, 4, 10]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values4 = sc.parallelize([1,2])\n",
    "values4.flatMap(lambda x:(x,x*2,10)).collect() # last element as 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Spark SQL\n",
    "- It can take data from CSV, Json etc and create SQL dataframe(relational database)\n",
    "- We can use SQL queries, too.\n",
    "- To create spark dataframe, We use spark session|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = sc.textFile('data.csv')\n",
    "#reading RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RecordNumber,Country,City,Zipcode,State', '1,US,PARC PARQUE,704,PR']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RecordNumber,Country,City,Zipcode,State',\n",
       " '1,US,PARC PARQUE,704,PR',\n",
       " '2,US,PASEO COSTA DEL SUR,704,PR',\n",
       " '10,US,BDA SAN LUIS,709,PR',\n",
       " '49347,US,HOLT,32564,FL',\n",
       " '49348,US,HOMOSASSA,34487,FL',\n",
       " '61391,US,CINGULAR WIRELESS,76166,TX',\n",
       " '61392,US,FORT WORTH,76177,TX',\n",
       " '61393,US,FT WORTH,76177,TX',\n",
       " '54356,US,SPRUCE PINE,35585,AL',\n",
       " '76511,US,ASH HILL,27007,NC',\n",
       " '4,US,URB EUGENE RICE,704,PR',\n",
       " '39827,US,MESA,85209,AZ',\n",
       " '39828,US,MESA,85210,AZ',\n",
       " '49345,US,HILLIARD,32046,FL',\n",
       " '49346,US,HOLDER,34445,FL',\n",
       " '3,US,SECT LANAUSSE,704,PR',\n",
       " '54354,US,SPRING GARDEN,36275,AL',\n",
       " '54355,US,SPRINGVILLE,35146,AL',\n",
       " '76512,US,ASHEBORO,27203,NC',\n",
       " '76513,US,ASHEBORO,27204,NC']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RecordNumber,Country,City,Zipcode,State',\n",
       " '76513,US,ASHEBORO,27204,NC',\n",
       " '76512,US,ASHEBORO,27203,NC',\n",
       " '76511,US,ASH HILL,27007,NC',\n",
       " '61393,US,FT WORTH,76177,TX']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe.top(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RecordNumber', 'Country', 'City', 'Zipcode', 'State']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = dataframe.first()\n",
    "cols = temp.split(',')\n",
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "sparksession = pyspark.sql.SparkSession.builder.master(\"local\").appName(\"BasicApp\").getOrCreate() # get or create for R/W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.session.SparkSession"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sparksession)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- RecordNumber: integer (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Zipcode: integer (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df= sparksession.read.csv('data.csv', header=True, inferSchema=True)\n",
    "#creating tree structre\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count() #count of the rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(RecordNumber=1, Country='US', City='PARC PARQUE', Zipcode=704, State='PR')"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------+-------------------+-------+-----+\n",
      "|RecordNumber|Country|               City|Zipcode|State|\n",
      "+------------+-------+-------------------+-------+-----+\n",
      "|           1|     US|        PARC PARQUE|    704|   PR|\n",
      "|           2|     US|PASEO COSTA DEL SUR|    704|   PR|\n",
      "|          10|     US|       BDA SAN LUIS|    709|   PR|\n",
      "|       49347|     US|               HOLT|  32564|   FL|\n",
      "|       49348|     US|          HOMOSASSA|  34487|   FL|\n",
      "|       61391|     US|  CINGULAR WIRELESS|  76166|   TX|\n",
      "|       61392|     US|         FORT WORTH|  76177|   TX|\n",
      "|       61393|     US|           FT WORTH|  76177|   TX|\n",
      "|       54356|     US|        SPRUCE PINE|  35585|   AL|\n",
      "|       76511|     US|           ASH HILL|  27007|   NC|\n",
      "+------------+-------+-------------------+-------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------+\n",
      "|RecordNumber|ZipCode|\n",
      "+------------+-------+\n",
      "|           1|    704|\n",
      "|           2|    704|\n",
      "|          10|    709|\n",
      "|       49347|  32564|\n",
      "|       49348|  34487|\n",
      "|       61391|  76166|\n",
      "|       61392|  76177|\n",
      "|       61393|  76177|\n",
      "|       54356|  35585|\n",
      "|       76511|  27007|\n",
      "+------------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#select statement from SQL\n",
    "df.select(\"RecordNumber\",\"ZipCode\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------+-----------------+-------+-----+\n",
      "|RecordNumber|Country|             City|Zipcode|State|\n",
      "+------------+-------+-----------------+-------+-----+\n",
      "|       49347|     US|             HOLT|  32564|   FL|\n",
      "|       49348|     US|        HOMOSASSA|  34487|   FL|\n",
      "|       61391|     US|CINGULAR WIRELESS|  76166|   TX|\n",
      "|       61392|     US|       FORT WORTH|  76177|   TX|\n",
      "|       61393|     US|         FT WORTH|  76177|   TX|\n",
      "|       54356|     US|      SPRUCE PINE|  35585|   AL|\n",
      "|       76511|     US|         ASH HILL|  27007|   NC|\n",
      "|       39827|     US|             MESA|  85209|   AZ|\n",
      "|       39828|     US|             MESA|  85210|   AZ|\n",
      "|       49345|     US|         HILLIARD|  32046|   FL|\n",
      "|       49346|     US|           HOLDER|  34445|   FL|\n",
      "|       54354|     US|    SPRING GARDEN|  36275|   AL|\n",
      "|       54355|     US|      SPRINGVILLE|  35146|   AL|\n",
      "|       76512|     US|         ASHEBORO|  27203|   NC|\n",
      "|       76513|     US|         ASHEBORO|  27204|   NC|\n",
      "+------------+-------+-----------------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Where equivalent filter\n",
    "df.filter(\"ZipCode >=25000\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|summary|Country|\n",
      "+-------+-------+\n",
      "|  count|     20|\n",
      "|   mean|   null|\n",
      "| stddev|   null|\n",
      "|    min|     US|\n",
      "|    max|     US|\n",
      "+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#describe like pandas dataframe\n",
    "df.describe(\"Country\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop duplicates like pandas datfarme\n",
    "new_df = df.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------+-------------------+-------+-----+\n",
      "|RecordNumber|Country|               City|Zipcode|State|\n",
      "+------------+-------+-------------------+-------+-----+\n",
      "|       76513|     US|           ASHEBORO|  27204|   NC|\n",
      "|       39827|     US|               MESA|  85209|   AZ|\n",
      "|       49347|     US|               HOLT|  32564|   FL|\n",
      "|           3|     US|      SECT LANAUSSE|    704|   PR|\n",
      "|           1|     US|        PARC PARQUE|    704|   PR|\n",
      "|       61391|     US|  CINGULAR WIRELESS|  76166|   TX|\n",
      "|       39828|     US|               MESA|  85210|   AZ|\n",
      "|       61392|     US|         FORT WORTH|  76177|   TX|\n",
      "|          10|     US|       BDA SAN LUIS|    709|   PR|\n",
      "|       49345|     US|           HILLIARD|  32046|   FL|\n",
      "|           4|     US|    URB EUGENE RICE|    704|   PR|\n",
      "|       49348|     US|          HOMOSASSA|  34487|   FL|\n",
      "|           2|     US|PASEO COSTA DEL SUR|    704|   PR|\n",
      "|       49346|     US|             HOLDER|  34445|   FL|\n",
      "|       54354|     US|      SPRING GARDEN|  36275|   AL|\n",
      "|       76512|     US|           ASHEBORO|  27203|   NC|\n",
      "|       61393|     US|           FT WORTH|  76177|   TX|\n",
      "|       54355|     US|        SPRINGVILLE|  35146|   AL|\n",
      "|       54356|     US|        SPRUCE PINE|  35585|   AL|\n",
      "|       76511|     US|           ASH HILL|  27007|   NC|\n",
      "+------------+-------+-------------------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dropping nulls\n",
    "df.dropna('any').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------+-------------------+-------+-----+\n",
      "|RecordNumber|Country|               City|Zipcode|State|\n",
      "+------------+-------+-------------------+-------+-----+\n",
      "|           1|     US|        PARC PARQUE|    704|   PR|\n",
      "|           2|     US|PASEO COSTA DEL SUR|    704|   PR|\n",
      "|          10|     US|       BDA SAN LUIS|    709|   PR|\n",
      "|       49347|     US|               HOLT|  32564|   FL|\n",
      "|       49348|     US|          HOMOSASSA|  34487|   FL|\n",
      "|       61391|     US|  CINGULAR WIRELESS|  76166|   TX|\n",
      "|       61392|     US|         FORT WORTH|  76177|   TX|\n",
      "|       61393|     US|           FT WORTH|  76177|   TX|\n",
      "|       54356|     US|        SPRUCE PINE|  35585|   AL|\n",
      "|       76511|     US|           ASH HILL|  27007|   NC|\n",
      "|           4|     US|    URB EUGENE RICE|    704|   PR|\n",
      "|       39827|     US|               MESA|  85209|   AZ|\n",
      "|       39828|     US|               MESA|  85210|   AZ|\n",
      "|       49345|     US|           HILLIARD|  32046|   FL|\n",
      "|       49346|     US|             HOLDER|  34445|   FL|\n",
      "|           3|     US|      SECT LANAUSSE|    704|   PR|\n",
      "|       54354|     US|      SPRING GARDEN|  36275|   AL|\n",
      "|       54355|     US|        SPRINGVILLE|  35146|   AL|\n",
      "|       76512|     US|           ASHEBORO|  27203|   NC|\n",
      "|       76513|     US|           ASHEBORO|  27204|   NC|\n",
      "+------------+-------+-------------------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------+\n",
      "|RecordNumber|   Name|\n",
      "+------------+-------+\n",
      "|           1|  fdsfd|\n",
      "|           2|  fsafs|\n",
      "|          10|    dgg|\n",
      "|       49347|  hdshd|\n",
      "|       49348| hdjhjj|\n",
      "|       61391|    gfj|\n",
      "|       61392|    hfg|\n",
      "|       61393|     fd|\n",
      "|       54356|     gg|\n",
      "|       76511|     kk|\n",
      "|           4|     jj|\n",
      "|       39827|   rhfh|\n",
      "|       39828|   jytg|\n",
      "|       49345|   jnnn|\n",
      "|       49346| mhgnhg|\n",
      "|           3|  gfdfh|\n",
      "|       54354|   jfgh|\n",
      "|       54355|   jgfj|\n",
      "|       76512|   jgfj|\n",
      "|       76513|jgfjgfj|\n",
      "+------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#reading second df\n",
    "df2= sparksession.read.csv('data1.csv', header=True, inferSchema=True)\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------+-------------------+-------+-----+------------+-------+\n",
      "|RecordNumber|Country|               City|Zipcode|State|RecordNumber|   Name|\n",
      "+------------+-------+-------------------+-------+-----+------------+-------+\n",
      "|           1|     US|        PARC PARQUE|    704|   PR|           1|  fdsfd|\n",
      "|           2|     US|PASEO COSTA DEL SUR|    704|   PR|           2|  fsafs|\n",
      "|          10|     US|       BDA SAN LUIS|    709|   PR|          10|    dgg|\n",
      "|       49347|     US|               HOLT|  32564|   FL|       49347|  hdshd|\n",
      "|       49348|     US|          HOMOSASSA|  34487|   FL|       49348| hdjhjj|\n",
      "|       61391|     US|  CINGULAR WIRELESS|  76166|   TX|       61391|    gfj|\n",
      "|       61392|     US|         FORT WORTH|  76177|   TX|       61392|    hfg|\n",
      "|       61393|     US|           FT WORTH|  76177|   TX|       61393|     fd|\n",
      "|       54356|     US|        SPRUCE PINE|  35585|   AL|       54356|     gg|\n",
      "|       76511|     US|           ASH HILL|  27007|   NC|       76511|     kk|\n",
      "|           4|     US|    URB EUGENE RICE|    704|   PR|           4|     jj|\n",
      "|       39827|     US|               MESA|  85209|   AZ|       39827|   rhfh|\n",
      "|       39828|     US|               MESA|  85210|   AZ|       39828|   jytg|\n",
      "|       49345|     US|           HILLIARD|  32046|   FL|       49345|   jnnn|\n",
      "|       49346|     US|             HOLDER|  34445|   FL|       49346| mhgnhg|\n",
      "|           3|     US|      SECT LANAUSSE|    704|   PR|           3|  gfdfh|\n",
      "|       54354|     US|      SPRING GARDEN|  36275|   AL|       54354|   jfgh|\n",
      "|       54355|     US|        SPRINGVILLE|  35146|   AL|       54355|   jgfj|\n",
      "|       76512|     US|           ASHEBORO|  27203|   NC|       76512|   jgfj|\n",
      "|       76513|     US|           ASHEBORO|  27204|   NC|       76513|jgfjgfj|\n",
      "+------------+-------+-------------------+-------+-----+------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# joining two csv datadset\n",
    "df.join(df2, df.RecordNumber == df2.RecordNumber).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
