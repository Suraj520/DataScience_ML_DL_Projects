{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### About\n",
    "\n",
    "> Stream processing\n",
    "\n",
    "Stream processing is a method of processing continuous streams of data in real time or near real time as they are generated. This involves processing and analyzing data while it is still in motion, rather than waiting for the data to be stored in a database or data warehouse. Data streams can come from a variety of sources, such as sensors, social media, or IoT devices, and contain large amounts of data that need to be processed quickly to gain insights, detect anomalies, or trigger alerts.\n",
    "\n",
    "Stream processing technology makes it possible to process these high-speed data streams in real-time by breaking the data into smaller chunks and processing each chunk as it is received. It enables businesses to make decisions and act in real-time based on insights gained from data streams.\n",
    "\n",
    "For example, stream processing can be used to monitor stock market data in real time, identify trading patterns, and send alerts to traders when certain conditions are met. It can also be used to analyze social media feeds to identify trends and sentiment around a particular product or event, or process IoT sensor data to identify equipment failures before they occur.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use PySpark's built-in streaming API, which allows us to read data from a variety of sources (such as Kafka, Flume, and HDFS), process it using Spark's RDD (Resilient Distributed Datasets) operations, and write the results to an output sink (such as HDFS, console, or a database)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.streaming import StreamingContext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark session init\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Stream Processing Example\") \\\n",
    "    .master(\"local[2]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "ssc = StreamingContext(spark.sparkContext, 1) # Batch interval of 1 second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting a stream of data by generating random numbers and converting them to a pyspark RDD\n",
    "import random\n",
    "\n",
    "def generate_random_data():\n",
    "    while True:\n",
    "        yield random.randint(1, 100)\n",
    "\n",
    "rdd = spark.sparkContext.parallelize(generate_random_data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a Dstream(Discretized Stream) by dividing the RDD into small batches and treating each batch as a separate RDD\n",
    "dstream = ssc.queueStream([rdd])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#performing some transformation on D stream\n",
    "filtered_dstream = dstream.filter(lambda num: num > 50)\n",
    "#filtering numbers less than 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_dstream.pprint()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#streaming context\n",
    "ssc.start()\n",
    "ssc.awaitTermination()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
