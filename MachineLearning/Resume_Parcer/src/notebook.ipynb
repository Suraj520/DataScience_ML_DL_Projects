{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for reading text from pdf document ie. CV\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.pdfinterp import PDFPageInterpreter\n",
    "from pdfminer.pdfinterp import PDFResourceManager\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "import io\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    with open(pdf_path, 'rb') as fh:\n",
    "        # iterate over all pages of PDF document\n",
    "        for page in PDFPage.get_pages(fh, caching=True, check_extractable=True):\n",
    "            # creating a resoure manager\n",
    "            resource_manager = PDFResourceManager()\n",
    "            \n",
    "            # create a file handle\n",
    "            fake_file_handle = io.StringIO()\n",
    "            \n",
    "            # creating a text converter object\n",
    "            converter = TextConverter(\n",
    "                                resource_manager, \n",
    "                                fake_file_handle, \n",
    "                                codec='utf-8', \n",
    "                                laparams=LAParams()\n",
    "                        )\n",
    "\n",
    "            # creating a page interpreter\n",
    "            page_interpreter = PDFPageInterpreter(\n",
    "                                resource_manager, \n",
    "                                converter\n",
    "                            )\n",
    "\n",
    "            # process current page\n",
    "            page_interpreter.process_page(page)\n",
    "            \n",
    "            # extract text\n",
    "            text = fake_file_handle.getvalue()\n",
    "            yield text\n",
    "\n",
    "            # close open handles\n",
    "            converter.close()\n",
    "            fake_file_handle.close()\n",
    "\n",
    "\n",
    "file_path_double_col = '/home/suraj/ClickUp/Jan-Feb/DataScience_ML_DL_Projects/MachineLearning/Resume_Parcer/data/CV/DoubleCol/Suraj.pdf'\n",
    "file_path_single_col = '/home/suraj/ClickUp/Jan-Feb/DataScience_ML_DL_Projects/MachineLearning/Resume_Parcer/data/CV/SingleCol/Suraj.pdf'\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Suraj\n",
      "\n",
      "Website | Google Scholar\n",
      "suraj.suraj.4.4.1996@gmail.com| +91-8486656592\n",
      "\n",
      "Last Updated on 20th December 2021\n",
      "\n",
      "EDUCATION\n",
      "IIIT GUWAHATI\n",
      "B.Tech in ECE. CPI: 7.32\n",
      "Grad. May 2018 | India\n",
      "LOYOLA HIGH SCHOOL\n",
      "Grad. May 2013| India\n",
      "\n",
      "LINKS\n",
      "Facebook:// suraj5200\n",
      "Github:// Suraj520\n",
      "LinkedIn:// suraj-690209218\n",
      "YouTube:// Suraj\n",
      "Twitter:// @suraj520__\n",
      "Quora:// Suraj-230\n",
      "\n",
      "COURSEWORK\n",
      "UNDERGRADUATE\n",
      "Computer Programming(C)\n",
      "Data Structres\n",
      "Operating Systems\n",
      "Shell Scripting\n",
      "Pattern Recognition & ML\n",
      "Linguistics\n",
      "Design of Internet of Things\n",
      "Consumer behaviour & Economics\n",
      "(Alongwith Academic Electronics Coursework)\n",
      "\n",
      "SKILLS\n",
      "PROGRAMMING\n",
      "Over 10000 lines:\n",
      "• Python • Shell • HTML • LATEX\n",
      "Over 1000 lines:\n",
      "•C • C++ • Java/Kotlin • C#\n",
      "Familiar:\n",
      "• Computer vision • Machine Learning\n",
      "• Natural language processing\n",
      "• Deep learning • Image Processing\n",
      "• Django • Flask • FastAPI\n",
      "SOFT SKILLS\n",
      "Project Management • Leadership\n",
      "\n",
      "CERTIFICATIONS\n",
      "COURSERA\n",
      "•Machine Learning with Python\n",
      "•Neural Networks & Deep Learning\n",
      "•Improving Deep Neural Networks\n",
      "•Algorithm Toolbox\n",
      "•TensorFlow Developer Specialization\n",
      "•Structuring ML Projects.\n",
      "EXTRA(S)\n",
      "•Employee Appreciation Certificate\n",
      "• Robotics Competition Certificates\n",
      "\n",
      "EXPERIENCE\n",
      "DIMENSION NXG - Deep Learning Engineer\n",
      "\n",
      "Jul 2021-Present | India\n",
      "\n",
      "• Dockerized Dense SLAM resources like ElasticFusion and CoFusion along with\n",
      "modifying their source code for inferencing on pre-recorded samples and Intel\n",
      "realsense D435i.\n",
      "\n",
      "• Worked towards multiway-registration, Dense RGB-D SLAM and incremental\n",
      "\n",
      "SFM pipelines from Open3D & OpenMVG.\n",
      "\n",
      "• Incorporated state machines within mediapipe hand pose estimation to convert\n",
      "discrete gestures to stable gesture states. Accuracy was enhanced by using\n",
      "majority filter and array blocking queues on detected gesture fragments.\n",
      "• Worked towards the training and ideation pipeline of single and dual stage\n",
      "\n",
      "objectron pipeline in Android.\n",
      "\n",
      "DEEP LEARNING ANALYTICS - SWE, ML Engineer\n",
      "\n",
      "May 2021-June 2021 | Canada\n",
      "\n",
      "• Worked towards NER based Factoid Extraction, Abstractive and Extractive\n",
      "\n",
      "summarization of webinar videos using Spacy, BERT, BART, Roberta,\n",
      "Longformer-4096 and Big-Bird.\n",
      "\n",
      "• Worked towards ball-change point detection in bird eye view of soccer matches\n",
      "\n",
      "using bayesian change point detection and pytorch-forecasting.\n",
      "\n",
      "DIMENSION NXG - A.I Team Lead | CV & DL Engineer\n",
      "June 2018 - Dec 2020 | Mumbai, India\n",
      "\n",
      "• Led team of 3-5 developers independently, managed important milestones in\n",
      "\n",
      "the project.\n",
      "\n",
      "• Converted trained/fine-tuned model to Intermediate representation along\n",
      "with integration to cross platform apps(Snapdragon DLC, Tflite etc) through\n",
      "optimization and quantization techniques over various levels.\n",
      "\n",
      "DEVATHON | Software Engineering Intern\n",
      "Oct 2017 - Dec 2017 | Hyderabad, India\n",
      "\n",
      "• Developed a conversational e-marketing chatbot through DialogFlow.\n",
      "\n",
      "PREDIBLE HEALTH | Deep Learning Intern\n",
      "May 2017-Jul.2017 | Bangalore, India\n",
      "\n",
      "• Contributed to the existing pipeline of ML/DL based hepatic lesion detection\n",
      "\n",
      "from CT Scan Images.\n",
      "\n",
      "PUBLICATIONS\n",
      "\n",
      "Visual machine intelligence for home automation Suraj; Ish Kool; Dharmendra\n",
      "Kumar and Shovan Barma at IEEE IOT-SIU 2018.\n",
      "\n",
      "Keystroke Rhythm Analysis Based on Dynamics of Fingertips Suraj, Parthana\n",
      "Sarma, Amit Kumar Yadav and Shovan Barma at Springer MISP 2017.\n",
      "\n",
      "OTORNoC: Optical tree of rings network on chip for 1000 core systems Soumyajit\n",
      "Poddar, Suraj, Amit Kumar Yadav and Hafizur Rahaman at IEEE ISED 2017.\n",
      "\n",
      "PROJECTS\n",
      "\n",
      "Cognitive Annotation Tool- An automatic annotation tool for labelling images in bulk\n",
      "with their corresponding bounding box annotations.\n",
      "\n",
      "JetBrains Academy Python Developer track- Various mini projects of easy, medium\n",
      "and hard difficulty.\n",
      "\n",
      "Train-Infer-MLOps- Machine learning and deep learning model implementations\n",
      "with various real-world use-cases.\n",
      "\n",
      "\f\n"
     ]
    }
   ],
   "source": [
    "resume_text_double_col = ''\n",
    "for page in extract_text_from_pdf(file_path_double_col):\n",
    "    resume_text_double_col += ' ' + page\n",
    "\n",
    "print(resume_text_double_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " SURAJ\n",
      "+91-8486656592 (cid:5) India\n",
      "\n",
      "suraj.suraj.4.4.1996@gmail.com (cid:5) linkedin.com/in/suraj-690209218/ (cid:5) suraj520.github.io\n",
      "\n",
      "EXPERIENCE\n",
      "\n",
      "Deep learning Engineer\n",
      "AjnaLens by Dimension NXG\n",
      "\n",
      "5 months 15 days\n",
      "Thane, Maharastra, India\n",
      "\n",
      "• Generated dockerﬁles for building Dense RGB-D SLAM resources like ElasticFusion, CoFusion and MaskFusion\n",
      "along with modifying their source code and dependencies for inferencing on pre-recorded aligned RGB-D samples\n",
      "and Intel realsense D435i.\n",
      "\n",
      "• Incorporated various methodologies like multiway-registration, Dense RGB-D SLAM and incremental SFM\n",
      "\n",
      "pipelines from Open3D and OpenMVG for experimental 3D scene understanding tasks.\n",
      "\n",
      "• Modiﬁed an existing baseline resource for training 2D object detector and 3D keypoint regressor modules for\n",
      "\n",
      "the single and double stage objectron pipeline.\n",
      "\n",
      "• Implemented various methods involving the usages of Majority-ﬁlter, Array blocking queues etc., to formulate\n",
      "relevant state-machine diagrams for robust hand-gesture recognition using mediapipe 3D handpose estimation\n",
      "module in Native-Android.\n",
      "\n",
      "SWE, ML Engineer\n",
      "Deep learning Analytics\n",
      "\n",
      "1 month 15 days\n",
      "Remote, Canada\n",
      "\n",
      "• Implemented NER based Factoid Extraction, Abstractive and Extractive summarization of webinar videos over\n",
      "\n",
      "baseline resources using Spacy, BERT, BART, Roberta, Longformer-4096 and BigBird.\n",
      "\n",
      "• Incorporated bayesian change point detection and deep learning algorithms using pytorch-forecasting on the task\n",
      "\n",
      "of ball-change point detection in data acquired from bird eye view of soccer matches.\n",
      "\n",
      "A.I Team Lead\n",
      "AjnaLens by Dimension NXG\n",
      "\n",
      "6 months\n",
      "Thane, Maharastra, India\n",
      "\n",
      "• Spearheaded the team towards pioneering 30 FPS+ for real-time face-mask detection on Native-Android platform\n",
      "using architectures like Tiny-yolo v3 & SSDLite. Kernel-level acceleration via Tensorﬂow Lite’s NNAPI, GPU\n",
      "delegate and Hexagon delegate improved the inference speed further without decreasing the accuracy.\n",
      "\n",
      "• Independently interviewed, hired, mentored and streamlined the team of 5-7 interns who went on to expedite\n",
      "the prototyping of various A.I modules listed in the planned product roadmap. Moreover, Reinforced and\n",
      "led the entire MLOps lifecycle of all company-wide forked A.I projects after the POC/MVP stage towards\n",
      "productization.\n",
      "\n",
      "Computer vision and Deep learning Engineer\n",
      "AjnaLens by Dimension NXG\n",
      "\n",
      "24 months\n",
      "Thane, Maharastra, India\n",
      "\n",
      "• Used Cognitive Annotation Tool to generate labeled dataset for pinch, bloom and ﬁst close gestures from\n",
      "ego-centric views which were then trained for the task of discrete hand-gesture detection via SVM-HOG, Tiny-\n",
      "Yolo v3 and RetinaNet Architectures. The trained models were optimized, quantized and pruned over various\n",
      "levels(FP32-INT8) before deploying them over various edge platforms like Android, Intel-Movidius/NCS2, Qual-\n",
      "comm 835 Dev kits etc for achieving inference speed upto 30 FPS. Moreover, Depth based thresholding on Time\n",
      "of ﬂight data was incorporated to steer the accuracy to the desired goal.\n",
      "\n",
      "• Architected the pipeline, trained ML/DL models and deployed cloud & edge based micro-services for various\n",
      "experimental/archived machine learning projects for its application in Augmented Reality along with devising\n",
      "techniques for optimisation, quantization and pruning which yielded a latency of less than 50 ms on an average.\n",
      "• SDKs/Packages Used: Tensorﬂow Sharp, Barracuda, Tﬂite Interpreter, Intel Openvino Toolkit, Qualcomm\n",
      "\n",
      "SNPE.\n",
      "\n",
      "\f Software Engineering Intern\n",
      "Devathon\n",
      "\n",
      "2 months\n",
      "Hyderabad,India\n",
      "\n",
      "• Implemented an experimental prototype of conversational e-marketing chatbot through DialogFlow and TF-IDF\n",
      "\n",
      "techniques for the engineering team.\n",
      "\n",
      "Deep Learning Intern\n",
      "Predible Health\n",
      "\n",
      "2 months\n",
      "Bengaluru,India\n",
      "\n",
      "• Implemented experimental Gaussian mixture models for hepatic segmentation LiTS Lung tumor data along with\n",
      "\n",
      "contributing to the data processing pipeline of existing 3D-UNet based segmentation architecture.\n",
      "\n",
      "PATENTS & PUBLICATIONS\n",
      "\n",
      "• Visual Machine Intelligence for Home Automation: Suraj, Ish Kool, Dharmendra Kumar and Shovan\n",
      "\n",
      "Barma in IEEE IOT-SIU 2018.\n",
      "\n",
      "• Keystroke Rhythm Analysis Based on Dynamics of Fingertips: Suraj, Parthana Sarma, Amit Yadav,\n",
      "\n",
      "Amit Yadav and Shovan Barma in Springer MISP 2017.\n",
      "\n",
      "• OTORNoC: Optical Tree Of Rings Network on Chip for 1000 Core Systems: Soumyajit Poddar,\n",
      "\n",
      "Suraj, Amit Yadav and Haﬁzur Rahaman in IEEE ISED 2017.\n",
      "\n",
      "EDUCATION\n",
      "\n",
      "B.Tech in Electronics and Communications, IIIT Guwahati\n",
      "Relevant Coursework: Pattern Recognition & ML, Data Structres, C Programming, OS , Shell Scripting.\n",
      "\n",
      "May 2018\n",
      "\n",
      "SKILLS\n",
      "\n",
      "Technical Skills\n",
      "Soft Skills\n",
      "Familiar\n",
      "\n",
      "Python,C++,Java, ML, DL, CV, NLP, Javascript.\n",
      "Leadership, Project Management, Data Engineering.\n",
      "OpenCV, Android Studio, Apache Spark, Kafka, Hadoop, MySQL, RESTAPIs, Dockers.\n",
      "\n",
      "PROJECTS\n",
      "\n",
      "Cognitive Annotation Tool: An automatic annotation tool for labelling images in bulk with their corresponding\n",
      "bounding box annotations.\n",
      "\n",
      "JetBrains Developer Academy Mini projects of varying diﬃculty\n",
      "\n",
      "Train-Infer-MLOps: Machine learning and deep learning model implementations with various real-world use-cases\n",
      "\n",
      "Asynchronous Speech to Gender Classiﬁcation ML Pipeline: Speech to Gender detection as a service.\n",
      "\n",
      "ACHIEVEMENTS\n",
      "\n",
      "• Employee Appreciation Certiﬁcate\n",
      "\n",
      "• Robotics Competition Certiﬁcates\n",
      "\n",
      "EXTRA-CURRICULAR ACTIVITIES\n",
      "\n",
      "• Meditating, Reading, Acapella singing, Exploring music across genres, hiking.\n",
      "\n",
      "\f\n"
     ]
    }
   ],
   "source": [
    "resume_text_single_col = ''\n",
    "for page in extract_text_from_pdf(file_path_single_col):\n",
    "    resume_text_single_col += ' ' + page\n",
    "\n",
    "print(resume_text_single_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "# load pre-trained model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# initialize matcher with a vocab\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "def extract_name(resume_text):\n",
    "    nlp_text = nlp(resume_text)\n",
    "    \n",
    "    # First name and Last name are always Proper Nouns\n",
    "    pattern = [{'POS': 'PROPN'}] # for Mononymous entities.\n",
    "    # First name and Last name are always Proper Nouns\n",
    "    #pattern = [{'POS': 'PROPN'}, {'POS': 'PROPN'}]\n",
    "    \n",
    "    \n",
    "    matcher.add('NAME', [pattern])\n",
    "    \n",
    "    matches = matcher(nlp_text)\n",
    "    \n",
    "    for match_id, start, end in matches:\n",
    "        span = nlp_text[start:end]\n",
    "        return span.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted names are  SURAJ\n"
     ]
    }
   ],
   "source": [
    "names = extract_name(resume_text_single_col)\n",
    "print(\"Extracted names are \",names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted names are  Suraj\n"
     ]
    }
   ],
   "source": [
    "names = extract_name(resume_text_double_col)\n",
    "print(\"Extracted names are \",names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_mobile_number(text):\n",
    "    phone = re.findall(re.compile(r'(?:(?:\\+?([1-9]|[0-9][0-9]|[0-9][0-9][0-9])\\s*(?:[.-]\\s*)?)?(?:\\(\\s*([2-9]1[02-9]|[2-9][02-8]1|[2-9][02-8][02-9])\\s*\\)|([0-9][1-9]|[0-9]1[02-9]|[2-9][02-8]1|[2-9][02-8][02-9]))\\s*(?:[.-]\\s*)?)?([2-9]1[02-9]|[2-9][02-9]1|[2-9][02-9]{2})\\s*(?:[.-]\\s*)?([0-9]{4})(?:\\s*(?:#|x\\.?|ext\\.?|extension)\\s*(\\d+))?'), text)\n",
    "    \n",
    "    if phone:\n",
    "        number = ''.join(phone[0])\n",
    "        if len(number) > 10:\n",
    "            return '+' + number\n",
    "        else:\n",
    "            return number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+91848665659\n"
     ]
    }
   ],
   "source": [
    "contact_number_1 = extract_mobile_number(resume_text_single_col)\n",
    "print(contact_number_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+91848665659\n"
     ]
    }
   ],
   "source": [
    "contact_number_2 = extract_mobile_number(resume_text_double_col)\n",
    "print(contact_number_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_email(email):\n",
    "    email = re.findall(\"([^@|\\s]+@[^@]+\\.[^@|\\s]+)\", email)\n",
    "    if email:\n",
    "        try:\n",
    "            return email[0].split()[0].strip(';')\n",
    "        except IndexError:\n",
    "            return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "suraj.suraj.4.4.1996@gmail.com|\n"
     ]
    }
   ],
   "source": [
    "email_1 = extract_email(resume_text_double_col)\n",
    "print(email_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "suraj.suraj.4.4.1996@gmail.com\n"
     ]
    }
   ],
   "source": [
    "email_2 = extract_email(resume_text_single_col)\n",
    "print(email_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "# load pre-trained model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def extract_skills(resume_text):\n",
    "    nlp_text = nlp(resume_text)\n",
    "\n",
    "    # removing stop words and implementing word tokenization\n",
    "    tokens = [token.text for token in nlp_text if not token.is_stop]\n",
    "    \n",
    "    # reading the csv file\n",
    "    data = pd.read_csv(\"/home/suraj/ClickUp/Jan-Feb/DataScience_ML_DL_Projects/MachineLearning/Resume_Parcer/data/CV/skills.csv\") \n",
    "    \n",
    "    # extract values\n",
    "    skills = list(data.columns.values)\n",
    "    \n",
    "    skillset = []\n",
    "    \n",
    "    # check for one-grams (example: python)\n",
    "    for token in tokens:\n",
    "        if token.lower() in skills:\n",
    "            skillset.append(token)\n",
    "    \n",
    "    # check for bi-grams and tri-grams (example: machine learning)\n",
    "    for token in nlp_text.noun_chunks:\n",
    "        token = token.text.lower().strip()\n",
    "        if token in skills:\n",
    "            skillset.append(token)\n",
    "    \n",
    "    return [i.capitalize() for i in set([i.lower() for i in skillset])]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C', 'Machine learning', 'Nlp', 'Ml', 'Computer vision', 'Android', 'Python']\n"
     ]
    }
   ],
   "source": [
    "print(extract_skills(resume_text_single_col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C', 'Machine learning', 'Nlp', 'Ml', 'Computer vision', 'Java', 'Flask', 'Django', 'Android', 'Python']\n"
     ]
    }
   ],
   "source": [
    "print(extract_skills(resume_text_double_col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# load pre-trained model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Grad all general stop words\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "# Education Degrees\n",
    "EDUCATION = [\n",
    "            'BE','B.E.', 'B.E', 'BS', 'B.S', \n",
    "            'ME', 'M.E', 'M.E.', 'MS', 'M.S', \n",
    "            'BTECH', 'B.TECH', 'M.TECH', 'MTECH', \n",
    "            'SSC', 'HSC', 'CBSE', 'ICSE', 'X', 'XII'\n",
    "        ]\n",
    "\n",
    "def extract_education(resume_text):\n",
    "    nlp_text = nlp(resume_text)\n",
    "\n",
    "    # Sentence Tokenizer\n",
    "    nlp_text = [sent.text.strip() for sent in nlp_text.sents]\n",
    "\n",
    "    edu = {}\n",
    "    # Extract education degree\n",
    "    for index, text in enumerate(nlp_text):\n",
    "        for tex in text.split():\n",
    "            # Replace all special symbols\n",
    "            tex = re.sub(r'[?|$|.|!|,]', r'', tex)\n",
    "            if tex.upper() in EDUCATION and tex not in STOPWORDS:\n",
    "                edu[tex] = text + nlp_text[index + 1]\n",
    "\n",
    "    # Extract year\n",
    "    education = []\n",
    "    for key in edu.keys():\n",
    "        year = re.search(re.compile(r'(((20|19)(\\d{2})))'), edu[key])\n",
    "        if year:\n",
    "            education.append((key, ''.join(year[0])))\n",
    "        else:\n",
    "            education.append(key)\n",
    "    return education"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ms', ('BTech', '2018')]\n"
     ]
    }
   ],
   "source": [
    "print(extract_education(resume_text_single_col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('BTech', '2018'), 'ms']\n"
     ]
    }
   ],
   "source": [
    "print(extract_education(resume_text_double_col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "50052c996937e9a0e161d422489677fdaadc23d756ac209b7397e80e5ea8cea0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
