{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About the dataset\n",
    "\n",
    "The dataset consists of 4 columns: product category (e.g. headsets, cell phones etc.), review title, review content and rating. The rating is a numerical type that can take one of the following value: 1, 2, 3, 4, 5. The value of 1 is the worst score, the value of 5 is the best score. The data is not cleaned. It need to be preprocessed for building models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_34200/2627288149.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mwordcloud\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import tensorflow\n",
    "import keras\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Lambda\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM, SimpleRNN, GRU\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from collections import defaultdict\n",
    "from keras.layers.convolutional import Convolution1D\n",
    "from keras import backend as K\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mThe Jupyter notebook server failed to launch in time. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv('/home/suraj/ClickUp/Jan-Feb/DataScience_ML_DL_Projects/MachineLearning/Ecommerce_Product_Review/sentiment_analysis/data/ebay_reviews.csv')\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's clean the dataset by following steps\n",
    "\n",
    "1. Bringing rating to -1,0 and 1 scale where 0 is neutral, posiitive being 4 and 5 and 1,2 being negative.\n",
    "\n",
    "2. Removing duplicate words, stop words, duplicate reviews and applying stemmer.\n",
    "\n",
    "Preprocessing reference - https://www.kaggle.com/code/wojtekbonicki/text-data-cleaning-using-user-defined-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['review_title', 'review_content', 'rating']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset transformations via https://scikit-learn.org/stable/data_transforms.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DuplicatesRemover(BaseEstimator, TransformerMixin):\n",
    "    #Transformer to remove duplicate reviews\n",
    "    def fit(self,X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self,X):\n",
    "        X2 = X.copy()\n",
    "        #indices of duplicated reviews\n",
    "        duplicate_idx = X2.duplicated()\n",
    "        X2= X2[~duplicate_idx].dropna()\n",
    "        return X2.set_index(np.arange(X2.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text cleaner\n",
    "class TextCleaner(BaseEstimator, TransformerMixin):\n",
    "    # Transformer to remove punctuation and multiple spaces from text and change uppercase to lowercase\n",
    "\n",
    "    def __init__(self,pattern=\"[!\\\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~]\"):\n",
    "        self.pattern= pattern\n",
    "    \n",
    "    def fit(self,X,y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self,X):\n",
    "        X2= X.copy()\n",
    "        X2.replace({\"\\s\\s+\":\" \"}, regex=False, inplace=True)\n",
    "\n",
    "        for col in X2.columns:\n",
    "            if X2.loc[:,col].dtypes == int: continue\n",
    "            X2.loc[:,col] = X2.loc[:,col].str.replace(self.pattern,\"\", regex=True).str.lower()\n",
    "        return X2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StopWordsRemover(BaseEstimator,TransformerMixin):\n",
    "    # Transformer to remove popular english words with some default exceptions. User can add his own words to keep.\n",
    "    # This is basically done to ensure keywords remain mostly in the reviews\n",
    "    def __init__(self,words_to_keep = ['few','not','off','all','any','not','no','very']):\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        self.eng_words = stop_words.difference(set(words_to_keep))\n",
    "\n",
    "    def fit(self,X,y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self,X):\n",
    "        X2 = X.copy()\n",
    "\n",
    "        for col in X2.columns:\n",
    "            if X2.loc[:, col].dtypes == int:continue\n",
    "            for en,review in enumerate(X2.loc[:,col].astype(str)):\n",
    "                new = (\" \").join(j for j in review.split(\" \") if j.lower() not in self.eng_words)\n",
    "                try:\n",
    "                    X2.loc[:,col].iloc[en] = new\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "\n",
    "        return X2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bring down keywords to their root by stemming\n",
    "class Stemmer(BaseEstimator,TransformerMixin):\n",
    "    # Transformer to stem words.\n",
    "    def __init__(self, stem=True):\n",
    "        self.stemmer = nltk.PorterStemmer()\n",
    "        self.stem = stem\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        if self.stem == False:\n",
    "            return X\n",
    "        else:\n",
    "            X2 = X.copy()  \n",
    "            for col in X2.columns:\n",
    "                if X2.loc[:, col].dtypes == int: continue\n",
    "                for en, review in enumerate(X2.loc[:, col].astype(str)):\n",
    "                    new = (\" \").join(self.stemmer.stem(j) for j in review.split(\" \"))\n",
    "                    try:\n",
    "                        X2[:, col].iloc[en] = new\n",
    "                    except:\n",
    "                        continue\n",
    "            return X2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rating(BaseEstimator, TransformerMixin):\n",
    "    # Transformer to change reviews rating number to positive, negative and neutral\n",
    "    # -1 for negative, 0 for neutral and 1 for positive\n",
    "\n",
    "    def __init__(self, scale={1:-1, 2:-1, 3:0, 4:1, 5:1}, labels_to_del=[]):\n",
    "        self.scale = scale\n",
    "        self.labels_to_del = labels_to_del\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        if self.labels_to_del != []:\n",
    "            self.idx_to_del = X['rating'] == self.labels_to_del[0]\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):    \n",
    "        X2 = X.copy()\n",
    "        if self.labels_to_del:\n",
    "            X2 = X2[~self.idx_to_del]\n",
    "        X2.replace(self.scale, inplace=True)\n",
    "        return X2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['review title', 'review content', 'rating']\n",
    "\n",
    "#pipeline\n",
    "preprocessor = Pipeline([\n",
    "    #at first duplicated reviews will be removed\n",
    "    ('DuplicateRemover', DuplicatesRemover()),\n",
    "    #symbols that will be removed are defined in the transformer but a user can define his own/some additional symbols\n",
    "    ('TextCleaning',TextCleaner()),\n",
    "    #removing popular english words\n",
    "    ('StopWordsRemover',StopWordsRemover()),\n",
    "    #if stem is False the words will not be stemmed\n",
    "    ('Stemmer', Stemmer(stem=False)),\n",
    "    #rating changer, in this example negative(1, 2) ratings are equal to -1, neutral (3) 0 and positive(4,5) 1\n",
    "    ('Rating', Rating(scale={1:-1, 2:-1, 3:0, 4:1, 5:1})),\n",
    "    #the autor noticed that after cleaning the reviews some duplicated reviews are left, one more time duplicateremover is used (we could use it only one time, but it would make the process of data cleaning longer)\n",
    "    ('DuplicateRemover2', DuplicatesRemover())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaned reviews are available after running this cell (note: it may take a while)\n",
    "preprocessor.fit(raw_data[cols])\n",
    "data_preprocessed = preprocessor.transform(raw_data[cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_preprocessed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Approach 1\n",
    "Weighted sum basic approach\n",
    "\n",
    "* remove stop words, bring back each word via stemmer and check net-rating of the sentence.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a list of positive, negative and neutral keywords\n",
    "with open('/home/suraj/ClickUp/Jan-Feb/DataScience_ML_DL_Projects/MachineLearning/Ecommerce_Product_Review/sentiment_analysis/data/positive-words.txt', 'r') as f:\n",
    "    positive_words = [line.strip() for line in f]\n",
    "with open('/home/suraj/ClickUp/Jan-Feb/DataScience_ML_DL_Projects/MachineLearning/Ecommerce_Product_Review/sentiment_analysis/data/negative-words.txt', 'r') as f:\n",
    "    negative_words = [line.strip() for line in f]\n",
    "with open('/home/suraj/ClickUp/Jan-Feb/DataScience_ML_DL_Projects/MachineLearning/Ecommerce_Product_Review/sentiment_analysis/data/neutral-words.txt', 'r') as f:\n",
    "    neutral_words = [line.strip() for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(positive_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(negative_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(neutral_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking a random review and calculating its sentiment\n",
    "# review taken from - https://www.amazon.in/Top-Examples-Use-Essay-Evidence/product-reviews/1479248738\n",
    "review = \"\"\"This is a useful book for SAT practice. My daughter was able to increase her SAT score significantly by understanding how the SATs are reviewed and graded. It's not a short cut but it does help practice the test and allow you to focus on what's important. It is beyond my understanding why the school system in our area doesn't use such a tool rather than leaving it to the kids to have to navigate this area on their own. This is not cheating, it simply explains what the examiners are looking for.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a function that can preprocess a paragraph of review by cleaning,removing stop words, stemming etc.\n",
    "# Steps >>\n",
    "# 1. Duplicate remover\n",
    "# 2. Text cleaning\n",
    "# 3. Stop word remover\n",
    "# 4. Stemmer and lemmatizer\n",
    "from collections import Counter\n",
    "import re\n",
    "import nltk.corpus\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# This is a helper function to map NTLK position tags\n",
    "# Full list is available here: https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "#function to remove duplicates\n",
    "def remove_duplicate(review):\n",
    "    review = review.split(\" \")\n",
    "    #Unique words\n",
    "    unique_words = Counter(review)\n",
    "    # joining two adjacent element in iterable way\n",
    "    s = \" \".join(unique_words.keys())\n",
    "    return s\n",
    "\n",
    "#function to clean text\n",
    "def clean_text(review):\n",
    "    #converting to lowercase\n",
    "    text = review.lower()\n",
    "    # removing unicode characters.\n",
    "    text = re.sub(r\"(@\\[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\", text)\n",
    "    #removing stopwords\n",
    "    stop = stopwords.words('english')\n",
    "    text = \" \".join([word for word in text.split() if word not in (stop)])\n",
    "    # stemming and lemmatization\n",
    "    snow = SnowballStemmer('english')\n",
    "    stemmed_sentence = []\n",
    "    # Word Tokenizer\n",
    "    words = word_tokenize(text)\n",
    "    for w in words:\n",
    "        #Apply stemming\n",
    "        stemmed_sentence.append(snow.stem(w))\n",
    "    stemmed_text = \" \".join(stemmed_sentence)\n",
    "    #lemmatization\n",
    "    # Initialize the lemmatizer\n",
    "    wl = WordNetLemmatizer()\n",
    "    lemmatized_sentence = []\n",
    "\n",
    "    words = word_tokenize(stemmed_text)\n",
    "    word_pos_tags = nltk.pos_tag(words)\n",
    "    # Get position tags\n",
    "    word_pos_tags = nltk.pos_tag(words)\n",
    "    # Map the position tag and lemmatize the word/token\n",
    "    for idx, tag in enumerate(word_pos_tags):\n",
    "        lemmatized_sentence.append(wl.lemmatize(tag[0], get_wordnet_pos(tag[1])))\n",
    "    lemmatized_text = \" \".join(lemmatized_sentence)\n",
    "    return lemmatized_text\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating predict_function\n",
    "def predict_sentiment(review):\n",
    "    #removing duplicate\n",
    "    review=remove_duplicate(review)\n",
    "    review= clean_text(review)\n",
    "    total_review_rating = []\n",
    "    words = review.split()\n",
    "    for word in words:\n",
    "        if word in negative_words:\n",
    "            total_review_rating.append(-1)\n",
    "        elif word in positive_words:\n",
    "            total_review_rating.append(1)\n",
    "        elif word in neutral_words:\n",
    "            total_review_rating.append(0)\n",
    "    print(total_review_rating)\n",
    "    net_rating = sum(total_review_rating)\n",
    "    if net_rating ==0:\n",
    "        return \"neutral\"\n",
    "    elif net_rating >0:\n",
    "        return \"positive\"\n",
    "    else:\n",
    "        return \"negative\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict\n",
    "print(predict_sentiment(review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# review linkk - https://www.amazon.in/gp/customer-reviews/RIKJKSB3IR6LP?ASIN=B0006U7FC0review2 = \"\"\"This product stopped working after just used for 3-4 times.\n",
    "review2= \"\"\"It is really worthless to buy such a costly product to waste money.\n",
    "Neither seller is helping nor Amazon is helping me to get it services. I am not able to find a service center in Hyderabad.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predict_sentiment(review2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# link - https://www.amazon.in/product-reviews/B01F8YGEXE\n",
    "review3 = \"\"\"Liked the product. Cushioning is good. Some what small. But then it is 3ft by 6ft. Which is good for 1 person to sleep.\n",
    "It is very comfortable. Has cotton cover. But its not waterproof. Hence better to put waterproof sheet over if using for an infant.\n",
    "Overall good product and recommended.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predict_sentiment(review3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion of first method \n",
    "First method of depicting sentiment is done and the results are subject to keywords in the three files(positive-words.txt, negative-words.txt and neutral-words.txt) which right now look satisfactory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Count Vectorizer with Multinomial Naive Bayes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing any neutral ratings equal to 0 from the database\n",
    "df = data_preprocessed[data_preprocessed['rating']!=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "# splitting the data into training set and validation\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['review content'], df['rating'], \\\n",
    "                                                    test_size=0.1, random_state=0)\n",
    "\n",
    "print('Load %d training examples and %d validation examples. \\n' %(X_train.shape[0],X_test.shape[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train MultinomialNB classifier\n",
    "\n",
    "\n",
    "countVect = CountVectorizer() \n",
    "X_train_countVect = countVect.fit_transform(X_train)\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(X_train_countVect, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelEvaluation(predictions):\n",
    "    '''\n",
    "    Print model evaluation to predicted result \n",
    "    '''\n",
    "    print (\"\\nAccuracy on validation set: {:.4f}\".format(accuracy_score(y_test, predictions)))\n",
    "    print(\"\\nAUC score : {:.4f}\".format(roc_auc_score(y_test, predictions)))\n",
    "    print(\"\\nClassification report : \\n\", metrics.classification_report(y_test, predictions))\n",
    "    print(\"\\nConfusion Matrix : \\n\", metrics.confusion_matrix(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on validaton set\n",
    "predictions = mnb.predict(countVect.transform(X_test))\n",
    "modelEvaluation(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    "Results of Count Vect with Multinomial naive bayes looks good!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. TFIDF vectorizer with Logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(min_df=5) #minimum document frequency of 5\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "print(\"Number of features : %d \\n\" %len(tfidf.get_feature_names())) #1722\n",
    "print(\"Show some feature names : \\n\", tfidf.get_feature_names()[::1000])\n",
    "\n",
    "# Logistic Regression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the top 10 features with smallest and the largest coefficients\n",
    "feature_names = np.array(tfidf.get_feature_names())\n",
    "sorted_coef_index = lr.coef_[0].argsort()\n",
    "print('\\nTop 10 features with smallest coefficients :\\n{}\\n'.format(feature_names[sorted_coef_index[:10]]))\n",
    "print('Top 10 features with largest coefficients : \\n{}'.format(feature_names[sorted_coef_index[:-11:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on the validaton set\n",
    "predictions = lr.predict(tfidf.transform(X_test))\n",
    "modelEvaluation(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Results of TFIDF with Logistic regression looks much better.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Using LSTM.\n",
    "\n",
    "In this, we will use the raw_data which is not cleaned to train LSTM models and see how they perform with respect to the machine learning models created with hand crafted features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping all 3 ratings from raw_data at first\n",
    "df = raw_data[raw_data['rating']!=3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting the ratings to boolean now via transformers pipeline\n",
    "cols = ['review title', 'review content', 'rating']\n",
    "\n",
    "#pipeline\n",
    "preprocessor = Pipeline([\n",
    "    #at first duplicated reviews will be removed\n",
    "    #('DuplicateRemover', DuplicatesRemover()),\n",
    "    #symbols that will be removed are defined in the transformer but a user can define his own/some additional symbols\n",
    "    #('TextCleaning',TextCleaner()),\n",
    "    #removing popular english words\n",
    "    #('StopWordsRemover',StopWordsRemover()),\n",
    "    #if stem is False the words will not be stemmed\n",
    "    #('Stemmer', Stemmer(stem=False)),\n",
    "    #rating changer, in this example negative(1, 2) ratings are equal to -1, neutral (3) 0 and positive(4,5) 1\n",
    "    ('Rating', Rating(scale={1:-1, 2:-1, 3:0, 4:1, 5:1})),\n",
    "    #('DuplicateRemover2', DuplicatesRemover())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaned reviews are available after running this cell (note: it may take a while)\n",
    "preprocessor.fit(df[cols])\n",
    "lstm_data = preprocessor.transform(df[cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(lstm_data['review content'], lstm_data['rating'], \\\n",
    "                                                    test_size=0.1, random_state=0)\n",
    "\n",
    "print('Load %d training examples and %d validation examples. \\n' %(X_train.shape[0],X_test.shape[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words = 20000 \n",
    "maxlen = 100 \n",
    "batch_size = 32\n",
    "nb_classes = 2\n",
    "nb_epoch = 3\n",
    "\n",
    "\n",
    "# Vectorize X_train and X_test to 2D tensor\n",
    "tokenizer = Tokenizer(nb_words=top_words) #only consider top 20000 words in the corpse\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "# tokenizer.word_index #access word-to-index dictionary of trained tokenizer\n",
    "\n",
    "sequences_train = tokenizer.texts_to_sequences(X_train)\n",
    "sequences_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "X_train_seq = sequence.pad_sequences(sequences_train, maxlen=maxlen)\n",
    "X_test_seq = sequence.pad_sequences(sequences_test, maxlen=maxlen)\n",
    "\n",
    "\n",
    "# one-hot encoding of y_train and y_test\n",
    "y_train_seq = np_utils.to_categorical(y_train, nb_classes)\n",
    "y_test_seq = np_utils.to_categorical(y_test, nb_classes)\n",
    "\n",
    "print('X_train shape:', X_train_seq.shape) #(27799, 100)\n",
    "print('X_test shape:', X_test_seq.shape) #(3089, 100)\n",
    "print('y_train shape:', y_train_seq.shape) #(27799, 2)\n",
    "print('y_test shape:', y_test_seq.shape) #(3089, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "50052c996937e9a0e161d422489677fdaadc23d756ac209b7397e80e5ea8cea0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
