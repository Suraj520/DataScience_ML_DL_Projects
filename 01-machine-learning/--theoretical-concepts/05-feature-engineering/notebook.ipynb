{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### About\n",
    "\n",
    "> Feature Engineering\n",
    "\n",
    "Feature construction is the process of selecting and transforming variables from a data set to create new variables that better represent the underlying problem of a predictive model. This process is an important step in machine learning because it can significantly affect a model's ability to make accurate predictions. The available methods are:\n",
    "\n",
    "1. Feature selection: Select the most relevant subset of features from the original data set. The goal is to reduce the dimensionality of the dataset while preserving the most relevant features. There are several feature selection methods, including correlation-based methods, mutual information-based methods, and tree-based methods. \n",
    "2. Feature Scaling: This involves converting feature values ​​to a common scale. This is important because many machine learning algorithms are sensitive to the scale of the input function. There are many ways to scale a function, including standardization, normalization, and min-max scaling. \n",
    "3. Feature Extraction: It involves creating new features from existing features. This can be done using techniques such as principal component analysis (PCA), independent component analysis (ICA) and linear discriminant analysis (LDA).\n",
    "4. Encoding categorical variables: Many datasets contain categorical variables that need to be encoded before they can be used in machine learning algorithms. There are several ways to code categorical variables, including single-rate coding, label coding, and target coding. \n",
    "5. Imputation of missing values: Many data sets have missing values ​​that must be accounted for before they can be used in machine learning algorithms. There are several methods for imputing missing values, including mean imputation, median imputation, and K-Nearest Neighbor imputation.\n",
    "6. Feature Composition: Create new features by combining existing features. This can be done using techniques such as polynomial feature generation, interaction feature generation, and transformation feature generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "#feature scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Standardize features\n",
    "def standardize_features(X):\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    return X_scaled\n",
    "\n",
    "\n",
    "X = [[13242,3564,3432652,6532,1345.0]]\n",
    "\n",
    "X_scaled = standardize_features(X)\n",
    "print(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
