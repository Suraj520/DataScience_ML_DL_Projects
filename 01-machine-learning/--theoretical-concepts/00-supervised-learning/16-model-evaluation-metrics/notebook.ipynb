{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### About\n",
    "\n",
    "> Model Evaluation metrics\n",
    "\n",
    "Model evaluation metrics are used to assess the performance and effectiveness of supervised machine learning models. These metrics provide quantitative measures to evaluate how well a trained model is performing in terms of its predictions or classifications. Following are the commonly used model evaluation metrics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Accuracy: Accuracy measures the proportion of correctly predicted instances out of the total instances. It is often used as a basic metric for classification problems when classes are balanced.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.625\n"
     ]
    }
   ],
   "source": [
    "y_true = [1, 0, 1, 1, 0, 1, 0, 1]\n",
    "y_pred = [1, 0, 1, 0, 1, 1, 0, 0]\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Precision: Precision measures the proportion of true positive predictions out of the total positive predictions. It is often used in binary classification problems when the focus is on minimizing false positives.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.75\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "\n",
    "y_true = [1, 0, 1, 1, 0, 1, 0, 1]\n",
    "y_pred = [1, 0, 1, 0, 1, 1, 0, 0]\n",
    "precision = precision_score(y_true, y_pred)\n",
    "print(\"Precision:\", precision)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Recall (Sensitivity or True Positive Rate): Recall measures the proportion of true positive predictions out of the total actual positive instances. It is often used in binary classification problems when the focus is on minimizing false negatives.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.6\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "\n",
    "y_true = [1, 0, 1, 1, 0, 1, 0, 1]\n",
    "y_pred = [1, 0, 1, 0, 1, 1, 0, 0]\n",
    "recall = recall_score(y_true, y_pred)\n",
    "print(\"Recall:\", recall)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. F1 Score: The F1 score is the harmonic mean of precision and recall, and provides a balanced measure of both precision and recall.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.6666666666666665\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "y_true = [1, 0, 1, 1, 0, 1, 0, 1]\n",
    "y_pred = [1, 0, 1, 0, 1, 1, 0, 0]\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "print(\"F1 Score:\", f1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Specificity (True Negative Rate): Specificity measures the proportion of true negative predictions out of the total actual negative instances. It is often used in binary classification problems when the focus is on minimizing false positives.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Area Under the Receiver Operating Characteristic (ROC) Curve: The ROC curve is a graphical representation of the true positive rate (TPR) against the false positive rate (FPR) at different classification thresholds. \n",
    "\n",
    "The Area Under the ROC Curve (AUC-ROC) is a common metric used to assess the performance of binary classification models, where a higher AUC-ROC value indicates better model performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC-ROC: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "y_true = [1, 0, 1, 1, 0, 1, 0, 1]\n",
    "y_pred_probs = [0.9, 0.3, 0.8, 0.7, 0.1, 0.6, 0.2, 0.8]\n",
    "roc_auc = roc_auc_score(y_true,y_pred_probs)\n",
    "print(\"AUC-ROC:\", roc_auc)\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Confusion Matrix: A confusion matrix is a table that is often used to describe the performance of a classification model on a set of data for which the true values are known. It provides a breakdown of the number of true positive, true negative, false positive, and false negative predictions made by the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[2 1]\n",
      " [2 3]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_true = [1, 0, 1, 1, 0, 1, 0, 1]\n",
    "y_pred = [1, 0, 1, 0, 1, 1, 0, 0]\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Use cases for model evaluation metrics in supervised learning:\n",
    "\n",
    "1. Classification problems: Metrics such as accuracy, precision, recall, F1 score, and AUC-ROC are commonly used to evaluate the performance of classification models in binary or multi-class classification problems.\n",
    "\n",
    "2. Imbalanced datasets: In cases where the dataset is imbalanced, i.e., one class is significantly more dominant than the other(s), metrics such as precision, recall, and specificity may be more relevant, as accuracy may not accurately reflect the model's performance.\n",
    "\n",
    "3. Threshold tuning: Some classification models, such as logistic regression and support vector machines, use a threshold to make predictions. The threshold can be tuned to achieve a trade-off between precision and recall based on the problem requirements, and the model evaluation metrics can help in selecting the optimal threshold.\n",
    "\n",
    "4. Model comparison: Model evaluation metrics can be used to compare the performance of different supervised learning models and select the best-performing model for a specific problem.\n",
    "\n",
    "5. Model monitoring: Model evaluation metrics can be used to monitor the performance of a trained model in production, and trigger alerts or actions based on the performance thresholds defined for the problem at hand.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
