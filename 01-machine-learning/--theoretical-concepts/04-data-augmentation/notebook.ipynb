{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### About\n",
    "\n",
    "> Data Augmentation\n",
    "\n",
    "Data augmentation is a technique used in machine learning to artificially increase the size of a training data set by creating new synthetic data samples from existing data. The purpose of data augmentation is to improve the performance of the model by giving it more data to learn from and making the model more robust to changes and noise in the input data. There are different ways to scale data depending on the type of data used. Some commonly used methods include:\n",
    "\n",
    "1. Enlarging Images: This includes applying various image transformations such as transform, rotate, resize, crop, change brightness and contrast. This allows the model to learn to recognize objects in different lighting conditions, angles and orientations.\n",
    "\n",
    "2. Augmentation text: This includes replacing words with synonyms, shuffling word order, and adding or removing words from the original text to create new text patterns. This helps the model improve its ability to understand language variations and generalize to new text patterns. \n",
    "\n",
    "3. Sound Enhancement: Adjust audio files by changing speed, volume and pitch, adding background noise and applying filters. This allows the model to learn to recognize speech and other audio signals in a variety of noise and environmental conditions.\n",
    "\n",
    "Data augmentation is a powerful technique that can significantly improve the performance of machine learning models, especially when the amount of training data is limited. It is important to choose the right data augmentation method based on the nature of your data and the requirements of your model.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(n_samples=100, n_features=4, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_noisy = X + np.random.normal(scale=0.1, size=X.shape)\n",
    "# add noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly flip the labels of some samples\n",
    "idx = np.random.choice(len(y), size=10)\n",
    "y_flip = y.copy()\n",
    "y_flip[idx] = 1 - y_flip[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the original dataset with the augmented dataset\n",
    "X_augmented = np.vstack([X, X_noisy])\n",
    "y_augmented = np.hstack([y, y_flip])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset: (100, 4) (100,)\n",
      "Augmented dataset: (200, 4) (200,)\n"
     ]
    }
   ],
   "source": [
    "# Print the shapes of the original and augmented datasets\n",
    "print('Original dataset:', X.shape, y.shape)\n",
    "print('Augmented dataset:', X_augmented.shape, y_augmented.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
