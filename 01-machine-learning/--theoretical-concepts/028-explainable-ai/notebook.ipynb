{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### About\n",
    "\n",
    "> Explainable AI\n",
    "\n",
    "Explainable Artificial Intelligence (XAI) refers to the design, development and implementation of artificial intelligence (AI) models whose prediction and decision-making processes are transparent, understandable and explainable to humans. XAI aims to provide insight into how AI models arrive at predictions or decisions, allowing users to understand and trust the results of those models. This is especially important in areas where the consequences of AI decisions can have a significant impact, such as healthcare, finance and criminal justice.\n",
    "\n",
    "XAI methods and techniques allow people to understand aspects of an AI model, such as its input, output, inner workings, and decision-making. By providing interpretable explanations for AI model predictions, XAI can help users gain insights, identify outliers, validate model accuracy, and make informed decisions based on model output. XAI can also help meet regulatory requirements, address ethical issues, and build trust between AI systems and users. There are several ways to achieve explainable AI, including:\n",
    "\n",
    "1. Rule-based methods: These methods involve the use of logical rules or decision trees to explain the decision-making process of an AI model. Examples are decision trees, rule-based expert systems, and symbolic AI techniques.\n",
    "\n",
    "2. Feature Importance Methods: These methods analyze the importance of various features or inputs to the AI ​​model's predictions. Examples: characteristic plots, permuted characteristic plots, and partial dependence plots. \n",
    "3. Locally Interpretable Model Agnostic Explanation (LIME): LIME is a technique that explains the predictions of any AI model by approximating it with a simpler, interpretable model for a given instance. This provides insight into how the AI ​​model predicted a particular case.\n",
    "\n",
    "4. Model-specific methods: These methods are tailored to specific types of AI models (such as linear regression, logistic regression, or decision trees) and provide explanatory explanations based on the inner workings of those models.\n",
    "\n",
    "5. Visualization technology: Visualization techniques can visualize the input, output, and decision-making processes of artificial intelligence models, making it easier for humans to understand and explain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from lime import lime_tabular\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline with a logistic regression model\n",
    "model = make_pipeline(StandardScaler(), LogisticRegression())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
       "                ('logisticregression', LogisticRegression())])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a LimeTabularExplainer\n",
    "explainer = lime_tabular.LimeTabularExplainer(X_train, feature_names=iris.feature_names, class_names=iris.target_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explain the predictions for a specific instance\n",
    "instance_idx = 0\n",
    "explanation = explainer.explain_instance(X_test[instance_idx], model.predict_proba, num_features=len(iris.feature_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explanation for instance 0\n",
      "[('0.30 < petal width (cm) <= 1.30', 0.16378350989955373), ('sepal width (cm) <= 2.80', 0.14363068402453236), ('5.75 < sepal length (cm) <= 6.40', 0.11026045397437777), ('4.25 < petal length (cm) <= 5.10', 0.05363215300671602)]\n"
     ]
    }
   ],
   "source": [
    "# Print the explanation\n",
    "print('Explanation for instance', instance_idx)\n",
    "print(explanation.as_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
