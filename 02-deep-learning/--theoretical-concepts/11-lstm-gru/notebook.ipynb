{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### About\n",
    "\n",
    "> LSTM and GRU\n",
    "\n",
    "Long-Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU) are two recurrent neural network (RNN) architectures commonly used for temporal computing tasks such as weather forecasting, natural language processing, and speech recognition. \n",
    "\n",
    "They are designed to overcome the limitations of traditional RNNs, such as the vanishing gradient problem that can occur when training deep neural networks. \n",
    "\n",
    "The LSTM and GRU architecture implement a gating mechanism that allows the network to selectively update and store long-sequence information, enabling better learning and modeling of long-term dependencies in sequence data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. LSTM (Long Short Term Memory):\n",
    "\n",
    "An LSTM cell has three gates: an input gate, a forget gate, and an output gate. The input gate controls how much new information is added to the cell state, the forget gate controls how much information is forgotten from the cell state, and the output gate controls how much information is sent to the output. \n",
    "\n",
    "\n",
    "\n",
    "```f_t = sigmoid(W_f . x_t + U_f . h_t-1 + b_f)  # forget gate\n",
    "i_t = sigmoid(W_i . x_t + U_i . h_t-1 + b_i)  # input gate\n",
    "o_t = sigmoid(W_o . x_t + U_o . h_t-1 + b_o)  # output gate\n",
    "c_t = f_t .*  c_t-1 + i_t .*  tanh(W_c . x_t + U_c . h_t-1 + b_c)  # cell state\n",
    "h_t = o_t .*  tanh(c_t)  # hidden state/output\n",
    "```\n",
    "\n",
    "Where:\n",
    "\n",
    "- x_t is the input at time step t\n",
    "- h_t is the hidden state/output at time step t\n",
    "- c_t is the cell state at time step t\n",
    "- W_f, W_i, W_o, W_c are the weight matrices for the input x_t\n",
    "- U_f, U_i, U_o, U_c are the weight matrices for the previous hidden state h_t-1\n",
    "- b_f, b_i, b_o, b_c are the bias vectors\n",
    "- sigmoid is the sigmoid activation function\n",
    "- tanh is the hyperbolic tangent activation function\n",
    "- .*  represents element-wise multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-22 03:23:43.336066: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-04-22 03:23:43.392197: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-04-22 03:23:43.392597: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-22 03:23:45.044380: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-22 03:23:46.928153: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-04-22 03:23:46.929315: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1956] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2023-04-22 03:23:47.628260: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-04-22 03:23:47.630506: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-04-22 03:23:47.632767: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(32, input_shape=(10, 5)))  # LSTM layer with 32 units and input shape of (10, 5)\n",
    "model.add(Dense(1, activation='sigmoid'))  # Output layer with sigmoid activation for binary classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. GRU (Restricted Periodic Unit):\n",
    "The GRU device has two gates: a update gate and a reset gate. The update gate determines how much of the previously hidden state is mixed with the new candidate hidden state, and the reset gate determines how much of the previously hidden state is forgotten.\n",
    "\n",
    "The key difference between GRU and LSTM is that GRU has fewer parameters than LSTM and is therefore faster to train. GRU also combines the forget and input gates of LSTM into a single \"update gate\" and merges the cell state and hidden state into a single \"hidden state\".\n",
    "\n",
    "\n",
    "\n",
    "Given an input sequence of length T = (x_1, x_2, ..., x_T), and hidden state h_t at time step t, GRU computes the update gate z_t, reset gate r_t, and new candidate hidden state h~_t as follows:\n",
    "\n",
    "```\n",
    "z_t = sigmoid(W_zx_t + U_zh_{t-1} + b_z)\n",
    "r_t = sigmoid(W_rx_t + U_rh_{t-1} + b_r)\n",
    "h~t = tanh(Wx_t + r_t(U*h{t-1}) + b)\n",
    "\n",
    "```\n",
    "\n",
    "where W, U, and b are the weight matrix, hidden state matrix, and bias vector, respectively, for the corresponding gate or hidden state. sigmoid is the sigmoid activation function, and tanh is the hyperbolic tangent activation function.\n",
    "\n",
    "Next, the current hidden state h_t is computed as a weighted sum of the previous hidden state h_{t-1} and the candidate hidden state h~_t, with the weights determined by the update gate z_t:\n",
    "\n",
    "h_t = (1 - z_t)h_{t-1} + z_th~_t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import GRU\n",
    "\n",
    "#num_units = dim of the output space\n",
    "gru = GRU(num_units, activation='tanh', recurrent_activation='sigmoid', \n",
    "          use_bias=True, kernel_initializer='glorot_uniform', \n",
    "          recurrent_initializer='orthogonal', bias_initializer='zeros', \n",
    "          kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, \n",
    "          activity_regularizer=None, dropout=0.0, recurrent_dropout=0.0, \n",
    "          implementation=1, return_sequences=False, return_state=False, \n",
    "          go_backwards=False, stateful=False, reset_after=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
