{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### About\n",
    "\n",
    "> Knowledge Distillation \n",
    "\n",
    "Knowledge distillation is a technique used in machine learning to transfer knowledge from a complex or large model (often called a \"teacher\" model) to a simpler or smaller model (often called a \"learner\" model). The goal of knowledge distillation is to train the student model to mimic the behavior of the teacher model, thus benefiting from the knowledge of the teacher model while reducing the memory footprint and potentially faster inference time. \n",
    "\n",
    "The basic idea behind knowledge distillation is to use teacher model outputs (eg, predicted probabilities or logistic) as \"soft targets\" during training, rather than the hard labels (eg, one-time coded labels) typically used in default supervision. Soft targets are more informative than hard targets because they encode the confidence or uncertainty of the teacher model in its predictions. The learner model is then trained to minimize the difference between its predictions and the soft measures produced by the teacher model. This allows the learner model to learn not only from the ground truth labels, but also from the knowledge and insights that the teacher model captures during training. The knowledge distillation process typically involves the following steps:\n",
    "\n",
    "1. Train teacher models: Train complex or large models on large labeled data sets to achieve high accuracy or performance. This model is used as a source of knowledge to transfer to the student model.\n",
    "\n",
    "2. Gathering soft measures: Use a trained teacher model to generate soft measures (such as expected probabilities or logistic) for a set of unlabeled or labeled data samples that the learner model will use during training.\n",
    "\n",
    "3. Training the trained model: Use the labeled dataset and the soft objects generated by the teacher model to train simpler or smaller models (eg, with fewer parameters or layers). A learner model is typically trained to minimize the difference between predictions and soft targets using an appropriate loss function. \n",
    "\n",
    "4. Refine the learner model. The trained model can optionally be further refined using labeled datasets and ground truth labels (ie, hard labels) to improve its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data\n",
    "x_train = np.random.rand(1000, 10)\n",
    "y_train = np.random.randint(0, 2, size=(1000,))\n",
    "y_train_onehot = np.eye(2)[y_train]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the teacher model\n",
    "teacher_model = Sequential()\n",
    "teacher_model.add(Dense(units=64, activation='relu', input_dim=10))\n",
    "teacher_model.add(Dense(units=2, activation='softmax'))\n",
    "teacher_model.compile(optimizer=Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "32/32 [==============================] - 1s 3ms/step - loss: 0.6896 - accuracy: 0.5450\n",
      "Epoch 2/10\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.6898 - accuracy: 0.5450\n",
      "Epoch 3/10\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.6890 - accuracy: 0.5460\n",
      "Epoch 4/10\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.6863 - accuracy: 0.5500\n",
      "Epoch 5/10\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6858 - accuracy: 0.5490\n",
      "Epoch 6/10\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.6853 - accuracy: 0.5650\n",
      "Epoch 7/10\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.6853 - accuracy: 0.5560\n",
      "Epoch 8/10\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.6844 - accuracy: 0.5620\n",
      "Epoch 9/10\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.6829 - accuracy: 0.5630\n",
      "Epoch 10/10\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.6825 - accuracy: 0.5530\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f83a06a2f10>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the teacher model\n",
    "teacher_model.fit(x_train, y_train, batch_size=32, epochs=10, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "# Obtain teacher logits during training\n",
    "teacher_logits = teacher_model.predict(x_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the student model\n",
    "student_model = Sequential()\n",
    "student_model.add(Dense(units=32, activation='relu', input_dim=10))\n",
    "student_model.add(Dense(units=2, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the temperature hyperparameter\n",
    "temperature = 5  # Example value, adjust as needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the distillation loss function\n",
    "def distillation_loss(y_true, y_pred):\n",
    "\n",
    "    # Soften the logits by dividing by temperature\n",
    "    softened_logits = y_pred / temperature\n",
    "    \n",
    "    # Compute cross-entropy between softened logits and true labels\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(labels=y_true, logits=softened_logits)\n",
    "    \n",
    "    # Return the loss\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the student model with the distillation loss function\n",
    "student_model.compile(optimizer=Adam(), loss=distillation_loss, metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "32/32 [==============================] - 1s 4ms/step - loss: 0.6905 - accuracy: 0.5370\n",
      "Epoch 2/10\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6902 - accuracy: 0.5370\n",
      "Epoch 3/10\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 0.6901 - accuracy: 0.5370\n",
      "Epoch 4/10\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6900 - accuracy: 0.5370\n",
      "Epoch 5/10\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6899 - accuracy: 0.5370\n",
      "Epoch 6/10\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.6898 - accuracy: 0.5370\n",
      "Epoch 7/10\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.6897 - accuracy: 0.5370\n",
      "Epoch 8/10\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6896 - accuracy: 0.5370\n",
      "Epoch 9/10\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6895 - accuracy: 0.5370\n",
      "Epoch 10/10\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6894 - accuracy: 0.5370\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f83a05b1190>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the student model with knowledge distillation\n",
    "student_model.fit(x_train, y_train_onehot, batch_size=32, epochs=10, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic test data\n",
    "x_test = np.random.rand(200, 10)\n",
    "y_test = np.random.randint(0, 2, size=(200,))\n",
    "y_test_onehot = np.eye(2)[y_test]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "# Predict using the teacher model\n",
    "teacher_pred = np.argmax(teacher_model.predict(x_test), axis=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of teacher model: 0.5550\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy of teacher model\n",
    "teacher_accuracy = np.mean(teacher_pred == y_test)\n",
    "print(f\"Accuracy of teacher model: {teacher_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "# Predict using the student model\n",
    "student_pred = np.argmax(student_model.predict(x_test), axis=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of student model: 0.5250\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy of student model\n",
    "student_accuracy = np.mean(student_pred == y_test)\n",
    "print(f\"Accuracy of student model: {student_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
