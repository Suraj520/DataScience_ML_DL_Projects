{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### About\n",
    "\n",
    "> Gradient initialization\n",
    "\n",
    "Gradient initialization is an important step in deep neural network training because it determines the initial values ​​of model parameters, which have a significant impact on model convergence and performance during training. Here is an explanation of some common gradient initialization methods used in deep learning:\n",
    "\n",
    "1 Zero initialization\n",
    "\n",
    "With zero initialization, all model parameters (weights and biases) are initialized to zeros. Mathematically, this can be expressed as:\n",
    "\n",
    "\n",
    "W[l] = 0, where W[l] is the weight matrix of layer l\n",
    "b[l] = 0, where b[l] is the bias vector of layer l\n",
    "\n",
    "\n",
    "But initializing all parameters to zero can cause a \"symmetry problem\", i.e. all neurons in a single layer will have the same output and gradient and will be constantly updated with the same values ​​during training, resulting in poor model performance. bad.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'W1': array([[0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.]]), 'b1': array([[0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.]]), 'W2': array([[0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0.]]), 'b2': array([[0.],\n",
      "       [0.]])}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def initialize_parameters_zeros(layers_dims):\n",
    "    parameters = {}\n",
    "    L = len(layers_dims)\n",
    "\n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.zeros((layers_dims[l], layers_dims[l-1]))\n",
    "        parameters['b' + str(l)] = np.zeros((layers_dims[l], 1))\n",
    "\n",
    "    return parameters\n",
    "\n",
    "layers_dims = [3, 4, 2] # example network architecture\n",
    "parameters = initialize_parameters_zeros(layers_dims)\n",
    "print(parameters)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 Sample initialization\n",
    "\n",
    "Random initialization involves initializing the parameters with random values ​​drawn from a given distribution. This helps eliminate symmetry issues and introduces some randomness to the pattern. Common distributions used for random initialization are the Gaussian (or normal) distribution and the Xavier/Glorot initialization. \n",
    "\n",
    "\n",
    "2.1. Gaussian (normal) initialization\n",
    "\n",
    "\n",
    "In Gaussian initialization, parameters are initialized with random values ​​drawn from a Gaussian distribution with mean 0 and specified standard deviation (sigma). Mathematically, this can be expressed as:\n",
    "\n",
    "W[l] = np.random.randn(lag_dims[l], lag_dims[l-1]) * sigma,\n",
    "where W[l] is the weight matrix of layer l,\n",
    "Sigma is the standard deviation,\n",
    "np.random.randn generates random samples from a standard normal distribution\n",
    "b[l] = np.zeros((layers_dims[l], 1))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'W1': array([[-0.00695358,  0.00745988, -0.00420675],\n",
      "       [-0.00317408,  0.00562514, -0.00397311],\n",
      "       [-0.00289508,  0.00873246, -0.00480993],\n",
      "       [ 0.00251623, -0.00502894,  0.01279217]]), 'b1': array([[0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.]]), 'W2': array([[-0.00066028, -0.00999176,  0.00600391,  0.01159534],\n",
      "       [-0.00347197, -0.0055126 ,  0.00232397, -0.00591203]]), 'b2': array([[0.],\n",
      "       [0.]])}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def initialize_parameters_gaussian(layers_dims, sigma):\n",
    "    parameters = {}\n",
    "    L = len(layers_dims)\n",
    "\n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.random.randn(layers_dims[l], layers_dims[l-1]) * sigma\n",
    "        parameters['b' + str(l)] = np.zeros((layers_dims[l], 1))\n",
    "\n",
    "    return parameters\n",
    "\n",
    "\n",
    "layers_dims = [3, 4, 2] # network architecture\n",
    "sigma = 0.01 # standard deviation\n",
    "parameters = initialize_parameters_gaussian(layers_dims, sigma)\n",
    "print(parameters)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2 Initialization of Xavier/Glorot\n",
    "\n",
    "Xavier/Glorot initialization is a popular method for parameter initialization of deep neural networks. Its purpose is to make the activations and gradient differences of different layers roughly the same during training. Mathematically, Xavier's initialization can be expressed as:\n",
    "\n",
    "W[l] = np.random.randn(layers_dims[l], layers_dims[l-1]) * np.sqrt(1 / layers_dims[l-1]),\n",
    "where W[l] is the weight matrix of layer l,\n",
    "np.sqrt(1 / layers_dims[l-1]) is the scaling factor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'W1': array([[-0.11098635, -0.24493321, -0.67057116],\n",
      "       [-0.28679694, -0.28776603,  0.24651697],\n",
      "       [ 0.46407216,  0.85892708, -0.06777731],\n",
      "       [-1.41570239,  0.40839064, -0.06516387]]), 'b1': array([[0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.]]), 'W2': array([[-0.33810429,  0.68169883, -0.03199655, -0.01838845],\n",
      "       [ 0.88044223, -0.02293936, -0.07781658,  0.71352702]]), 'b2': array([[0.],\n",
      "       [0.]])}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def initialize_parameters_xavier(layers_dims):\n",
    "    parameters = {}\n",
    "    L = len(layers_dims)\n",
    "\n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.random.randn(layers_dims[l], layers_dims[l-1]) * np.sqrt(1 / layers_dims[l-1])\n",
    "        parameters['b' + str(l)] = np.zeros((layers_dims[l], 1))\n",
    "\n",
    "    return parameters\n",
    "\n",
    "\n",
    "layers_dims = [3, 4, 2] # network architecture\n",
    "parameters = initialize_parameters_xavier(layers_dims)\n",
    "print(parameters)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 He initialization\n",
    "Initialization is another popular parameter initialization method for deep neural networks, especially for ReLU activation functions. Its purpose is to make the differences in activations and gradients of different layers roughly equal during training. Mathematically, the initialization of He can be expressed as:\n",
    "\n",
    "W[l] = np.random.randn(layers_dims[l], layers_dims[l-1]) * np.sqrt(2 / layers_dims[l-1]),\n",
    "where W[l] is the weight matrix of layer l,\n",
    "np.sqrt(2 / layers_dims[l-1]) is the scaling factor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'W1': array([[-1.98472076e-02,  1.87633010e-02, -8.23628512e-02],\n",
      "       [-8.91678901e-01, -8.56789613e-01, -1.77551319e-01],\n",
      "       [ 9.49111713e-01, -5.85277205e-01, -9.94537159e-04],\n",
      "       [-7.57846243e-01, -6.68402924e-01, -1.53590788e+00]]), 'b1': array([[0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.]]), 'W2': array([[ 1.07533685, -0.2003852 ,  0.15137989, -0.41358273],\n",
      "       [-0.19039654,  0.11565043,  0.04322966, -0.97289426]]), 'b2': array([[0.],\n",
      "       [0.]])}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def initialize_parameters_he(layers_dims):\n",
    "    parameters = {}\n",
    "    L = len(layers_dims)\n",
    "\n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.random.randn(layers_dims[l], layers_dims[l-1]) * np.sqrt(2 / layers_dims[l-1])\n",
    "        parameters['b' + str(l)] = np.zeros((layers_dims[l], 1))\n",
    "\n",
    "    return parameters\n",
    "\n",
    "\n",
    "layers_dims = [3, 4, 2]\n",
    "parameters = initialize_parameters_he(layers_dims)\n",
    "print(parameters)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient initialization is a critical step in training deep neural networks, and the choice of initialization method can significantly affect model convergence and performance. It is important to test different initialization methods and choose the one that works best for your particular model and task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
