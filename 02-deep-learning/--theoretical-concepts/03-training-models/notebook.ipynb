{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### About\n",
    "\n",
    "> Training models\n",
    "\n",
    "Model training in deep learning involves the process of optimizing model parameters (weights and biases) to minimize a loss or cost function. This is usually done using an optimization algorithm, such as gradient descent, which iteratively adjusts the model parameters according to the gradient of the loss function with respect to the parameters. \n",
    "\n",
    "The general steps for training a deep learning model can be summarized as follows:\n",
    "\n",
    "1. Data preparation: Prepare training data by performing pre-processing such as resizing images, normalizing pixel values, and splitting the data into training and validation sets.\n",
    "\n",
    "2. Model architecture: Define the architecture of the deep learning model, including the number of layers, the types of layers (such as convolutional layers, pooling layers, fully connected layers), and their configurations (such as the number of filters, filter size, and activation). functions). \n",
    "\n",
    "3. Initialization: Initialize the model parameters (weights and biases) with small random values ​​or use special initialization methods (such as Xavier or He initialization) to ensure that the model starts with a good set of initial values.\n",
    "\n",
    "5. Forward Propagation: Implements the forward propagation step, which involves passing the input data through the layers of the model to obtain the expected results. This includes applying activation functions, convolutions, pooling, and other operations based on the model architecture.\n",
    "\n",
    "5. Compute Loss: Compute a loss or cost function that measures the difference between the predicted output and the ground truth labels. This is usually done using one of the loss functions discussed above.\n",
    "\n",
    "6. Backpropagation: implements a backpropagation step that involves computing the gradient of the loss function with respect to the model parameters. This is done by using a computational chain rule to propagate the gradients from the output layer to the input layer.\n",
    "\n",
    "7. Update parameters: Use an optimization algorithm such as gradient descent or its variants to update the model parameters by subtracting the product of the gradient from the learning rate. It adjusts the parameters in a direction that minimizes the loss function.\n",
    "\n",
    "8. Iterate: Repeat forward propagation, loss calculation, backpropagation, and parameter update iteratively for multiple epochs (iterating over the entire training dataset) or until a stopping criterion is met, such as convergence of the loss function. \n",
    "\n",
    "9. Validation: Regularly evaluate the performance of the model on the validation set to monitor its generalizability and prevent overfitting. This may include the calculation of various performance measures such as precision, accuracy, recall and F1 scores. \n",
    "\n",
    "10.  Testing: Finally, the trained model is evaluated with a separate set of tests to obtain its performance on unseen data and evaluate its effectiveness in the real world."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample code for training CNN using CIFAR 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-22 01:09:53.841718: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-04-22 01:09:53.906192: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-04-22 01:09:53.907555: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-22 01:09:55.368987: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras.optimizers import SGD\n",
    "from keras.datasets import cifar10\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170498071/170498071 [==============================] - 72s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess CIFAR-10 dataset\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255\n",
    "y_train = to_categorical(y_train, num_classes=10)\n",
    "y_test = to_categorical(y_test, num_classes=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-22 01:11:17.018604: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-04-22 01:11:17.020687: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1956] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "# Define CNN architecture\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(32, 32, 3)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/suraj/anaconda3/envs/dl/lib/python3.9/site-packages/keras/optimizers/legacy/gradient_descent.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "#compile model\n",
    "model.compile(optimizer=SGD(lr=0.01), loss=categorical_crossentropy, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1407/1407 [==============================] - 47s 33ms/step - loss: 1.8729 - accuracy: 0.3305 - val_loss: 1.7336 - val_accuracy: 0.3754\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f42b022cdf0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train model\n",
    "model.fit(x_train, y_train, batch_size=32, epochs=1, validation_split=0.1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 3s 8ms/step - loss: 1.7243 - accuracy: 0.3838\n",
      "Test loss: 1.724299430847168\n",
      "Test accuracy: 0.3837999999523163\n"
     ]
    }
   ],
   "source": [
    "#Evaluate model\n",
    "\n",
    "score = model.evaluate(x_test, y_test, batch_size=32)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
