{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### About\n",
    "\n",
    "> Federated learning\n",
    "\n",
    "Federated learning is a machine learning approach that allows multiple distributed entities (such as devices or servers) to collaborate to train a shared machine learning model without sharing their raw data with a central server. Instead, each device trains a model locally based on its own data and only shares model updates or gradients with a central server that aggregates these updates to update the global model. It allows you to train machine learning models on multiple devices while keeping your data decentralized and secure.\n",
    "\n",
    "The main idea behind federated learning is to use available data from different entities (often called customers or employees) to train a global model that uses the collective knowledge of all entities without transmitting or revealing the raw data. Blended learning is particularly useful when data is distributed across multiple devices (e.g. edge devices (e.g. smartphones, IoT devices)) or between different organizations (e.g. hospitals, banks) that may have privacy concerns or legal restrictions on data sharing, which is important.\n",
    "\n",
    "The blended learning process usually includes the following activities:\n",
    "\n",
    "1. Initialization: The global machine learning model is initialized on a central server.\n",
    "2. Client Update: Each client device locally trains the global model using its own data and updates the model by computing gradients.\n",
    "3. Model Aggregation: A central server aggregates client gradients to update the global model.\n",
    "4. Model Deployment: The updated global model is then distributed to client entities that use it for local inference or further training.\n",
    "5. Iteration: 2-4. the operation is repeated iteratively for several rounds until the global model converges to an acceptable level of performance.\n",
    "\n",
    "Federated training offers several benefits, including data protection because raw data is never left on the local device, reducing the risk of data breaches or privacy violations. It also enables efficient and scalable training of machine learning models on distributed devices, enabling faster model updates and reducing communication overhead. But federated learning also presents challenges such as data heterogeneity between clients, communication and synchronization overhead, and potential bias due to non-IID. (independent and identically distributed) data. Successful implementation of federated learning in real-world scenarios requires careful consideration of these issues, as well as appropriate model development and optimization techniques.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-22 05:46:08.829414: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-04-22 05:46:08.902495: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-04-22 05:46:08.904334: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-22 05:46:10.363731: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-22 05:46:13.431601: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-04-22 05:46:13.434452: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1956] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "# Define the client model architecture\n",
    "client_model = Sequential()\n",
    "client_model.add(layers.Dense(16, activation='relu', input_dim=8))\n",
    "client_model.add(layers.Dense(8, activation='relu'))\n",
    "client_model.add(layers.Dense(1, activation='sigmoid'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the client model\n",
    "client_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the global model architecture\n",
    "global_model = Sequential()\n",
    "global_model.add(layers.Dense(16, activation='relu', input_dim=8))\n",
    "global_model.add(layers.Dense(8, activation='relu'))\n",
    "global_model.add(layers.Dense(1, activation='sigmoid'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the global model\n",
    "global_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the global model with the client model's weights\n",
    "global_model.set_weights(client_model.get_weights())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Federated Learning loop\n",
    "num_clients = 5  # Number of client entities\n",
    "num_epochs = 10  # Number of epochs for each client update\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulated client data\n",
    "client_data = []  \n",
    "for i in range(num_clients):\n",
    "    X = np.random.rand(100, 8)  # Random input data\n",
    "    y = np.random.randint(0, 2, size=(100,))  # Random labels\n",
    "    client_data.append((X, y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Federated Learning loop\n",
    "for epoch in range(num_epochs):\n",
    "    for client in client_data:\n",
    "        X, y = client\n",
    "        client_model.fit(X, y, epochs=1, verbose=0)  # Local client update\n",
    "\n",
    "    # Aggregate the client model weights to update the global model\n",
    "    global_weights = global_model.get_weights()\n",
    "    client_weights = client_model.get_weights()\n",
    "    updated_weights = [(global_weights[i] + client_weights[i]) / num_clients for i in range(len(global_weights))]\n",
    "    global_model.set_weights(updated_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the global model for prediction or further training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
