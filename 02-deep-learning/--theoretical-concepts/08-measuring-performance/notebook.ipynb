{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### About\n",
    "\n",
    "> Measuring performance\n",
    "\n",
    "Measuring the performance of deep learning networks involves the ability of a trained model to make accurate predictions on unseen data. This is usually done using various performance measures that quantify the ability of the model to correctly classify or predict the target variable. Types of metrics are - \n",
    "\n",
    "1. Accuracy: Accuracy is the simplest performance measure, defined as the ratio of correctly classified samples to the total number of samples. It provides an overall measure of model performance in terms of correct predictions. \n",
    "\n",
    "Accuracy = (number of samples correctly classified) / (total number of samples)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_pred, y_true):\n",
    "    _, predicted = torch.max(y_pred, 1)\n",
    "    correct = (predicted == y_true).sum().item()\n",
    "    total = y_true.size(0)\n",
    "    return correct / total\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Loss: Loss is a measure of how well a model is able to reduce the difference between predicted and true values ​​during training. It is often used as an optimization objective during model training. Lower loss values ​​indicate better performance. Depending on the type of problem, different types of loss functions can be used, such as mean squared error (MSE) for regression problems and cross-entropy loss for classification problems. \n",
    "\n",
    "loss = f(y_pred, y_true)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Precision, recall and F1 score: Precision, recall and F1 score are common performance measures for binary or multi-class classification problems. These metrics provide insight into the trade-off between precision (the ability to correctly identify positive samples), recall (the ability to correctly identify all positive samples), and the harmonic mean of precision and recall (F1 score). \n",
    "\n",
    "Precision = (true positives) / (true positives + false positives)\n",
    "\n",
    "Recall = (true positives) / (true positives + false negatives)\n",
    "\n",
    "F1 score = 2 * (precision * recall) / (precision + recall)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:228: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "def calculate_metrics(y_pred, y_true):\n",
    "    _, predicted = torch.max(y_pred, 1)\n",
    "    y_pred = predicted.cpu().numpy()\n",
    "    y_true = y_true.cpu().numpy()\n",
    "    precision = precision_score(y_true, y_pred, average='macro')\n",
    "    recall = recall_score(y_true, y_pred, average='macro')\n",
    "    f1 = f1_score(y_true, y_pred, average='macro')\n",
    "    return precision, recall, f1\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Confusion Matrix: A confusion matrix is ​​a table often used to describe the performance of a classification model for a set of data whose true values ​​are known. It shows the number of correctly and incorrectly predicted samples for each class. The diagonal of the confusion matrix represents true positive (TP) and true negative (TN) values, while the off-diagonal elements represent false positive (FP) and false negative (FN) values. The confusion matrix provides a detailed breakdown of the model's performance for each class, which is useful for identifying specific areas where the model may have problems. Mathematically, the confusion matrix can be expressed as:\n",
    "\n",
    "```Forecast\n",
    "|Category 1 |Category 2 | ... |n category |\n",
    "the actual class\n",
    "Category 1 | TP | Family Planning | ... |Family Planning |\n",
    "Category 2 | Normal | TP | ... | Planned Parenthood |\n",
    "... | ... | ... | ... | ... |\n",
    "n class | normal | normal | ... | tp |\n",
    "```\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def calculate_confusion_matrix(y_pred, y_true, labels):\n",
    "    _, predicted = torch.max(y_pred, 1)\n",
    "    y_pred = predicted.cpu().numpy()\n",
    "    y_true = y_true.cpu().numpy()\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "    return cm\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Receiver operating characteristic (ROC) curve: The ROC curve is a graphical plot that shows the true positive rate (TPR) versus the false positive rate (FPR) for different classification thresholds. It is often used in binary classification problems to evaluate a model's ability to correctly classify positive and negative samples. The area under the ROC curve (AUC) is a widely used measure of performance, and a higher AUC indicates better performance. Mathematically, the ROC curve and AUC can be calculated as follows:\n",
    "\n",
    "TPR = TP / (TP + FN)\n",
    "FPR = FP / (FP + TN)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "def calculate_roc_auc(y_pred, y_true):\n",
    "    y_probs = torch.softmax(y_pred, dim=1).cpu().numpy()\n",
    "    y_true = y_true.cpu().numpy()\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_probs[:, 1])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    return fpr, tpr, roc_auc\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary, measuring the performance of a deep learning network involves evaluating various performance metrics such as precision, loss, precision, recall, F1 score, confusion matrix, and ROC curve. These metrics provide insight into the model's ability to correctly classify or predict the target variable and help evaluate overall performance and identify any areas of the model that require improvement."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
