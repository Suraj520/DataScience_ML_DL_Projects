{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### About\n",
    "\n",
    "> Model compression and quantization\n",
    "\n",
    "Model compression and quantization are techniques used to reduce the size or complexity of machine learning models, making them more efficient in terms of storage, memory usage, and computation.\n",
    "\n",
    "1. Model Compression: Model compression techniques aim to reduce the size of a model by reducing its parameter count or memory footprint while maintaining its performance or accuracy. Some common methods of model compression include:\n",
    "\n",
    "- Pruning: Pruning involves removing redundant or unnecessary weights or neurons from the model. This can be done during training (e.g. by setting small weights to zero) or after training (e.g. by removing neurons with negligible activation values).\n",
    "\n",
    "- Quantization: Quantization involves reducing the precision of model weights and/or activations from floating-point numbers (eg 32-bit) to a smaller bit representation (eg 8-bit or less). This reduces the memory footprint of the model and can speed up inference on hardware where precision support is limited.\n",
    "\n",
    "- Distillation of knowledge. Knowledge distillation involves training a smaller “learner” model to mimic the predictions of a larger “teacher” model. The student model learns to approximate the output of the teacher model, which can often be a more compact representation of the knowledge in the original model.\n",
    "\n",
    "2. Model quantization. Model quantization techniques aim to represent model weights and/or activations in low-bit representation (e.g., 8-bit or smaller) rather than floating-point numbers (e.g., 32-bit). This reduces the memory footprint of the model and can speed up inference on hardware where accuracy support is limited. Some common methods of pattern quantization include:\n",
    "\n",
    "- Post-training quantization: Post-training quantization involves post-training quantization of the weights and/or activations of a trained model. This can be done using techniques such as uniform quantization, where values ​​are quantized to a fixed set of levels, or uneven quantization, where quantization levels are adaptive based on the distribution of the data. \n",
    "\n",
    "- Quantization Awareness Training: Quantization training involves training a model with the goal of optimizing its performance using quantized weights and/or activations. This may include techniques such as quantization-aware backpropagation, which takes quantization effects into account during gradient computation and weight update, or the use of specialized quantization-aware optimization algorithms. \n",
    "\n",
    "\n",
    "Model compression and quantization techniques are often used in cases where model size, memory consumption, or computational efficiency are critical, such as deployment on resource-constrained devices such as mobile devices, embedded systems, or edge devices with limited computing power or memory. However, it is important to note that model compression and quantization techniques may change the level of model accuracy or performance in favor of reduced size or complexity, the effectiveness of which depends on the specific use case and application requirements.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize pixel values to be between 0 and 1\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0\n",
    "\n",
    "# Flatten the input images\n",
    "x_train = x_train.reshape(-1, 784)\n",
    "x_test = x_test.reshape(-1, 784)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(128, activation='relu', input_shape=(784,)))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1688/1688 [==============================] - 9s 4ms/step - loss: 0.2576 - accuracy: 0.9255 - val_loss: 0.1240 - val_accuracy: 0.9623\n",
      "Epoch 2/5\n",
      "1688/1688 [==============================] - 7s 4ms/step - loss: 0.1073 - accuracy: 0.9674 - val_loss: 0.0767 - val_accuracy: 0.9782\n",
      "Epoch 3/5\n",
      "1688/1688 [==============================] - 7s 4ms/step - loss: 0.0746 - accuracy: 0.9773 - val_loss: 0.0775 - val_accuracy: 0.9773\n",
      "Epoch 4/5\n",
      "1688/1688 [==============================] - 7s 4ms/step - loss: 0.0544 - accuracy: 0.9823 - val_loss: 0.0803 - val_accuracy: 0.9752\n",
      "Epoch 5/5\n",
      "1688/1688 [==============================] - 8s 5ms/step - loss: 0.0430 - accuracy: 0.9861 - val_loss: 0.0744 - val_accuracy: 0.9782\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f947027f4c0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=5, batch_size=32, validation_split=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0865 - accuracy: 0.9748\n",
      "Accuracy: 0.9747999906539917\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "_, accuracy = model.evaluate(x_test, y_test, batch_size=32)\n",
    "print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp8ngs7tut/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp8ngs7tut/assets\n",
      "2023-04-22 06:06:44.167954: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:364] Ignored output_format.\n",
      "2023-04-22 06:06:44.168012: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:367] Ignored drop_control_dependency.\n",
      "2023-04-22 06:06:44.168312: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/tmp8ngs7tut\n",
      "2023-04-22 06:06:44.169497: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-04-22 06:06:44.169576: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/tmp8ngs7tut\n",
      "2023-04-22 06:06:44.176735: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-04-22 06:06:44.234895: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/tmp8ngs7tut\n",
      "2023-04-22 06:06:44.250530: I tensorflow/cc/saved_model/loader.cc:314] SavedModel load for tags { serve }; Status: success: OK. Took 82218 microseconds.\n"
     ]
    }
   ],
   "source": [
    "# Convert the model to a quantized model with 8-bit weights and activations\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.target_spec.supported_types = [tf.float16]\n",
    "tflite_model_quantized = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the quantized model to a file\n",
    "with open('model_quantized.tflite', 'wb') as f:\n",
    "    f.write(tflite_model_quantized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9748\n"
     ]
    }
   ],
   "source": [
    "# Load the quantized model\n",
    "interpreter = tf.lite.Interpreter(model_path='model_quantized.tflite')\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get the input and output details of the model\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Prepare the test data\n",
    "test_data = np.array(x_test, dtype=np.float32)\n",
    "\n",
    "# Run inference on the quantized model\n",
    "predictions = []\n",
    "for i in range(len(test_data)):\n",
    "    interpreter.set_tensor(input_details[0]['index'], [test_data[i]])\n",
    "    interpreter.invoke()\n",
    "    output = interpreter.get_tensor(output_details[0]['index'])\n",
    "    predicted_label = np.argmax(output)\n",
    "    predictions.append(predicted_label)\n",
    "\n",
    "# Calculate accuracy\n",
    "correct_predictions = np.sum(np.array(predictions) == y_test)\n",
    "accuracy = correct_predictions / len(y_test)\n",
    "\n",
    "print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
