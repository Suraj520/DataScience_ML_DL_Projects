{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### About\n",
    "\n",
    "> Loss functions\n",
    "\n",
    "A loss function, also known as an objective function or cost function, is a mathematical function that measures the difference between the predicted output and the actual target output of a machine learning model. They play a key role in training deep neural networks because they are used to guide the optimization process by quantifying the error or loss in model predictions. The goal of training is to minimize the value of the loss function, thereby improving the performance of the model.\n",
    "\n",
    "Different types of loss functions\n",
    "are used for different types of problems, and the choice of an appropriate loss function depends on the particular problem being solved.\n",
    "\n",
    "1. Mean Square Error (MSE):\n",
    "\n",
    "Math formula: \n",
    "\n",
    "MSE(y_true, y_pred) = 1/n * sum((y_true - y_pred)^2)\n",
    "\n",
    "Explanation: \n",
    "\n",
    "MSE calculates the mean squared difference between the predicted value (y_pred) and the true label (y_true). It is often used in regression problems that aim to predict continuous values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementation\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def mean_squared_error(y_true, y_pred):\n",
    "    n = len(y_true)\n",
    "    mse = np.mean((y_true - y_pred)**2)\n",
    "    return mse"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Binary Cross Entropy (BCE):\n",
    "\n",
    "Mathematical formula: \n",
    "BCE(y_true, y_pred) = -[y_true * log(y_pred) + (1 - y_true) * log(1 - y_pred)]\n",
    "\n",
    "Explanation: \n",
    "BCE measures the cross-entropy between predicted probabilities (y_pred) and binary ground truth features (y_true). It is commonly used in binary classification problems that aim to predict a binary outcome (eg true/false, yes/no, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def binary_cross_entropy(y_true, y_pred):\n",
    "    bce = -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "    return bce"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Categorical Cross Entropy (CCE):\n",
    "\n",
    "Math formula: \n",
    "CCE(y_true, y_pred) = -sum(y_true * log(y_pred))\n",
    "\n",
    "Explanation: \n",
    "\n",
    "CCE measures the mutual entropy between predicted probabilities (y_pred) and categorical ground truth labels (y_true) encoded with a single heat. It is often used in multi-class classification problems that aim to predict multiple mutually exclusive classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def categorical_cross_entropy(y_true, y_pred):\n",
    "    cce = -np.sum(y_true * np.log(y_pred))\n",
    "    return cce\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Hinge loss:\n",
    "\n",
    "Math formula: hinge loss(y_true, y_pred) = max(0, 1 - y_true * y_pred)\n",
    "\n",
    "Explanation: \n",
    "\n",
    "The hinge loss measures the difference between the predicted value (y_pred) and the true label (y_true) in a binary classification task. It penalizes misclassified samples with margin less than 1, encouraging the model to use larger differences between different classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def hinge_loss(y_true, y_pred):\n",
    "    hinge = np.maximum(0, 1 - y_true * y_pred)\n",
    "    return hinge"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Dice Loss:\n",
    "\n",
    "Math formula: \n",
    "\n",
    "dice loss(y_true, y_pred) = 1 - (2 * sum(y_true * y_pred) + epsilon) / (sum(y_true) + sum(y_pred) + epsilon)\n",
    "\n",
    "Explanation: \n",
    "\n",
    "Dice loss is often used in image segmentation problems. It measures the similarity between the predicted segmentation mask (y_pred) and the ground truth mask (y_true) using the cube coefficient formula. It penalizes the discrepancy between model predictions and the true mask with values ​​between 0 (no overlap) and 1 (complete overlap)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def dice_loss(y_true, y_pred, epsilon=1e-7):\n",
    "    numerator = 2 * np.sum(y_true * y_pred) + epsilon\n",
    "    denominator = np.sum(y_true) + np.sum(y_pred) + epsilon\n",
    "    dice = 1 - (numerator / denominator)\n",
    "    return dice\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Kullback-Leibler deviation (KL difference):\n",
    "\n",
    "\n",
    "Math formula: \n",
    "\n",
    "KL difference(y_true, y_pred) = sum(y_true * log(y_true / y_pred))\n",
    "\n",
    "\n",
    "Explanation: \n",
    "\n",
    "KL Divergence measures the deviation between the predicted probability distribution (y_pred) and the ground truth distribution (y_true). It is often used in probabilistic models such as variational autoencoders to measure the difference between the predicted distribution and the ground truth distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
