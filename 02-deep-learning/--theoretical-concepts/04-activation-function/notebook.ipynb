{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### About\n",
    "\n",
    "> Activation functions\n",
    "\n",
    "Activation functions are an important part of deep neural networks because they introduce non-linearity into the model, allowing it to learn complex and non-linear data patterns. Here we discuss some of the more commonly used activation functions, their mathematical formulations, and provide code examples for each function.\n",
    "\n",
    "\n",
    "1. Sigmoid Activation Function\n",
    "\n",
    "sigmoid(x) = 1 / (1 + exp(-x))\n",
    "\n",
    "\n",
    "It maps input values ​​in the range 0 to 1, making it suitable for binary classification problems. However, it suffers from the vanishing gradient problem, where the gradient can become very small for large input values, resulting in slow convergence during training. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.11920292 0.26894142 0.5        0.73105858 0.88079708]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "x = np.array([-2, -1, 0, 1, 2])\n",
    "print(sigmoid(x))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. ReLU (Rectified Linear Unit) Activation Function\n",
    "\n",
    "ReLU(x) = max(0, x)\n",
    "\n",
    "It sets all negative input values ​​to zero while sending positive input values ​​unchanged. ReLU is a popular choice of activation function due to its computational efficiency and ability to alleviate the vanishing gradient problem.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 1 2]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "x = np.array([-2, -1, 0, 1, 2])\n",
    "print(relu(x))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Leaky ReLU (LReLU) activation function\n",
    "\n",
    "The Leaky ReLU activation function is a variant of the ReLU function that addresses the \"dying ReLU\" problem, where some ReLU neurons may become inactive during training and never recover.\n",
    "\n",
    "LReLU(x) = max(α * x, x), where α is a small positive constant (typically around 0.01)\n",
    "\n",
    "\n",
    "It introduces a slight slope for negative input values, allowing some gradient flow even for negative inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.02 -0.01  0.    1.    2.  ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def leaky_relu(x, alpha=0.01):\n",
    "    return np.maximum(alpha * x, x)\n",
    "\n",
    "x = np.array([-2, -1, 0, 1, 2])\n",
    "print(leaky_relu(x))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Softmax activation function\n",
    "\n",
    "The Softmax activation function is often used in multi-class classification problems because it produces a probability distribution over multiple classes.\n",
    "\n",
    "softmax(x_i) = exp(x_i) / sum(exp(x_j)), for all i {1, 2, ..., K}\n",
    "\n",
    "\n",
    "\n",
    "where K is the number of categories and x_i is the input value of the ith category. This ensures that the output values ​​are 1, making it suitable for multi-class classification problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.01165623 0.03168492 0.08612854 0.23412166 0.63640865]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(x):\n",
    "    exp_vals = np.exp(x)\n",
    "    return exp_vals / np.sum(exp_vals)\n",
    "\n",
    "x = np.array([1, 2, 3, 4, 5])\n",
    "print(softmax(x))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Parametric ReLU (PReLU) activation function\n",
    "\n",
    "The activation function of Parametric ReLU (PReLU) is similar to Leaky ReLU, but instead of using a fixed slope for negative inputs, it allows the slope to be learned during training. The PReLU function is defined by the following mathematical formula:\n",
    "\n",
    "\n",
    "PReLU(x) = max(α * x, x), where α is a learnable parameter\n",
    "\n",
    "\n",
    "This makes the PReLU function more flexible than the Leaky ReLU function because it can adjust the negative input slope according to the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.02 -0.01  0.    1.    2.  ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def prelu(x, alpha):\n",
    "    return np.maximum(alpha * x, x)\n",
    "\n",
    "\n",
    "x = np.array([-2, -1, 0, 1, 2])\n",
    "alpha = 0.01  \n",
    "print(prelu(x, alpha))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. GELU (Gaussian Error Linear Unit) activation function\n",
    "\n",
    "The GELU activation function is a smooth approximation of the rectifier function designed to combine the best properties of the ReLU and sigmoid functions. The GELU function is defined by the following mathematical formula:\n",
    "\n",
    "GELU(x) = 0.5 * x * (1 + tanh(sqrt(2 / π) * (x + 0.044715 * x^3)))\n",
    "\n",
    "\n",
    "The GELU function is smooth and differentiable and does not have the gradient vanishing problem like the sigmoid function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.04540231 -0.15880801  0.          0.84119199  1.95459769]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def gelu(x):\n",
    "    return 0.5 * x * (1 + np.tanh(np.sqrt(2 / np.pi) * (x + 0.044715 * x**3)))\n",
    "\n",
    "x = np.array([-2, -1, 0, 1, 2])\n",
    "print(gelu(x))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. ELU (Exponential Linear Unit) activation function\n",
    "\n",
    "The ELU activation function is another smooth approximation of the rectifier function designed to mitigate the vanishing gradient problem.\n",
    "\n",
    "ELU(x) = x, if x >= 0\n",
    "       = α * (exp(x) - 1), if x < 0, where α is a positive constant\n",
    "\n",
    "\n",
    "The ELU function implements a zero output for negative inputs, providing better gradient flow during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.86466472 -0.63212056  0.          1.          2.        ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def elu(x, alpha=1.0):\n",
    "    return np.where(x >= 0, x, alpha * (np.exp(x) - 1))\n",
    "\n",
    "x = np.array([-2, -1, 0, 1, 2])\n",
    "print(elu(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
