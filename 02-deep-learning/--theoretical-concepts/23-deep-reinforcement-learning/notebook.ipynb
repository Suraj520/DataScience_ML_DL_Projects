{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### About\n",
    "\n",
    "> Deep reinforcement learning\n",
    "\n",
    "Deep reinforcement learning (DRL) is a type of machine learning that combines reinforcement learning (RL) with deep neural networks (DNN). It involves training the agent to make decisions in the environment by acting on observed states with the goal of maximizing the cumulative reward signal. Mathematically, DRL can be described using the following components:\n",
    "\n",
    "1. Markov Decision Process (MDP): MDP is a mathematical model that defines the interaction between an agent and its environment. It is defined by the string (S, A, P, R), where:\n",
    "\n",
    "- S is the state space representing all the possible states the environment can be in.\n",
    "- A is the action space representing all the possible actions an agent can take. - \"P\" is a transition probability function that determines the probability of going from one state to another when an action is performed.\n",
    "- R is the reward function that defines the immediate reward that the agent receives after performing a certain action in a certain state. \n",
    "\n",
    "2. Policy: A policy is a strategy used by an agent to decide what action to take in a given state. It can be deterministic or stochastic and is usually expressed as a function that maps states to actions.\n",
    "\n",
    "3. Value Function: A value function estimates the expected cumulative reward that an agent can obtain from a given state or state-action pair under a given policy. It is used to guide the agent's decision-making process by evaluating the long-term desirability of various states or actions. \n",
    "\n",
    "4. Q-value function: The Q-value function, also known as the action-value function, is similar to the value function, but it considers the specific action performed in addition to the state. It calculates the expected cumulative reward that an agent can obtain from a given state-action pair under a given policy.\n",
    "\n",
    "5. Bellman Equation: The Bellman equation is a key equation in reinforcement learning that expresses the relationship between the value of a state or state-action pair and the value of an adjacent state or state-action pair. It is used to update the value and Q value function during the learning process. \n",
    "\n",
    "6. Deep Neural Network (DNN): DNN is used to approximate a policy, value function or Q-value function in DRL. These are typically multilayer neural networks with multiple hidden layers that can learn complex representations from raw state or state action inputs.\n",
    "\n",
    "7. Replay Buffer: The replay buffer is a DRL data structure used to store and sample the agent's previous experience. This helps to break the temporal relationship between successive samples and improve the stability of the learning process. \n",
    "\n",
    "8. Exploration vs. Exploitation: Exploration refers to trying out different behaviors to discover their impact on the environment, while exploitation refers to current policies. Achieving a balance between exploration and exploitation is a key challenge in DRL to ensure that the agent does enough exploration to learn an optimal policy without getting stuck in a suboptimal one.\n",
    "\n",
    "9. Learning Algorithms: DRL algorithms typically use a combination of RL techniques (such as Q-learning, SARSA, or Actor-Critic) and DNNs to learn a policy, value function, or Q-value function. These algorithms update model parameters based on observed experience and the Bellman equation to optimize strategies or value estimates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the gridworld environment\n",
    "# The agent starts at position 0 and can take actions 'left' or 'right'\n",
    "# The agent receives a reward of +1 for reaching position 3 and -1 for reaching position 1\n",
    "num_states = 4\n",
    "num_actions = 2\n",
    "transitions = np.array([[0, -1, 1, -1],\n",
    "                        [-1, 0, -1, 1]])\n",
    "rewards = np.array([[0, -1, 1, -1],\n",
    "                    [-1, 0, -1, 1]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define DNN model\n",
    "model = Sequential()\n",
    "model.add(Dense(10, input_dim=1, activation='relu'))\n",
    "model.add(Dense(num_actions, activation='linear'))\n",
    "model.compile(loss='mse', optimizer=Adam(lr=0.01))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "Episode: 0, Total Reward: 0\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "Episode: 1, Total Reward: 0\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "Episode: 2, Total Reward: 0\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "Episode: 3, Total Reward: 0\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "Episode: 4, Total Reward: 0\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "Episode: 5, Total Reward: 0\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "Episode: 6, Total Reward: 0\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "Episode: 7, Total Reward: 0\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "Episode: 8, Total Reward: 0\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "Episode: 9, Total Reward: 0\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "Episode: 10, Total Reward: 0\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "Episode: 11, Total Reward: 0\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "Episode: 12, Total Reward: 0\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "Episode: 13, Total Reward: 0\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "Episode: 14, Total Reward: 0\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "Episode: 15, Total Reward: -1\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "Episode: 16, Total Reward: 0\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "Episode: 17, Total Reward: 0\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "Episode: 18, Total Reward: 0\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "Episode: 19, Total Reward: 0\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 20\n",
    "for episode in range(num_episodes):\n",
    "    state = 0\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        # Choose action based on epsilon-greedy policy\n",
    "        epsilon = 0.1\n",
    "        if np.random.rand() <= epsilon:\n",
    "            action = np.random.randint(num_actions)\n",
    "        else:\n",
    "            q_values = model.predict(np.array([[state]]))[0]\n",
    "            action = np.argmax(q_values)\n",
    "\n",
    "        # Take action and observe next state and reward\n",
    "        next_state = transitions[action, state]\n",
    "        reward = rewards[action, state]\n",
    "        total_reward += reward\n",
    "\n",
    "        # Update Q-value function using Bellman equation\n",
    "        q_values = model.predict(np.array([[state]]))[0]\n",
    "        next_q_values = model.predict(np.array([[next_state]]))[0]\n",
    "        q_values[action] = reward + np.max(next_q_values)\n",
    "\n",
    "        # Update the model with the new Q-values\n",
    "        model.fit(np.array([[state]]), np.array([q_values]), verbose=0)\n",
    "\n",
    "        # Update current state for next iteration\n",
    "        state = next_state\n",
    "\n",
    "        # Check if the episode is done\n",
    "        if state == 1 or state == 3:\n",
    "            done = True\n",
    "\n",
    "    print(\"Episode: {}, Total Reward: {}\".format(episode, total_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 40ms/step\n",
      "State: -1, Action: 1\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "State: 1, Action: 1\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "state = 0\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    # Get Q-values for current state\n",
    "    q_values = model.predict(np.array([[state]]))[0]\n",
    "\n",
    "    # Choose action with highest Q-value\n",
    "    action = np.argmax(q_values)\n",
    "\n",
    "    # Take action and observe next state and reward\n",
    "    next_state = transitions[action, state]\n",
    "    reward = rewards[action, state]\n",
    "\n",
    "    # Update current state for next iteration\n",
    "    state = next_state\n",
    "\n",
    "    # Print current state and action taken\n",
    "    print(\"State: {}, Action: {}\".format(state, action))\n",
    "\n",
    "    # Check if the episode is done\n",
    "    if state == 1 or state == 3:\n",
    "        done = True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first episode, the agent received a reward of -1 when taking action 1 from state -1, and a reward of 1 when taking action 1 from state 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
