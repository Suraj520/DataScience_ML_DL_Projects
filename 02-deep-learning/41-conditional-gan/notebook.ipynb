{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### About\n",
    "1. Conditional GANs are GANs that allow us to condition the network with additional information such as class labels. It means that during the phase of training, Images are passed to the network along with their actual class labels for it to learn the difference between them.\n",
    "2. The limitation of generating random samples with a GAN is overcome via a conditional GAN i.e control on output is maintained. For e.g - In Fashion MNIST, CGAN can help output all jacket's image or equivalent customization.\n",
    "3. The loss function of GANs quoted below\n",
    "![gan_loss.png](gan_loss.png)\n",
    "is modified by conditioning class labels as i.e conditional probabilities\n",
    "![cgan_loss.png](cgan_loss.png)\n",
    "4. In this notebook, We'll implement CGANs on Fashion MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "import os\n",
    "import torchvision.utils as utils\n",
    "import numpy as numpy\n",
    "from torchvision import  datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data/FashionMNIST/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4559c5df6fce4d7981a9f32bab86cee7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/26421880 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/FashionMNIST/FashionMNIST/raw/train-images-idx3-ubyte.gz to ./data/FashionMNIST/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae6b7d4d051b40f4b2818330d47083cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29515 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/FashionMNIST/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3efdd03b02c41f2987f2a8478961db8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4422102 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/FashionMNIST/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25fd77cf5a734969863aece5b01bd48a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5148 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/FashionMNIST/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/FashionMNIST/raw\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#downloading fashion mnist dataset\n",
    "dataset_path = os.path.join('./data', 'FashionMNIST')\n",
    "os.makedirs(dataset_path, exist_ok=True)\n",
    "model_path = os.path.join('./model', 'FashionMNIST')\n",
    "os.makedirs(model_path,exist_ok=True)\n",
    "samples_path = os.path.join('./samples','FashionMNIST')\n",
    "os.makedirs(samples_path,exist_ok=True)\n",
    "\n",
    "#defining the transform\n",
    "transform = transforms.Compose([transforms.Resize([32,32]),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize([0.5],[0.5])])\n",
    "\n",
    "dataset = datasets.FashionMNIST(dataset_path, train=True, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=dataset, batch_size=256, shuffle=True, num_workers=4, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[[[-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
      "          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
      "          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
      "          ...,\n",
      "          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
      "          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
      "          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000]]],\n",
      "\n",
      "\n",
      "        [[[-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
      "          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
      "          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
      "          ...,\n",
      "          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
      "          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
      "          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000]]],\n",
      "\n",
      "\n",
      "        [[[-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
      "          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
      "          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
      "          ...,\n",
      "          [-1.0000, -1.0000, -1.0000,  ..., -0.9922, -1.0000, -1.0000],\n",
      "          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
      "          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
      "          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
      "          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
      "          ...,\n",
      "          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
      "          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
      "          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000]]],\n",
      "\n",
      "\n",
      "        [[[-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
      "          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
      "          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
      "          ...,\n",
      "          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
      "          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
      "          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000]]],\n",
      "\n",
      "\n",
      "        [[[-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
      "          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
      "          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
      "          ...,\n",
      "          [-1.0000, -1.0000, -1.0000,  ..., -0.7804, -1.0000, -1.0000],\n",
      "          [-1.0000, -1.0000, -1.0000,  ..., -0.9137, -1.0000, -1.0000],\n",
      "          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000]]]]), tensor([0, 7, 9, 3, 3, 0, 5, 4, 7, 2, 7, 3, 8, 1, 1, 8, 8, 0, 1, 2, 7, 2, 2, 3,\n",
      "        5, 8, 1, 5, 1, 3, 5, 4, 7, 9, 1, 3, 1, 2, 2, 3, 2, 1, 4, 3, 7, 1, 8, 7,\n",
      "        4, 4, 2, 5, 3, 2, 7, 9, 6, 7, 6, 6, 8, 9, 8, 6, 3, 3, 0, 9, 9, 3, 4, 7,\n",
      "        5, 9, 8, 2, 2, 6, 8, 6, 4, 2, 3, 1, 3, 4, 8, 7, 2, 9, 0, 0, 5, 5, 7, 8,\n",
      "        9, 9, 1, 3, 5, 5, 2, 7, 4, 8, 6, 3, 6, 2, 7, 2, 1, 1, 2, 3, 5, 1, 9, 7,\n",
      "        9, 8, 2, 0, 4, 2, 0, 4, 4, 9, 3, 3, 9, 5, 0, 3, 3, 1, 0, 0, 1, 5, 2, 0,\n",
      "        4, 0, 1, 8, 3, 7, 8, 6, 5, 5, 6, 3, 0, 1, 6, 5, 8, 4, 2, 5, 9, 7, 1, 3,\n",
      "        5, 5, 7, 1, 0, 7, 5, 0, 1, 2, 9, 3, 8, 4, 4, 9, 7, 2, 2, 3, 0, 3, 7, 9,\n",
      "        8, 0, 5, 0, 4, 9, 9, 4, 8, 4, 7, 8, 5, 1, 1, 8, 4, 5, 0, 7, 8, 9, 6, 1,\n",
      "        2, 6, 3, 2, 6, 5, 7, 7, 8, 6, 0, 4, 7, 9, 9, 4, 5, 3, 6, 7, 0, 6, 2, 5,\n",
      "        7, 3, 8, 2, 6, 1, 7, 3, 9, 9, 6, 5, 0, 2, 1, 0])]\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    print(batch)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing generated images\n",
    "def generate_store_image(z,fixed_label,epoch=0):\n",
    "    #putting generator model to eval mode\n",
    "    gen.eval()\n",
    "    fake_imgs = gen(z,fixed_label)\n",
    "    fake_imgs = (fake_imgs+1)/2\n",
    "    fake_imgs_ = utils.make_grid(fake_imgs, normalize=False, nrow=10)\n",
    "    utils.save_image(fake_imgs_, os.path.join(samples_path, 'sample_'+str(epoch)+'.png'))\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Architecture \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolution_block(in_channels,out_channels, kernel=4,stride=2, pad=1,bias=False, transpose=False):\n",
    "    module= []\n",
    "    if transpose:\n",
    "        module.append(nn.ConvTranspose2d(in_channels,out_channels,kernel,stride, pad, bias=bias))\n",
    "    else:\n",
    "        module.append(nn.Conv2d(in_channels,out_channels,kernel,stride,pad,bias=bias))\n",
    "    if bias == False:\n",
    "        #use batch norm\n",
    "        module.append(nn.BatchNorm2d(out_channels))\n",
    "    \n",
    "    return nn.Sequential(*module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self,z_dim=10, num_classes=10, label_embed_size=5, channels=3, conv_dim=64):\n",
    "        super().__init__()\n",
    "        self.label_embedding = nn.Embedding(num_classes, label_embed_size)\n",
    "        self.transpose_conv1 =convolution_block(z_dim+label_embed_size,conv_dim*4, pad=0, transpose=True)\n",
    "        self.transpose_conv2 = convolution_block(conv_dim*4, conv_dim*2, transpose=True)\n",
    "        self.transpose_conv3 = convolution_block(conv_dim*2, conv_dim, transpose=True)\n",
    "        self.transpose_conv4 = convolution_block(conv_dim, channels, transpose=True,bias=True) #no batch norm\n",
    "\n",
    "        for m in self.modules():\n",
    "            #initialising weights\n",
    "            if isinstance(m,nn.Conv2d) or isinstance(m,nn.ConvTranspose2d):\n",
    "                nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "            if isinstance(m,nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight,1)\n",
    "                nn.init.constant_(m.bias,0)\n",
    "\n",
    "    #enforcing label in forward pass\n",
    "    def forward(self,x,label):\n",
    "        #reshaping x\n",
    "        x = x.reshape([x.shape[0],-1,1,1])\n",
    "        label_embed = self.label_embedding(label)\n",
    "        label_embed = label_embed.reshape([label_embed.shape[0],-1,1,1])\n",
    "        x = torch.cat((x,label_embed),dim=1)\n",
    "        x = F.relu(self.transpose_conv1(x))\n",
    "        x = F.relu(self.transpose_conv2(x))\n",
    "        x = F.relu(self.transpose_conv3(x))\n",
    "        x = torch.tanh(self.transpose_conv4(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self,num_classes=10, channels=3, conv_dim=64):\n",
    "        super().__init__()\n",
    "        self.image_size=32\n",
    "        self.label_embedding = nn.Embedding(num_classes,self.image_size*self.image_size)\n",
    "        self.conv1 = convolution_block(channels+1, conv_dim,bias=True) # no batch norm\n",
    "        self.conv2 = convolution_block(conv_dim, conv_dim*2)\n",
    "        self.conv3 = convolution_block(conv_dim*2, conv_dim*4)\n",
    "        self.conv4 = convolution_block(conv_dim*4, 1,kernel=4,stride=1,pad=0, bias=True)# no batch norm\n",
    "\n",
    "        # init weights\n",
    "        for m in self.modules():\n",
    "            if isinstance(m,nn.Conv2d):\n",
    "                nn.init.normal_(m.weight,0.0,0.02)\n",
    "\n",
    "            if isinstance(m,nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight,1)\n",
    "                nn.init.constant_(m.bias,0)\n",
    "    \n",
    "    #enforcing labels in forward pass\n",
    "    def forward(self,x,label):\n",
    "        alpha=0.2\n",
    "        label_embed = self.label_embedding(label)\n",
    "        label_embbed = label_embed.reshape([label_embed.shape[0],1,self.image_size,self.image_size])\n",
    "        x = torch.cat((x,label_embed),dim=1)\n",
    "        x = F.leaky_relu(self.conv1(x),alpha)\n",
    "        x = F.leaky_relu(self.conv2(x),alpha)\n",
    "        x = F.leaky_relu(self.conv3(x),alpha)\n",
    "        x = torch.sigmoid(self.conv4(x),alpha) # prob of real or fake\n",
    "\n",
    "        return x.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_DIM=10\n",
    "LABEL_EMBEDDING_SIZE=5\n",
    "NUM_CLASSES=10\n",
    "IMGS_TO_DISPLAY_PER_CLASS=10\n",
    "LOAD_MODEL = False\n",
    "CHANNELS=1\n",
    "EPOCHS =100\n",
    "\n",
    "gen = Generator(z_dim=Z_DIM, num_classes=NUM_CLASSES, label_embed_size=LABEL_EMBEDDING_SIZE, channels=CHANNELS)\n",
    "dis = Discriminator(num_classes=NUM_CLASSES, channels=CHANNELS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOAD_MODEL:\n",
    "    gen.load_state_dict(torch.load(os.path.join(model_path,'gen.pth')))\n",
    "    dis.load_state_dict(torch.load(os.path.join(model_path,'dis.pth')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "gen = gen.to(device)\n",
    "dis = dis.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generator(\n",
       "  (label_embedding): Embedding(10, 5)\n",
       "  (transpose_conv1): Sequential(\n",
       "    (0): ConvTranspose2d(15, 256, kernel_size=(4, 4), stride=(2, 2), bias=False)\n",
       "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (transpose_conv2): Sequential(\n",
       "    (0): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (transpose_conv3): Sequential(\n",
       "    (0): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (transpose_conv4): Sequential(\n",
       "    (0): ConvTranspose2d(64, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discriminator(\n",
       "  (label_embedding): Embedding(10, 1024)\n",
       "  (conv1): Sequential(\n",
       "    (0): Conv2d(2, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  )\n",
       "  (conv2): Sequential(\n",
       "    (0): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (conv3): Sequential(\n",
       "    (0): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (conv4): Sequential(\n",
       "    (0): Conv2d(256, 1, kernel_size=(4, 4), stride=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
