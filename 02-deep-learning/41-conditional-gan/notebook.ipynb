{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### About\n",
    "1. Conditional GANs are GANs that allow us to condition the network with additional information such as class labels. It means that during the phase of training, Images are passed to the network along with their actual class labels for it to learn the difference between them.\n",
    "2. The limitation of generating random samples with a GAN is overcome via a conditional GAN i.e control on output is maintained. For e.g - In Fashion MNIST, CGAN can help output all jacket's image or equivalent customization.\n",
    "3. The loss function of GANs quoted below\n",
    "![gan_loss.png](gan_loss.png)\n",
    "is modified by conditioning class labels as i.e conditional probabilities\n",
    "![cgan_loss.png](cgan_loss.png)\n",
    "4. In this notebook, We'll implement CGANs on Fashion MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "import os\n",
    "import torchvision.utils as utils\n",
    "import numpy as numpy\n",
    "from torchvision import  datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#downloading fashion mnist dataset\n",
    "dataset_path = os.path.join('./data', 'FashionMNIST')\n",
    "os.makedirs(dataset_path, exist_ok=True)\n",
    "model_path = os.path.join('./model', 'FashionMNIST')\n",
    "os.makedirs(model_path,exist_ok=True)\n",
    "samples_path = os.path.join('./samples','FashionMNIST')\n",
    "os.makedirs(samples_path,exist_ok=True)\n",
    "\n",
    "#defining the transform\n",
    "transform = transforms.Compose([transforms.Resize([32,32]),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize([0.5],[0.5])])\n",
    "\n",
    "dataset = datasets.FashionMNIST(dataset_path, train=True, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=dataset, batch_size=256, shuffle=True, num_workers=4, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[[[-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
      "          [-1.0000, -1.0000, -1.0000,  ..., -0.9922, -0.9843, -1.0000],\n",
      "          [-1.0000, -1.0000, -1.0000,  ..., -0.9843, -0.9608, -1.0000],\n",
      "          ...,\n",
      "          [-1.0000, -1.0000, -1.0000,  ..., -0.2471, -0.4275, -1.0000],\n",
      "          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
      "          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000]]],\n",
      "\n",
      "\n",
      "        [[[-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
      "          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
      "          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
      "          ...,\n",
      "          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
      "          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
      "          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000]]],\n",
      "\n",
      "\n",
      "        [[[-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
      "          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
      "          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
      "          ...,\n",
      "          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
      "          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
      "          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
      "          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
      "          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
      "          ...,\n",
      "          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
      "          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
      "          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000]]],\n",
      "\n",
      "\n",
      "        [[[-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
      "          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
      "          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
      "          ...,\n",
      "          [-1.0000, -1.0000, -0.9922,  ..., -1.0000, -1.0000, -1.0000],\n",
      "          [-1.0000, -1.0000, -0.9922,  ..., -1.0000, -1.0000, -1.0000],\n",
      "          [-1.0000, -1.0000, -0.9843,  ..., -1.0000, -1.0000, -1.0000]]],\n",
      "\n",
      "\n",
      "        [[[-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
      "          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
      "          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
      "          ...,\n",
      "          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
      "          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
      "          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000]]]]), tensor([5, 5, 3, 9, 2, 9, 8, 7, 3, 5, 0, 1, 8, 8, 1, 1, 0, 0, 4, 7, 7, 1, 9, 4,\n",
      "        7, 4, 8, 1, 6, 6, 6, 8, 8, 5, 5, 7, 2, 9, 7, 7, 9, 4, 3, 0, 4, 0, 3, 2,\n",
      "        4, 0, 6, 9, 4, 1, 7, 9, 7, 3, 5, 5, 6, 9, 7, 3, 1, 1, 6, 1, 3, 4, 3, 5,\n",
      "        1, 6, 9, 4, 9, 8, 1, 0, 5, 7, 8, 1, 2, 1, 9, 3, 8, 6, 1, 8, 4, 0, 9, 7,\n",
      "        0, 3, 0, 6, 7, 3, 8, 9, 7, 4, 4, 9, 9, 2, 8, 6, 4, 7, 2, 8, 1, 0, 4, 6,\n",
      "        8, 3, 5, 0, 6, 8, 2, 4, 0, 1, 2, 8, 3, 6, 6, 4, 8, 9, 1, 9, 2, 1, 2, 0,\n",
      "        5, 5, 4, 3, 0, 8, 4, 6, 3, 0, 9, 8, 1, 0, 5, 0, 8, 8, 3, 7, 3, 9, 1, 1,\n",
      "        7, 9, 0, 1, 3, 6, 5, 0, 9, 7, 7, 2, 8, 2, 5, 1, 8, 2, 6, 4, 4, 0, 5, 6,\n",
      "        5, 2, 5, 7, 1, 7, 5, 0, 3, 6, 0, 2, 7, 0, 2, 5, 2, 8, 0, 0, 5, 5, 3, 9,\n",
      "        4, 6, 4, 1, 2, 1, 7, 3, 2, 2, 8, 3, 7, 2, 1, 0, 3, 1, 5, 5, 1, 2, 9, 7,\n",
      "        4, 3, 0, 3, 0, 4, 5, 1, 0, 3, 5, 2, 1, 0, 0, 3])]\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    print(batch)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing generated images\n",
    "def generate_store_image(z,fixed_label,epoch=0):\n",
    "    #putting generator model to eval mode\n",
    "    gen.eval()\n",
    "    fake_imgs = gen(z,fixed_label)\n",
    "    fake_imgs = (fake_imgs+1)/2\n",
    "    fake_imgs_ = utils.make_grid(fake_imgs, normalize=False, nrow=10)\n",
    "    utils.save_image(fake_imgs_, os.path.join(samples_path, 'sample_'+str(epoch)+'.png'))\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Architecture \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolution_block(in_channels,out_channels, kernel=4,stride=2, pad=1,bias=False, transpose=False):\n",
    "    module= []\n",
    "    if transpose:\n",
    "        module.append(nn.ConvTranspose2d(in_channels,out_channels,kernel,stride, pad, bias=bias))\n",
    "    else:\n",
    "        module.append(nn.Conv2d(in_channels,out_channels,kernel,stride,pad,bias=bias))\n",
    "    if bias == False:\n",
    "        #use batch norm\n",
    "        module.append(nn.BatchNorm2d(out_channels))\n",
    "    \n",
    "    return nn.Sequential(*module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self,z_dim=10, num_classes=10, label_embed_size=5, channels=3, conv_dim=64):\n",
    "        super().__init__()\n",
    "        self.label_embedding = nn.Embedding(num_classes, label_embed_size)\n",
    "        self.transpose_conv1 =convolution_block(z_dim+label_embed_size,conv_dim*4, pad=0, transpose=True)\n",
    "        self.transpose_conv2 = convolution_block(conv_dim*4, conv_dim*2, transpose=True)\n",
    "        self.transpose_conv3 = convolution_block(conv_dim*2, conv_dim, transpose=True)\n",
    "        self.transpose_conv4 = convolution_block(conv_dim, channels, transpose=True,bias=True) #no batch norm\n",
    "\n",
    "        for m in self.modules():\n",
    "            #initialising weights\n",
    "            if isinstance(m,nn.Conv2d) or isinstance(m,nn.ConvTranspose2d):\n",
    "                nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "            if isinstance(m,nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight,1)\n",
    "                nn.init.constant_(m.bias,0)\n",
    "\n",
    "    #enforcing label in forward pass\n",
    "    def forward(self,x,label):\n",
    "        #reshaping x\n",
    "        x = x.reshape([x.shape[0],-1,1,1])\n",
    "        label_embed = self.label_embedding(label)\n",
    "        label_embed = label_embed.reshape([label_embed.shape[0],-1,1,1])\n",
    "        x = torch.cat((x,label_embed),dim=1)\n",
    "        x = F.relu(self.transpose_conv1(x))\n",
    "        x = F.relu(self.transpose_conv2(x))\n",
    "        x = F.relu(self.transpose_conv3(x))\n",
    "        x = torch.tanh(self.transpose_conv4(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, num_classes=10, channels=3, conv_dim=64):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.image_size = 32\n",
    "        self.label_embedding = nn.Embedding(num_classes, self.image_size*self.image_size)\n",
    "        self.conv1 = convolution_block(channels + 1, conv_dim, bias=True)\n",
    "        self.conv2 = convolution_block(conv_dim, conv_dim * 2)\n",
    "        self.conv3 = convolution_block(conv_dim * 2, conv_dim * 4)\n",
    "        self.conv4 = convolution_block(conv_dim * 4, 1, kernel=4, stride=1, pad=0, bias=True)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "\n",
    "            if isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x, label):\n",
    "        alpha = 0.2\n",
    "        label_embed = self.label_embedding(label)\n",
    "        label_embed = label_embed.reshape([label_embed.shape[0], 1, self.image_size, self.image_size])\n",
    "        x = torch.cat((x, label_embed), dim=1)\n",
    "        x = F.leaky_relu(self.conv1(x), alpha)\n",
    "        x = F.leaky_relu(self.conv2(x), alpha)\n",
    "        x = F.leaky_relu(self.conv3(x), alpha)\n",
    "        x = torch.sigmoid(self.conv4(x))\n",
    "        return x.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_DIM=10\n",
    "LABEL_EMBEDDING_SIZE=5\n",
    "NUM_CLASSES=10\n",
    "IMGS_TO_DISPLAY_PER_CLASS=10\n",
    "LOAD_MODEL = False\n",
    "CHANNELS=1\n",
    "EPOCHS =100\n",
    "BATCH_SIZE=256\n",
    "gen = Generator(z_dim=Z_DIM, num_classes=NUM_CLASSES, label_embed_size=LABEL_EMBEDDING_SIZE, channels=CHANNELS)\n",
    "dis = Discriminator(num_classes=NUM_CLASSES, channels=CHANNELS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOAD_MODEL:\n",
    "    gen.load_state_dict(torch.load(os.path.join(model_path,'gen.pth')))\n",
    "    dis.load_state_dict(torch.load(os.path.join(model_path,'dis.pth')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "gen = gen.to(device)\n",
    "dis = dis.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generator(\n",
       "  (label_embedding): Embedding(10, 5)\n",
       "  (tconv1): Sequential(\n",
       "    (0): ConvTranspose2d(15, 256, kernel_size=(4, 4), stride=(2, 2), bias=False)\n",
       "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (tconv2): Sequential(\n",
       "    (0): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (tconv3): Sequential(\n",
       "    (0): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (tconv4): Sequential(\n",
       "    (0): ConvTranspose2d(64, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discriminator(\n",
       "  (label_embedding): Embedding(10, 1024)\n",
       "  (conv1): Sequential(\n",
       "    (0): Conv2d(2, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  )\n",
       "  (conv2): Sequential(\n",
       "    (0): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (conv3): Sequential(\n",
       "    (0): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (conv4): Sequential(\n",
       "    (0): Conv2d(256, 1, kernel_size=(4, 4), stride=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "loss_function = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Optimizers\n",
    "g_opt = optim.Adam(gen.parameters(), lr=0.0002, betas=(0.5, 0.999), weight_decay=2e-5)\n",
    "d_opt = optim.Adam(dis.parameters(), lr=0.0002, betas=(0.5, 0.999), weight_decay=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixing images for visualization\n",
    "fixed_z = torch.randn(IMGS_TO_DISPLAY_PER_CLASS*NUM_CLASSES, Z_DIM)\n",
    "fixed_label = torch.arange(0, NUM_CLASSES)\n",
    "fixed_label = torch.repeat_interleave(fixed_label, IMGS_TO_DISPLAY_PER_CLASS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7,\n",
       "        7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9,\n",
       "        9, 9, 9, 9])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fixed_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels\n",
    "real_label = torch.ones(BATCH_SIZE)\n",
    "fake_label = torch.zeros(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transferring to device\n",
    "real_label, fake_label = real_label.to(device), fake_label.to(device)\n",
    "fixed_z, fixed_label = fixed_z.to(device), fixed_label.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch - 0,step-0, Discriminator Loss - 0.020337749272584915, Generator Loss - 1.454424262046814\n",
      "Epoch - 0,step-10, Discriminator Loss - 0.026353558525443077, Generator Loss - 2.975874900817871\n"
     ]
    }
   ],
   "source": [
    "# training model\n",
    "for epoch in range(EPOCHS):\n",
    "    gen.train()\n",
    "    dis.train()\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        x_real,x_label = batch\n",
    "        z_fake = torch.randn(BATCH_SIZE,Z_DIM)\n",
    "\n",
    "        x_real, x_label = x_real.to(device),x_label.to(device)\n",
    "\n",
    "        # generate fake data\n",
    "        x_fake = gen(z_fake,x_label)\n",
    "\n",
    "        # train discriminator\n",
    "        fake_out = dis(x_fake.detach(), x_label)\n",
    "        real_out = dis(x_real.detach(), x_label)\n",
    "\n",
    "        d_loss = (loss_function(fake_out,fake_label)+ loss_function(real_out,real_label))/21\n",
    "\n",
    "        d_opt.zero_grad()\n",
    "        d_loss.backward()\n",
    "        d_opt.step()\n",
    "\n",
    "        # Training generator\n",
    "        fake_out = dis(x_fake,x_label)\n",
    "        g_loss = loss_function(fake_out, real_label)\n",
    "        g_opt.zero_grad()\n",
    "        g_loss.backward()\n",
    "        g_opt.step()\n",
    "\n",
    "        if step%10==0:\n",
    "            print(\"Epoch - {},step-{}, Discriminator Loss - {}, Generator Loss - {}\".format(epoch,step,d_loss.item(),g_loss.item()))\n",
    "\n",
    "    if epoch+1%10==0:\n",
    "        torch.save(gen.state_dict(),os.path.join(model_path,'gen.pth'))\n",
    "        torch.save(dis.state_dict(),os.path.join(model_path,'dis.pth'))\n",
    "\n",
    "        generate_store_image(fixed_z, fixed_label, epoch=epoch+1)\n",
    "    \n",
    "    generate_store_image(fixed_z,fixed_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
