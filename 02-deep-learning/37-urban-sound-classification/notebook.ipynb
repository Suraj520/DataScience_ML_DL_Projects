{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### About\n",
    "Urban sound8k classification in PyTorch.\n",
    "* Dataset Link -https://www.kaggle.com/datasets/chrisfilo/urbansound8k "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing modules\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from torchaudio import transforms\n",
    "import pandas as pd\n",
    "import os\n",
    "import torchaudio\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>slice_file_name</th>\n",
       "      <th>fsID</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>salience</th>\n",
       "      <th>fold</th>\n",
       "      <th>classID</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100032-3-0-0.wav</td>\n",
       "      <td>100032</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.317551</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>dog_bark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100263-2-0-117.wav</td>\n",
       "      <td>100263</td>\n",
       "      <td>58.5</td>\n",
       "      <td>62.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100263-2-0-121.wav</td>\n",
       "      <td>100263</td>\n",
       "      <td>60.5</td>\n",
       "      <td>64.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100263-2-0-126.wav</td>\n",
       "      <td>100263</td>\n",
       "      <td>63.0</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100263-2-0-137.wav</td>\n",
       "      <td>100263</td>\n",
       "      <td>68.5</td>\n",
       "      <td>72.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      slice_file_name    fsID  start        end  salience  fold  classID  \\\n",
       "0    100032-3-0-0.wav  100032    0.0   0.317551         1     5        3   \n",
       "1  100263-2-0-117.wav  100263   58.5  62.500000         1     5        2   \n",
       "2  100263-2-0-121.wav  100263   60.5  64.500000         1     5        2   \n",
       "3  100263-2-0-126.wav  100263   63.0  67.000000         1     5        2   \n",
       "4  100263-2-0-137.wav  100263   68.5  72.500000         1     5        2   \n",
       "\n",
       "              class  \n",
       "0          dog_bark  \n",
       "1  children_playing  \n",
       "2  children_playing  \n",
       "3  children_playing  \n",
       "4  children_playing  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading the dataframe\n",
    "annotation_dir = '/home/suraj/ClickUp/Mar-Apr/data/UrbanSound8K.csv'\n",
    "df = pd.read_csv(annotation_dir)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"/home/suraj/ClickUp/Mar-Apr/data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating dataset class\n",
    "class UrbanSoundDataset(Dataset):\n",
    "    def __init__(self,annotation_file,audio_dir, transforms, target_sample_rate,num_samples):\n",
    "        self.annotations = pd.read_csv(annotation_file)\n",
    "        self.audio_dir = audio_dir\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.transforms = transforms\n",
    "        self.target_sample_rate = target_sample_rate\n",
    "        self.num_samples = num_samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations) \n",
    "    \n",
    "    def _get_audio_sample_label(self,index):\n",
    "        return self.annotations.iloc[index,6]\n",
    "    \n",
    "    def _get_audio_sample_path(self,index):\n",
    "        fold = f\"fold{self.annotations.iloc[index,5]}\"\n",
    "        path = os.path.join(self.audio_dir, fold,self.annotations.iloc[index,0])\n",
    "        return path\n",
    "    \n",
    "    def _right_pad(self, signal):\n",
    "        length_signal = signal.shape[1]\n",
    "        if length_signal< self.num_samples:\n",
    "            num_missing_samples = self.num_samples - length_signal\n",
    "            last_dim_padding = (0,num_missing_samples)\n",
    "            signal= F.pad(signal,last_dim_padding)\n",
    "        return signal\n",
    "    \n",
    "    def resample(self,signal,sample_rate):\n",
    "        if sample_rate!=self.target_sample_rate:\n",
    "            resampler = transforms.Resample(sample_rate, self.target_sample_rate)\n",
    "            signal = resampler(signal)\n",
    "        \n",
    "        return signal\n",
    "    \n",
    "    def clip_signal(self,signal):\n",
    "        if signal.shape[1] > self.num_samples:\n",
    "            signal = signal[:,:self.num_samples]\n",
    "        return signal\n",
    "    \n",
    "    def mix_signal(self,signal):\n",
    "        if signal.shape[0]>1:\n",
    "            signal = torch.mean(signal,dim=0,keepdim=True)\n",
    "        return signal\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        audio_sample_path = self._get_audio_sample_path(index)\n",
    "        label = self._get_audio_sample_label(index)\n",
    "        signal,sr = torchaudio.load(audio_sample_path)\n",
    "        signal = signal.to(self.device)\n",
    "        signal = self.resample(signal,sr)\n",
    "        signal = self.mix_signal(signal)\n",
    "        signal = self.clip_signal(signal)\n",
    "        signal = self._right_pad(signal)\n",
    "        signal = self.transforms(signal)\n",
    "        return signal,label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_rate = 22050 #22.5 KHz\n",
    "num_samples = 22050\n",
    "mel_spectogram = transforms.MelSpectrogram(\n",
    "    sample_rate=sample_rate,\n",
    "    n_fft=1024, hop_length=512, n_mels=64\n",
    ")\n",
    "\n",
    "data = UrbanSoundDataset(annotation_dir, dataset_path, mel_spectogram,sample_rate, num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 44]) 3\n"
     ]
    }
   ],
   "source": [
    "signal,label = data.__getitem__(54)\n",
    "print(signal.shape,label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1,out_channels=16,kernel_size=3,stride=1,padding=2,bias=False),\n",
    "        nn.BatchNorm2d(16),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=2))\n",
    "\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=16,out_channels=2*16,kernel_size=3,stride=1,padding=2,bias=False),\n",
    "        nn.BatchNorm2d(32),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=2))\n",
    "\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=32,out_channels=4*16,kernel_size=3,stride=1,padding=2,bias=False),\n",
    "        nn.BatchNorm2d(2*32),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=2))\n",
    "\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=2*32,out_channels=8*16,kernel_size=3,stride=1,padding=2,bias=False),\n",
    "        nn.BatchNorm2d(4*32),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=2))\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear = nn.Linear(128*5*4,10) # 10 classes\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    \n",
    "    def forward(self,input_data):\n",
    "        x = self.conv1(input_data)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear(x)\n",
    "        preds = self.softmax(logits)\n",
    "\n",
    "        return preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 16, 66, 46]             144\n",
      "       BatchNorm2d-2           [-1, 16, 66, 46]              32\n",
      "              ReLU-3           [-1, 16, 66, 46]               0\n",
      "         MaxPool2d-4           [-1, 16, 33, 23]               0\n",
      "            Conv2d-5           [-1, 32, 35, 25]           4,608\n",
      "       BatchNorm2d-6           [-1, 32, 35, 25]              64\n",
      "              ReLU-7           [-1, 32, 35, 25]               0\n",
      "         MaxPool2d-8           [-1, 32, 17, 12]               0\n",
      "            Conv2d-9           [-1, 64, 19, 14]          18,432\n",
      "      BatchNorm2d-10           [-1, 64, 19, 14]             128\n",
      "             ReLU-11           [-1, 64, 19, 14]               0\n",
      "        MaxPool2d-12             [-1, 64, 9, 7]               0\n",
      "           Conv2d-13           [-1, 128, 11, 9]          73,728\n",
      "      BatchNorm2d-14           [-1, 128, 11, 9]             256\n",
      "             ReLU-15           [-1, 128, 11, 9]               0\n",
      "        MaxPool2d-16            [-1, 128, 5, 4]               0\n",
      "          Flatten-17                 [-1, 2560]               0\n",
      "           Linear-18                   [-1, 10]          25,610\n",
      "          Softmax-19                   [-1, 10]               0\n",
      "================================================================\n",
      "Total params: 123,002\n",
      "Trainable params: 123,002\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 2.64\n",
      "Params size (MB): 0.47\n",
      "Estimated Total Size (MB): 3.12\n",
      "----------------------------------------------------------------\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Classifier()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "print(summary(model,(1,64,44)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating dataloader\n",
    "batch_size=1\n",
    "num_epochs=1\n",
    "learning_rate = 1e-4\n",
    "\n",
    "train_loader = DataLoader(data,batch_size=batch_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8732"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch-0/1,Step-0, Loss- 1.46124267578125\n",
      "Epoch-0/1,Step-1, Loss- 2.4607274532318115\n",
      "Epoch-0/1,Step-2, Loss- 2.46048641204834\n",
      "Epoch-0/1,Step-3, Loss- 2.4604403972625732\n",
      "Epoch-0/1,Step-4, Loss- 2.4391653537750244\n",
      "Epoch-0/1,Step-5, Loss- 2.45102858543396\n",
      "Epoch-0/1,Step-6, Loss- 2.4604196548461914\n",
      "Epoch-0/1,Step-7, Loss- 2.46001935005188\n",
      "Epoch-0/1,Step-8, Loss- 2.45855975151062\n",
      "Epoch-0/1,Step-9, Loss- 2.4611501693725586\n",
      "Epoch-0/1,Step-10, Loss- 2.4611477851867676\n",
      "Epoch-0/1,Step-11, Loss- 2.4597115516662598\n",
      "Epoch-0/1,Step-12, Loss- 2.4607889652252197\n",
      "Epoch-0/1,Step-13, Loss- 2.4611499309539795\n",
      "Epoch-0/1,Step-14, Loss- 1.6186721324920654\n",
      "Epoch-0/1,Step-15, Loss- 1.4825078248977661\n",
      "Epoch-0/1,Step-16, Loss- 1.4714611768722534\n",
      "Epoch-0/1,Step-17, Loss- 1.4612092971801758\n",
      "Epoch-0/1,Step-18, Loss- 1.4611539840698242\n",
      "Epoch-0/1,Step-19, Loss- 1.461150884628296\n",
      "Epoch-0/1,Step-20, Loss- 1.4650858640670776\n",
      "Epoch-0/1,Step-21, Loss- 1.4623161554336548\n",
      "Epoch-0/1,Step-22, Loss- 2.438324451446533\n",
      "Epoch-0/1,Step-23, Loss- 2.4576752185821533\n",
      "Epoch-0/1,Step-24, Loss- 2.427577495574951\n",
      "Epoch-0/1,Step-25, Loss- 2.4596195220947266\n",
      "Epoch-0/1,Step-26, Loss- 2.4578051567077637\n",
      "Epoch-0/1,Step-27, Loss- 2.445215940475464\n",
      "Epoch-0/1,Step-28, Loss- 2.450024127960205\n",
      "Epoch-0/1,Step-29, Loss- 2.4588112831115723\n",
      "Epoch-0/1,Step-30, Loss- 2.4256725311279297\n",
      "Epoch-0/1,Step-31, Loss- 2.4400267601013184\n",
      "Epoch-0/1,Step-32, Loss- 2.4402503967285156\n",
      "Epoch-0/1,Step-33, Loss- 2.4448421001434326\n",
      "Epoch-0/1,Step-34, Loss- 2.45210599899292\n",
      "Epoch-0/1,Step-35, Loss- 2.424518346786499\n",
      "Epoch-0/1,Step-36, Loss- 2.4437084197998047\n",
      "Epoch-0/1,Step-37, Loss- 2.438096046447754\n",
      "Epoch-0/1,Step-38, Loss- 2.4241480827331543\n",
      "Epoch-0/1,Step-39, Loss- 2.425507068634033\n",
      "Epoch-0/1,Step-40, Loss- 2.4605836868286133\n",
      "Epoch-0/1,Step-41, Loss- 2.4583330154418945\n",
      "Epoch-0/1,Step-42, Loss- 2.425985336303711\n",
      "Epoch-0/1,Step-43, Loss- 2.4387121200561523\n",
      "Epoch-0/1,Step-44, Loss- 2.444033622741699\n",
      "Epoch-0/1,Step-45, Loss- 2.4235265254974365\n",
      "Epoch-0/1,Step-46, Loss- 2.431762218475342\n",
      "Epoch-0/1,Step-47, Loss- 2.4249072074890137\n",
      "Epoch-0/1,Step-48, Loss- 2.447028398513794\n",
      "Epoch-0/1,Step-49, Loss- 2.451373338699341\n",
      "Epoch-0/1,Step-50, Loss- 2.4352481365203857\n",
      "Epoch-0/1,Step-51, Loss- 2.460892677307129\n",
      "Epoch-0/1,Step-52, Loss- 2.450211763381958\n",
      "Epoch-0/1,Step-53, Loss- 1.46122407913208\n",
      "Epoch-0/1,Step-54, Loss- 1.4611505270004272\n",
      "Epoch-0/1,Step-55, Loss- 1.4995867013931274\n",
      "Epoch-0/1,Step-56, Loss- 2.461137294769287\n",
      "Epoch-0/1,Step-57, Loss- 2.4611315727233887\n",
      "Epoch-0/1,Step-58, Loss- 2.4611446857452393\n",
      "Epoch-0/1,Step-59, Loss- 2.46113657951355\n",
      "Epoch-0/1,Step-60, Loss- 2.4611501693725586\n",
      "Epoch-0/1,Step-61, Loss- 2.4600396156311035\n",
      "Epoch-0/1,Step-62, Loss- 2.461146831512451\n",
      "Epoch-0/1,Step-63, Loss- 2.4611501693725586\n",
      "Epoch-0/1,Step-64, Loss- 1.4611501693725586\n",
      "Epoch-0/1,Step-65, Loss- 1.461152195930481\n",
      "Epoch-0/1,Step-66, Loss- 1.4611501693725586\n",
      "Epoch-0/1,Step-67, Loss- 2.4611499309539795\n",
      "Epoch-0/1,Step-68, Loss- 2.4611499309539795\n",
      "Epoch-0/1,Step-69, Loss- 2.4611499309539795\n",
      "Epoch-0/1,Step-70, Loss- 2.4611499309539795\n",
      "Epoch-0/1,Step-71, Loss- 2.4611499309539795\n",
      "Epoch-0/1,Step-72, Loss- 2.4611501693725586\n",
      "Epoch-0/1,Step-73, Loss- 2.461148738861084\n",
      "Epoch-0/1,Step-74, Loss- 2.4611499309539795\n",
      "Epoch-0/1,Step-75, Loss- 2.461149215698242\n",
      "Epoch-0/1,Step-76, Loss- 2.4611449241638184\n",
      "Epoch-0/1,Step-77, Loss- 2.461146354675293\n",
      "Epoch-0/1,Step-78, Loss- 2.4611499309539795\n",
      "Epoch-0/1,Step-79, Loss- 2.4611499309539795\n",
      "Epoch-0/1,Step-80, Loss- 2.4611499309539795\n",
      "Epoch-0/1,Step-81, Loss- 2.4611411094665527\n",
      "Epoch-0/1,Step-82, Loss- 2.4611454010009766\n",
      "Epoch-0/1,Step-83, Loss- 2.461149215698242\n",
      "Epoch-0/1,Step-84, Loss- 2.461149215698242\n",
      "Epoch-0/1,Step-85, Loss- 2.4611496925354004\n",
      "Epoch-0/1,Step-86, Loss- 2.4611494541168213\n",
      "Epoch-0/1,Step-87, Loss- 2.4611496925354004\n",
      "Epoch-0/1,Step-88, Loss- 2.4611499309539795\n",
      "Epoch-0/1,Step-89, Loss- 2.4611496925354004\n",
      "Epoch-0/1,Step-90, Loss- 2.4611496925354004\n",
      "Epoch-0/1,Step-91, Loss- 2.4611499309539795\n",
      "Epoch-0/1,Step-92, Loss- 2.4611499309539795\n",
      "Epoch-0/1,Step-93, Loss- 2.4611496925354004\n",
      "Epoch-0/1,Step-94, Loss- 2.4595224857330322\n",
      "Epoch-0/1,Step-95, Loss- 2.4610776901245117\n",
      "Epoch-0/1,Step-96, Loss- 2.461019515991211\n",
      "Epoch-0/1,Step-97, Loss- 2.4603309631347656\n",
      "Epoch-0/1,Step-98, Loss- 2.4611446857452393\n",
      "Epoch-0/1,Step-99, Loss- 2.4606401920318604\n",
      "Epoch-0/1,Step-100, Loss- 1.4611501693725586\n",
      "Epoch-0/1,Step-101, Loss- 1.4611502885818481\n",
      "Epoch-0/1,Step-102, Loss- 1.4611504077911377\n",
      "Epoch-0/1,Step-103, Loss- 1.4611501693725586\n",
      "Epoch-0/1,Step-104, Loss- 1.4611501693725586\n",
      "Epoch-0/1,Step-105, Loss- 1.4611502885818481\n",
      "Epoch-0/1,Step-106, Loss- 2.4425158500671387\n",
      "Epoch-0/1,Step-107, Loss- 1.465253233909607\n",
      "Epoch-0/1,Step-108, Loss- 1.4611512422561646\n",
      "Epoch-0/1,Step-109, Loss- 1.4611530303955078\n",
      "Epoch-0/1,Step-110, Loss- 1.461152195930481\n",
      "Epoch-0/1,Step-111, Loss- 1.4611502885818481\n",
      "Epoch-0/1,Step-112, Loss- 1.4611501693725586\n",
      "Epoch-0/1,Step-113, Loss- 1.461152195930481\n",
      "Epoch-0/1,Step-114, Loss- 2.4611501693725586\n",
      "Epoch-0/1,Step-115, Loss- 2.4611501693725586\n",
      "Epoch-0/1,Step-116, Loss- 2.4611501693725586\n",
      "Epoch-0/1,Step-117, Loss- 2.4611501693725586\n",
      "Epoch-0/1,Step-118, Loss- 2.461148977279663\n",
      "Epoch-0/1,Step-119, Loss- 2.4611501693725586\n",
      "Epoch-0/1,Step-120, Loss- 2.4611501693725586\n",
      "Epoch-0/1,Step-121, Loss- 2.4611501693725586\n",
      "Epoch-0/1,Step-122, Loss- 2.3886070251464844\n",
      "Epoch-0/1,Step-123, Loss- 2.3657219409942627\n",
      "Epoch-0/1,Step-124, Loss- 2.321244478225708\n",
      "Epoch-0/1,Step-125, Loss- 2.288606882095337\n",
      "Epoch-0/1,Step-126, Loss- 1.789984107017517\n",
      "Epoch-0/1,Step-127, Loss- 1.5487109422683716\n",
      "Epoch-0/1,Step-128, Loss- 1.4823311567306519\n",
      "Epoch-0/1,Step-129, Loss- 1.4673895835876465\n",
      "Epoch-0/1,Step-130, Loss- 1.4650799036026\n",
      "Epoch-0/1,Step-131, Loss- 1.4621307849884033\n",
      "Epoch-0/1,Step-132, Loss- 1.4615678787231445\n",
      "Epoch-0/1,Step-133, Loss- 1.4615710973739624\n",
      "Epoch-0/1,Step-134, Loss- 1.461318016052246\n",
      "Epoch-0/1,Step-135, Loss- 1.4612618684768677\n",
      "Epoch-0/1,Step-136, Loss- 1.4612046480178833\n",
      "Epoch-0/1,Step-137, Loss- 1.4611896276474\n",
      "Epoch-0/1,Step-138, Loss- 1.4611881971359253\n",
      "Epoch-0/1,Step-139, Loss- 1.4611693620681763\n",
      "Epoch-0/1,Step-140, Loss- 1.4611657857894897\n",
      "Epoch-0/1,Step-141, Loss- 1.4611642360687256\n",
      "Epoch-0/1,Step-142, Loss- 1.4611642360687256\n",
      "Epoch-0/1,Step-143, Loss- 1.4611566066741943\n",
      "Epoch-0/1,Step-144, Loss- 1.4611581563949585\n",
      "Epoch-0/1,Step-145, Loss- 1.4611623287200928\n",
      "Epoch-0/1,Step-146, Loss- 1.461157202720642\n",
      "Epoch-0/1,Step-147, Loss- 1.4611597061157227\n",
      "Epoch-0/1,Step-148, Loss- 1.4611543416976929\n",
      "Epoch-0/1,Step-149, Loss- 1.4611566066741943\n",
      "Epoch-0/1,Step-150, Loss- 1.4611541032791138\n",
      "Epoch-0/1,Step-151, Loss- 1.4611543416976929\n",
      "Epoch-0/1,Step-152, Loss- 1.4611541032791138\n",
      "Epoch-0/1,Step-153, Loss- 1.461153507232666\n",
      "Epoch-0/1,Step-154, Loss- 1.4611537456512451\n",
      "Epoch-0/1,Step-155, Loss- 2.4611494541168213\n",
      "Epoch-0/1,Step-156, Loss- 2.4611451625823975\n",
      "Epoch-0/1,Step-157, Loss- 2.461148977279663\n",
      "Epoch-0/1,Step-158, Loss- 2.4611501693725586\n",
      "Epoch-0/1,Step-159, Loss- 2.461127281188965\n",
      "Epoch-0/1,Step-160, Loss- 2.4591164588928223\n",
      "Epoch-0/1,Step-161, Loss- 2.461073637008667\n",
      "Epoch-0/1,Step-162, Loss- 2.4611477851867676\n",
      "Epoch-0/1,Step-163, Loss- 2.4611501693725586\n",
      "Epoch-0/1,Step-164, Loss- 2.461148262023926\n",
      "Epoch-0/1,Step-165, Loss- 2.4611401557922363\n",
      "Epoch-0/1,Step-166, Loss- 2.4611501693725586\n",
      "Epoch-0/1,Step-167, Loss- 2.461148977279663\n",
      "Epoch-0/1,Step-168, Loss- 2.461019515991211\n",
      "Epoch-0/1,Step-169, Loss- 2.461148738861084\n",
      "Epoch-0/1,Step-170, Loss- 2.4611501693725586\n",
      "Epoch-0/1,Step-171, Loss- 2.3785884380340576\n",
      "Epoch-0/1,Step-172, Loss- 2.4600579738616943\n",
      "Epoch-0/1,Step-173, Loss- 2.4610750675201416\n",
      "Epoch-0/1,Step-174, Loss- 1.6298027038574219\n",
      "Epoch-0/1,Step-175, Loss- 2.2173776626586914\n",
      "Epoch-0/1,Step-176, Loss- 2.36476731300354\n",
      "Epoch-0/1,Step-177, Loss- 1.4613511562347412\n",
      "Epoch-0/1,Step-178, Loss- 1.4790161848068237\n",
      "Epoch-0/1,Step-179, Loss- 2.098964214324951\n",
      "Epoch-0/1,Step-180, Loss- 1.5629818439483643\n",
      "Epoch-0/1,Step-181, Loss- 1.4625426530838013\n",
      "Epoch-0/1,Step-182, Loss- 1.4611812829971313\n",
      "Epoch-0/1,Step-183, Loss- 1.4611507654190063\n",
      "Epoch-0/1,Step-184, Loss- 2.3554654121398926\n",
      "Epoch-0/1,Step-185, Loss- 1.9156967401504517\n",
      "Epoch-0/1,Step-186, Loss- 1.461307406425476\n",
      "Epoch-0/1,Step-187, Loss- 1.4611502885818481\n",
      "Epoch-0/1,Step-188, Loss- 1.4611501693725586\n",
      "Epoch-0/1,Step-189, Loss- 1.4611501693725586\n",
      "Epoch-0/1,Step-190, Loss- 1.4611501693725586\n",
      "Epoch-0/1,Step-191, Loss- 1.4611501693725586\n",
      "Epoch-0/1,Step-192, Loss- 1.4611543416976929\n",
      "Epoch-0/1,Step-193, Loss- 1.4612494707107544\n",
      "Epoch-0/1,Step-194, Loss- 1.4657905101776123\n",
      "Epoch-0/1,Step-195, Loss- 1.4612884521484375\n",
      "Epoch-0/1,Step-196, Loss- 2.4611496925354004\n",
      "Epoch-0/1,Step-197, Loss- 2.46113657951355\n",
      "Epoch-0/1,Step-198, Loss- 2.4611501693725586\n",
      "Epoch-0/1,Step-199, Loss- 2.4611501693725586\n",
      "Epoch-0/1,Step-200, Loss- 2.4611501693725586\n",
      "Epoch-0/1,Step-201, Loss- 2.461148977279663\n",
      "Epoch-0/1,Step-202, Loss- 2.4611501693725586\n",
      "Epoch-0/1,Step-203, Loss- 2.4611501693725586\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_15712/2294247355.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch_dl/lib/python3.8/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    486\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m             )\n\u001b[0;32m--> 488\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    489\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         )\n",
      "\u001b[0;32m~/anaconda3/envs/torch_dl/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(num_epochs):\n",
    "    for step,data in enumerate(train_loader):\n",
    "        sample,label = data\n",
    "        sample = sample.to(device)\n",
    "        label =  label.to(device)\n",
    "\n",
    "        pred = model(sample)\n",
    "        #calculate loss\n",
    "        loss = criterion(pred,label)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(\"Epoch-{}/{},Step-{}/{}, Loss- {}\".format(i,num_epochs,step,len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),\"classifier.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
