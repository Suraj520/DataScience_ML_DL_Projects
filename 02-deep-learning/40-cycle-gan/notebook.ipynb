{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### About\n",
    " 1. CycleGAN attempts to learn a mapping from one dataset to another like a dataset of plants to dataset of faces.\n",
    " 2. It does this with two generators G and F and two discriminators Dx and Dy.\n",
    " 3. G attempts to turn dataset A into dataset B and F attempts to turn dataset B into dataset A.\n",
    " 4. Dx is used to differentiate real images sampled from Dataset A and images generated by generator F. F is updated to get better at fooling Dx\n",
    " 5. Dy is used to differentiate real images sampled from Dataset B and images generated by generator G. G is updated to get better at fooling Dy.\n",
    " 6. Introduction of cycle loss is made to ensure an image that is yielded by generator can be mapped back to original image by other generator i.e F(G(x)) - x and G(F(y)) - y. Two cycles of Generators.\n",
    " 7. Introduction of identity loss is made to help preserve tint, contrast and color shade. It ensures that when an image of target class is given, Generator yields the same image. F(x) - x and G(y) - y.\n",
    " 8. A series of conv layers and residual layers are used in generator to map one image to another.\n",
    " 9. PatchGAN architecture is used to classify images via Discriminator.\n",
    " 10. Reflection padding is used in architecture instead of zero padding to reduce artifacts.\n",
    " 11. The generator consists of encoder and decoder. It downsamples or encode the input image then interpret the encoding with series of residual blocks having skip connections. After this, Upsampling is being done to decode the representation to the size of the fake image.\n",
    " 12. Instance normalisation is used instead of batch normalisation which results in better image generated by operating across image channels.\n",
    " 13. Discriminator uses a PatchGAN in modification to the regular GAN architecture. The regular GAN maps an input image to single scalar output in range of 0 to 1 indicating the image being real or fake via probability. Whereas PatchGAN provides a matrix as output where each element of the matrix signifies whether its corresponding patch is real or fake. The receptive field of discriminator is 70*70 in Cycle GAN.\n",
    " ![cycle_gan](cycle_gan.png)\n",
    " 14. To reduce model oscillation, Replay buffer method is implemented. The discriminator is updated using a history of generated images rather than the ones produced by the latest generators. An image buffer to store previous created images is stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mounting google drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing modules\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import time\n",
    "import glob\n",
    "import random\n",
    "from PIL import Image\n",
    "import math\n",
    "import itertools\n",
    "import scipy\n",
    "import copy\n",
    "import shutil\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.utils import save_image, make_grid\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets\n",
    "import torch.optim as optim\n",
    "from torchsummary import summary\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "from IPython.display import clear_output\n",
    "%matplotlib inline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unzipping the dataset\n",
    "os.chdir('/content/drive/MyDrive/datasets/')\n",
    "!unzip apple2orange.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#utility functions\n",
    "def create_dir(path):\n",
    "    if not isinstance(path, (list,tuple)):\n",
    "        paths = [path]\n",
    "    for path in paths:\n",
    "        if not os.path.isdir(path):\n",
    "            os.makedirs(path)\n",
    "\n",
    "def convert_to_rgb(image):\n",
    "    rgb_image = Image.new(\"RGB\", image.size)\n",
    "    rgb_image.paste(image)\n",
    "    return rgb_image\n",
    "\n",
    "dataset_dir = '/home/suraj/ClickUp/Jan-Feb/data/apple2orange/'\n",
    "apple_train_dir = dataset_dir + '/trainA/'\n",
    "orange_train_dir = dataset_dir + '/trainB/'\n",
    "apple_val_dir = dataset_dir + '/testA/'\n",
    "orange_val_dir = dataset_dir + '/testB/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GanDataset(Dataset):\n",
    "    def __init__(self, trainA_path,trainB_path, transform = None, unaligned = False):\n",
    "        self.transform = transform\n",
    "        self.unaligned = unaligned\n",
    "\n",
    "        self.file_names_A = sorted(glob.glob(trainA_path+\"*.*\"))\n",
    "        self.file_names_B = sorted(glob.glob(trainB_path+\"*.*\"))\n",
    "\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        image_A = Image.open(self.file_names_A[index])\n",
    "        if self.unaligned:\n",
    "            image_B = Image.open(self.file_names_B[random.randint(0,len(self.file_names_B)-1)])\n",
    "        else:\n",
    "            image_B = Image.open(self.file_names_B[index])\n",
    "        # if image is grayscale , then convert to RGB\n",
    "        if image_A.mode!=\"RGB\":\n",
    "            image_A = convert_to_rgb(image_A)\n",
    "        if image_B.mode!=\"RGB\":\n",
    "            image_B = convert_to_rgb(image_B)\n",
    "\n",
    "        if self.transform:\n",
    "            image_A = self.transform(image_A)\n",
    "            image_B = self.transform(image_B)\n",
    "\n",
    "        return {\"first_image\":image_A, \"second_image\":image_B}\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return max(len(self.file_names_A),len(self.file_names_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/suraj/anaconda3/envs/torch_dl/lib/python3.8/site-packages/torchvision/transforms/transforms.py:329: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128), Image.BICUBIC),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "\n",
    "train_dataset = GanDataset(apple_train_dir,orange_train_dir,transform= transform)\n",
    "val_dataset = GanDataset(apple_val_dir,orange_val_dir,transform= transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'first_image': tensor([[[-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
       "          ...,\n",
       "          [-1.0000, -1.0000, -1.0000,  ..., -0.9059, -0.7725, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -0.9922]],\n",
       " \n",
       "         [[-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
       "          ...,\n",
       "          [-1.0000, -1.0000, -1.0000,  ..., -0.9059, -0.7725, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -0.9922]],\n",
       " \n",
       "         [[-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
       "          ...,\n",
       "          [-1.0000, -1.0000, -1.0000,  ..., -0.9059, -0.7725, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -0.9922]]]),\n",
       " 'second_image': tensor([[[-0.4980, -0.2627, -0.0510,  ..., -0.4118, -0.4353, -0.4745],\n",
       "          [-0.1686, -0.0588,  0.1216,  ..., -0.4353, -0.4588, -0.4510],\n",
       "          [ 0.3961, -0.1059, -0.3412,  ..., -0.4353, -0.4588, -0.4275],\n",
       "          ...,\n",
       "          [ 0.3569,  0.3412,  0.3725,  ..., -0.2314, -0.2078, -0.1843],\n",
       "          [ 0.2941,  0.2863,  0.3412,  ..., -0.2235, -0.2314, -0.2078],\n",
       "          [ 0.3098,  0.3020,  0.3569,  ..., -0.2000, -0.2314, -0.2078]],\n",
       " \n",
       "         [[-0.3255, -0.0980,  0.1451,  ..., -0.0902, -0.1529, -0.1843],\n",
       "          [-0.0745,  0.0745,  0.2549,  ..., -0.0980, -0.1686, -0.1765],\n",
       "          [ 0.2863, -0.0275, -0.1922,  ..., -0.1216, -0.1686, -0.1294],\n",
       "          ...,\n",
       "          [-0.1373, -0.1294, -0.1216,  ..., -0.7255, -0.7176, -0.7176],\n",
       "          [-0.1765, -0.1843, -0.1373,  ..., -0.7333, -0.7333, -0.7333],\n",
       "          [-0.1451, -0.1137, -0.1216,  ..., -0.7412, -0.7333, -0.7412]],\n",
       " \n",
       "         [[-0.3020, -0.0667,  0.2471,  ..., -0.3804, -0.3098, -0.3647],\n",
       "          [-0.1294,  0.1294,  0.4039,  ..., -0.3255, -0.3176, -0.3804],\n",
       "          [ 0.2863, -0.1059, -0.2392,  ..., -0.2941, -0.3490, -0.4118],\n",
       "          ...,\n",
       "          [-0.1373, -0.1216, -0.1059,  ..., -0.6863, -0.7255, -0.7176],\n",
       "          [-0.1686, -0.1529, -0.0980,  ..., -0.6941, -0.6784, -0.6706],\n",
       "          [-0.1686, -0.1216, -0.1216,  ..., -0.6784, -0.7176, -0.6863]]])}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.__getitem__(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'first_image': tensor([[[0.5059, 0.5059, 0.5059,  ..., 0.4980, 0.4980, 0.4980],\n",
       "          [0.5059, 0.5059, 0.5059,  ..., 0.4980, 0.4980, 0.4980],\n",
       "          [0.5059, 0.5059, 0.5059,  ..., 0.4980, 0.4980, 0.4980],\n",
       "          ...,\n",
       "          [0.5059, 0.5059, 0.5059,  ..., 0.5059, 0.5059, 0.5059],\n",
       "          [0.5059, 0.5059, 0.5059,  ..., 0.5059, 0.5059, 0.5059],\n",
       "          [0.5059, 0.5059, 0.5059,  ..., 0.5059, 0.5059, 0.5059]],\n",
       " \n",
       "         [[0.5059, 0.5059, 0.5059,  ..., 0.5137, 0.5137, 0.5137],\n",
       "          [0.5059, 0.5059, 0.5059,  ..., 0.5137, 0.5137, 0.5137],\n",
       "          [0.5059, 0.5059, 0.5059,  ..., 0.5137, 0.5137, 0.5137],\n",
       "          ...,\n",
       "          [0.5059, 0.5059, 0.5059,  ..., 0.5059, 0.5059, 0.5059],\n",
       "          [0.5059, 0.5059, 0.5059,  ..., 0.5059, 0.5059, 0.5059],\n",
       "          [0.5059, 0.5059, 0.5059,  ..., 0.5059, 0.5059, 0.5059]],\n",
       " \n",
       "         [[0.5059, 0.5059, 0.5059,  ..., 0.5059, 0.5059, 0.5059],\n",
       "          [0.5059, 0.5059, 0.5059,  ..., 0.5059, 0.5059, 0.5059],\n",
       "          [0.5059, 0.5059, 0.5059,  ..., 0.5059, 0.5059, 0.5059],\n",
       "          ...,\n",
       "          [0.4902, 0.4902, 0.4902,  ..., 0.5059, 0.5059, 0.5059],\n",
       "          [0.4902, 0.4902, 0.4902,  ..., 0.5059, 0.5059, 0.5059],\n",
       "          [0.4902, 0.4902, 0.4902,  ..., 0.5059, 0.5059, 0.5059]]]),\n",
       " 'second_image': tensor([[[ 0.9686,  0.9922,  0.9843,  ...,  0.9686,  0.9686,  0.9686],\n",
       "          [ 0.9843,  0.9765,  0.9765,  ...,  0.9686,  0.9686,  0.9529],\n",
       "          [ 0.9843,  0.9765,  0.9843,  ...,  0.9686,  0.9529,  0.9529],\n",
       "          ...,\n",
       "          [ 0.9765,  0.9843,  0.9686,  ...,  0.5294,  0.4902,  0.6314],\n",
       "          [ 0.9765,  0.9843,  0.9843,  ...,  0.4196,  0.4824,  0.4353],\n",
       "          [ 0.9765,  0.9686,  0.9843,  ...,  0.4431,  0.5216,  0.4824]],\n",
       " \n",
       "         [[ 0.3647,  0.2941,  0.2627,  ...,  0.0275, -0.0118, -0.0824],\n",
       "          [ 0.2941,  0.2235,  0.2314,  ...,  0.0902,  0.1529, -0.0510],\n",
       "          [ 0.2549,  0.2314,  0.2314,  ...,  0.0980,  0.1373, -0.0039],\n",
       "          ...,\n",
       "          [ 0.0980,  0.1608,  0.2235,  ..., -0.6078, -0.7333, -0.6078],\n",
       "          [ 0.1922,  0.1216,  0.1451,  ..., -0.9216, -0.7490, -0.8980],\n",
       "          [ 0.1294,  0.1529,  0.1216,  ..., -0.8824, -0.6863, -0.8431]],\n",
       " \n",
       "         [[-0.9765, -0.9765, -0.9843,  ..., -0.9686, -0.9843, -0.9843],\n",
       "          [-0.9765, -0.9843, -0.9765,  ..., -0.9843, -0.9373, -0.9765],\n",
       "          [-0.9843, -0.9922, -0.9843,  ..., -0.9765, -0.9529, -0.9686],\n",
       "          ...,\n",
       "          [-0.9765, -0.9765, -0.9922,  ..., -0.9608, -0.9529, -0.9686],\n",
       "          [-1.0000, -0.9765, -0.9765,  ..., -0.9608, -0.9529, -0.9608],\n",
       "          [-0.9765, -0.9922, -0.9843,  ..., -0.9608, -0.9765, -0.9686]]])}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dataset.__getitem__(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating data laoder\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset,batch_size=32,shuffle=True,num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 128, 128]) torch.Size([32, 3, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    print(batch['first_image'].shape, batch['second_image'].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualizing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
