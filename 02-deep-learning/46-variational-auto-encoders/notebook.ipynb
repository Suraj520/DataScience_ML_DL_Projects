{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### About\n",
    "> VAE\n",
    "- Variational Autoencoder (VAE) is a type of artificial neural network used for unsupervised learning that can generate new data based on the input data. It is an extension of the basic autoencoder (AE) architecture that is used to learn a compressed representation of the input data.\n",
    "- The key difference between VAEs and AEs is that VAEs are probabilistic models that learn the probability distribution of the input data, while AEs learn the deterministic mapping between the input and the output. VAEs also have an encoder-decoder architecture, but they are trained to optimize a lower bound on the likelihood of the data given a prior distribution, typically a Gaussian distribution, in the latent space. The resulting compressed representation is not only learned to be useful for reconstruction but also to generate new data points in the latent space.\n",
    "- The main use case of VAEs is in generative modeling, where they can be used to generate new data points that are similar to the training data. This is particularly useful in domains where data is scarce, and it can be challenging to collect large amounts of labeled data for supervised learning.\n",
    "- The KL divergence is often used as a regularization term in variational autoencoders (VAEs) to encourage the learned latent space to follow a specific prior distribution, typically a Gaussian distribution. In this case, the KL divergence measures the distance between the learned latent distribution and the desired prior distribution.\n",
    "- Kullback-Leibler (KL) divergence is a measure of the difference between two probability distributions. It is often used in information theory and statistics to compare a model distribution to an actual distribution. KL divergence measures the amount of information lost when one distribution is used to approximate another distribution.\n",
    "- The VAE architecture takes an input image, maps it to a lower-dimensional latent space, samples a latent variable from a learned distribution in that space, and then maps the sampled variable back to the original input space to generate a reconstructed image. The output of the VAE is not just the reconstructed image but also the mean and variance of the learned latent distribution, which are used to compute the KL divergence term in the loss function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Image Reconstruction.\n",
    "\n",
    "> Architecture Description\n",
    "\n",
    "1. The encoder is responsible for mapping the input image x to a lower-dimensional latent space representation z. In this architecture, the encoder is a feedforward neural network consisting of one fully connected layer with 400 hidden units followed by a rectified linear unit (ReLU) activation function. The output of this layer is split into two separate linear layers: the first one, fc21, outputs the mean of the latent distribution, while the second one, fc22, outputs the logarithm of the variance of the latent distribution.\n",
    "2. The reparameterization module is responsible for sampling a latent variable z from the learned distribution represented by the mean and variance outputted by the encoder. In this architecture, the reparameterization module first computes the standard deviation of the latent distribution by exponentiating half of the logarithm of the variance outputted by the encoder. Then, it samples a latent variable z from a standard normal distribution and scales it by the computed standard deviation, adds the mean of the latent distribution, and returns the resulting z as the latent variable.\n",
    "3. The decoder is responsible for mapping the latent variable z back to the original input space. In this architecture, the decoder is a feedforward neural network consisting of one fully connected layer with 400 hidden units followed by a ReLU activation function, and another fully connected layer with 784 units followed by a sigmoid activation function. The output of the second layer is the reconstructed image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VAE architecture\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 400)\n",
    "        self.fc21 = nn.Linear(400, 20) # mean\n",
    "        self.fc22 = nn.Linear(400, 20) # variance\n",
    "        self.fc3 = nn.Linear(20, 400)\n",
    "        self.fc4 = nn.Linear(400, 784)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        return self.fc21(h1), self.fc22(h1) # returns mean and variance\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps.mul(std).add(mu) # returns sampled latent variable z\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = F.relu(self.fc3(z))\n",
    "        return torch.sigmoid(self.fc4(h3)) # returns reconstructed image\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x.view(-1, 784))\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function - BCE with regularisation\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 784), reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + KLD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the training loop\n",
    "def train(model, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        loss = loss_function(recon_batch, data, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader),\n",
    "                loss.item() / len(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79f68d38e1f7490d91f4a55fb54881f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9912422 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d492d1cd32004a1881127381bbca14e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28881 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52b779954f404302985246dded168db0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1648877 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ab4a64170bd45f888f7ee9e9efd7b47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4542 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the MNIST dataset\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "train_set = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_set, batch_size=128, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the VAE and optimizer\n",
    "model = VAE()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 113.789513\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 107.989738\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 103.629074\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 107.575668\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 107.880119\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 106.521118\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 107.928642\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 104.636993\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 104.111397\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 108.849319\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 105.204422\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 101.997269\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 105.190674\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 104.183670\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 107.010544\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 107.345474\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 105.362755\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 107.639160\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 106.976318\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 106.916389\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 108.273804\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 108.729263\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 106.964798\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 104.176262\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 108.910721\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 104.833885\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 104.593781\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 107.691833\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 100.479706\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 107.848495\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 102.931137\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 105.950905\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 106.210838\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 101.167259\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 109.778580\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 104.164993\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 106.204041\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 102.522400\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 105.140793\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 105.325394\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 105.152512\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 106.872116\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 103.623779\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 104.491089\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 103.496300\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 103.360809\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 104.636185\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 104.388130\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 108.448990\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 103.922569\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 103.551743\n",
      "Train Epoch: 11 [12800/60000 (21%)]\tLoss: 104.259628\n",
      "Train Epoch: 11 [25600/60000 (43%)]\tLoss: 103.722420\n",
      "Train Epoch: 11 [38400/60000 (64%)]\tLoss: 104.039894\n",
      "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 106.513664\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 104.072357\n",
      "Train Epoch: 12 [12800/60000 (21%)]\tLoss: 105.611649\n",
      "Train Epoch: 12 [25600/60000 (43%)]\tLoss: 101.820198\n",
      "Train Epoch: 12 [38400/60000 (64%)]\tLoss: 104.158005\n",
      "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 103.179443\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 106.381874\n",
      "Train Epoch: 13 [12800/60000 (21%)]\tLoss: 103.349770\n",
      "Train Epoch: 13 [25600/60000 (43%)]\tLoss: 104.353867\n",
      "Train Epoch: 13 [38400/60000 (64%)]\tLoss: 103.378143\n",
      "Train Epoch: 13 [51200/60000 (85%)]\tLoss: 105.143013\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 101.337112\n",
      "Train Epoch: 14 [12800/60000 (21%)]\tLoss: 101.341568\n",
      "Train Epoch: 14 [25600/60000 (43%)]\tLoss: 100.993988\n",
      "Train Epoch: 14 [38400/60000 (64%)]\tLoss: 102.648575\n",
      "Train Epoch: 14 [51200/60000 (85%)]\tLoss: 97.731186\n",
      "Train Epoch: 15 [0/60000 (0%)]\tLoss: 103.106277\n",
      "Train Epoch: 15 [12800/60000 (21%)]\tLoss: 103.266151\n",
      "Train Epoch: 15 [25600/60000 (43%)]\tLoss: 98.691864\n",
      "Train Epoch: 15 [38400/60000 (64%)]\tLoss: 104.157051\n",
      "Train Epoch: 15 [51200/60000 (85%)]\tLoss: 106.129898\n",
      "Train Epoch: 16 [0/60000 (0%)]\tLoss: 100.889824\n",
      "Train Epoch: 16 [12800/60000 (21%)]\tLoss: 108.082069\n",
      "Train Epoch: 16 [25600/60000 (43%)]\tLoss: 103.606697\n",
      "Train Epoch: 16 [38400/60000 (64%)]\tLoss: 103.259598\n",
      "Train Epoch: 16 [51200/60000 (85%)]\tLoss: 103.634201\n",
      "Train Epoch: 17 [0/60000 (0%)]\tLoss: 103.630753\n",
      "Train Epoch: 17 [12800/60000 (21%)]\tLoss: 106.082451\n",
      "Train Epoch: 17 [25600/60000 (43%)]\tLoss: 103.781998\n",
      "Train Epoch: 17 [38400/60000 (64%)]\tLoss: 103.657082\n",
      "Train Epoch: 17 [51200/60000 (85%)]\tLoss: 102.422867\n",
      "Train Epoch: 18 [0/60000 (0%)]\tLoss: 103.595154\n",
      "Train Epoch: 18 [12800/60000 (21%)]\tLoss: 104.676575\n",
      "Train Epoch: 18 [25600/60000 (43%)]\tLoss: 102.933487\n",
      "Train Epoch: 18 [38400/60000 (64%)]\tLoss: 105.876480\n",
      "Train Epoch: 18 [51200/60000 (85%)]\tLoss: 102.620529\n",
      "Train Epoch: 19 [0/60000 (0%)]\tLoss: 103.075211\n",
      "Train Epoch: 19 [12800/60000 (21%)]\tLoss: 105.868584\n",
      "Train Epoch: 19 [25600/60000 (43%)]\tLoss: 105.480858\n",
      "Train Epoch: 19 [38400/60000 (64%)]\tLoss: 104.916290\n",
      "Train Epoch: 19 [51200/60000 (85%)]\tLoss: 102.555740\n",
      "Train Epoch: 20 [0/60000 (0%)]\tLoss: 100.811981\n",
      "Train Epoch: 20 [12800/60000 (21%)]\tLoss: 103.635651\n",
      "Train Epoch: 20 [25600/60000 (43%)]\tLoss: 99.397827\n",
      "Train Epoch: 20 [38400/60000 (64%)]\tLoss: 105.506912\n",
      "Train Epoch: 20 [51200/60000 (85%)]\tLoss: 104.501671\n",
      "Train Epoch: 21 [0/60000 (0%)]\tLoss: 100.162704\n",
      "Train Epoch: 21 [12800/60000 (21%)]\tLoss: 105.093506\n",
      "Train Epoch: 21 [25600/60000 (43%)]\tLoss: 103.218132\n",
      "Train Epoch: 21 [38400/60000 (64%)]\tLoss: 101.656334\n",
      "Train Epoch: 21 [51200/60000 (85%)]\tLoss: 104.383888\n",
      "Train Epoch: 22 [0/60000 (0%)]\tLoss: 102.309593\n",
      "Train Epoch: 22 [12800/60000 (21%)]\tLoss: 104.429314\n",
      "Train Epoch: 22 [25600/60000 (43%)]\tLoss: 103.120438\n",
      "Train Epoch: 22 [38400/60000 (64%)]\tLoss: 103.383209\n",
      "Train Epoch: 22 [51200/60000 (85%)]\tLoss: 103.010719\n",
      "Train Epoch: 23 [0/60000 (0%)]\tLoss: 99.919998\n",
      "Train Epoch: 23 [12800/60000 (21%)]\tLoss: 105.802338\n",
      "Train Epoch: 23 [25600/60000 (43%)]\tLoss: 102.495537\n",
      "Train Epoch: 23 [38400/60000 (64%)]\tLoss: 104.356277\n",
      "Train Epoch: 23 [51200/60000 (85%)]\tLoss: 104.059128\n",
      "Train Epoch: 24 [0/60000 (0%)]\tLoss: 103.600952\n",
      "Train Epoch: 24 [12800/60000 (21%)]\tLoss: 103.837318\n",
      "Train Epoch: 24 [25600/60000 (43%)]\tLoss: 100.499985\n",
      "Train Epoch: 24 [38400/60000 (64%)]\tLoss: 99.081436\n",
      "Train Epoch: 24 [51200/60000 (85%)]\tLoss: 102.600021\n",
      "Train Epoch: 25 [0/60000 (0%)]\tLoss: 104.527573\n",
      "Train Epoch: 25 [12800/60000 (21%)]\tLoss: 102.735321\n",
      "Train Epoch: 25 [25600/60000 (43%)]\tLoss: 101.411804\n",
      "Train Epoch: 25 [38400/60000 (64%)]\tLoss: 103.336357\n",
      "Train Epoch: 25 [51200/60000 (85%)]\tLoss: 99.553543\n",
      "Train Epoch: 26 [0/60000 (0%)]\tLoss: 105.708702\n",
      "Train Epoch: 26 [12800/60000 (21%)]\tLoss: 104.833702\n",
      "Train Epoch: 26 [25600/60000 (43%)]\tLoss: 99.938705\n",
      "Train Epoch: 26 [38400/60000 (64%)]\tLoss: 102.994904\n",
      "Train Epoch: 26 [51200/60000 (85%)]\tLoss: 101.979088\n",
      "Train Epoch: 27 [0/60000 (0%)]\tLoss: 99.507652\n",
      "Train Epoch: 27 [12800/60000 (21%)]\tLoss: 106.254333\n",
      "Train Epoch: 27 [25600/60000 (43%)]\tLoss: 103.294418\n",
      "Train Epoch: 27 [38400/60000 (64%)]\tLoss: 105.880585\n",
      "Train Epoch: 27 [51200/60000 (85%)]\tLoss: 102.335526\n",
      "Train Epoch: 28 [0/60000 (0%)]\tLoss: 103.424057\n",
      "Train Epoch: 28 [12800/60000 (21%)]\tLoss: 100.133301\n",
      "Train Epoch: 28 [25600/60000 (43%)]\tLoss: 101.762321\n",
      "Train Epoch: 28 [38400/60000 (64%)]\tLoss: 103.399612\n",
      "Train Epoch: 28 [51200/60000 (85%)]\tLoss: 100.859512\n",
      "Train Epoch: 29 [0/60000 (0%)]\tLoss: 102.318756\n",
      "Train Epoch: 29 [12800/60000 (21%)]\tLoss: 98.938713\n",
      "Train Epoch: 29 [25600/60000 (43%)]\tLoss: 101.246849\n",
      "Train Epoch: 29 [38400/60000 (64%)]\tLoss: 104.456261\n",
      "Train Epoch: 29 [51200/60000 (85%)]\tLoss: 101.887253\n",
      "Train Epoch: 30 [0/60000 (0%)]\tLoss: 103.994316\n",
      "Train Epoch: 30 [12800/60000 (21%)]\tLoss: 104.029091\n",
      "Train Epoch: 30 [25600/60000 (43%)]\tLoss: 103.864227\n",
      "Train Epoch: 30 [38400/60000 (64%)]\tLoss: 105.486649\n",
      "Train Epoch: 30 [51200/60000 (85%)]\tLoss: 102.468712\n",
      "Train Epoch: 31 [0/60000 (0%)]\tLoss: 101.805725\n",
      "Train Epoch: 31 [12800/60000 (21%)]\tLoss: 103.687729\n",
      "Train Epoch: 31 [25600/60000 (43%)]\tLoss: 99.748192\n",
      "Train Epoch: 31 [38400/60000 (64%)]\tLoss: 103.582619\n",
      "Train Epoch: 31 [51200/60000 (85%)]\tLoss: 104.911667\n",
      "Train Epoch: 32 [0/60000 (0%)]\tLoss: 104.329811\n",
      "Train Epoch: 32 [12800/60000 (21%)]\tLoss: 100.532303\n",
      "Train Epoch: 32 [25600/60000 (43%)]\tLoss: 99.492226\n",
      "Train Epoch: 32 [38400/60000 (64%)]\tLoss: 102.084991\n",
      "Train Epoch: 32 [51200/60000 (85%)]\tLoss: 104.227570\n",
      "Train Epoch: 33 [0/60000 (0%)]\tLoss: 105.547585\n",
      "Train Epoch: 33 [12800/60000 (21%)]\tLoss: 101.114952\n",
      "Train Epoch: 33 [25600/60000 (43%)]\tLoss: 103.372940\n",
      "Train Epoch: 33 [38400/60000 (64%)]\tLoss: 103.137604\n",
      "Train Epoch: 33 [51200/60000 (85%)]\tLoss: 100.129227\n",
      "Train Epoch: 34 [0/60000 (0%)]\tLoss: 101.617355\n",
      "Train Epoch: 34 [12800/60000 (21%)]\tLoss: 102.566139\n",
      "Train Epoch: 34 [25600/60000 (43%)]\tLoss: 100.354012\n",
      "Train Epoch: 34 [38400/60000 (64%)]\tLoss: 103.118973\n",
      "Train Epoch: 34 [51200/60000 (85%)]\tLoss: 102.639801\n",
      "Train Epoch: 35 [0/60000 (0%)]\tLoss: 103.338860\n",
      "Train Epoch: 35 [12800/60000 (21%)]\tLoss: 102.151802\n",
      "Train Epoch: 35 [25600/60000 (43%)]\tLoss: 99.169037\n",
      "Train Epoch: 35 [38400/60000 (64%)]\tLoss: 99.915504\n",
      "Train Epoch: 35 [51200/60000 (85%)]\tLoss: 104.076057\n",
      "Train Epoch: 36 [0/60000 (0%)]\tLoss: 106.565628\n",
      "Train Epoch: 36 [12800/60000 (21%)]\tLoss: 101.812233\n",
      "Train Epoch: 36 [25600/60000 (43%)]\tLoss: 102.083176\n",
      "Train Epoch: 36 [38400/60000 (64%)]\tLoss: 101.795067\n",
      "Train Epoch: 36 [51200/60000 (85%)]\tLoss: 96.791206\n",
      "Train Epoch: 37 [0/60000 (0%)]\tLoss: 103.590935\n",
      "Train Epoch: 37 [12800/60000 (21%)]\tLoss: 99.766647\n",
      "Train Epoch: 37 [25600/60000 (43%)]\tLoss: 100.350525\n",
      "Train Epoch: 37 [38400/60000 (64%)]\tLoss: 102.915573\n",
      "Train Epoch: 37 [51200/60000 (85%)]\tLoss: 106.999184\n",
      "Train Epoch: 38 [0/60000 (0%)]\tLoss: 104.794960\n",
      "Train Epoch: 38 [12800/60000 (21%)]\tLoss: 100.501427\n",
      "Train Epoch: 38 [25600/60000 (43%)]\tLoss: 103.356056\n",
      "Train Epoch: 38 [38400/60000 (64%)]\tLoss: 98.594025\n",
      "Train Epoch: 38 [51200/60000 (85%)]\tLoss: 104.921127\n",
      "Train Epoch: 39 [0/60000 (0%)]\tLoss: 101.400581\n",
      "Train Epoch: 39 [12800/60000 (21%)]\tLoss: 101.019295\n",
      "Train Epoch: 39 [25600/60000 (43%)]\tLoss: 98.656586\n",
      "Train Epoch: 39 [38400/60000 (64%)]\tLoss: 102.982262\n",
      "Train Epoch: 39 [51200/60000 (85%)]\tLoss: 102.362946\n",
      "Train Epoch: 40 [0/60000 (0%)]\tLoss: 99.105423\n",
      "Train Epoch: 40 [12800/60000 (21%)]\tLoss: 99.654541\n",
      "Train Epoch: 40 [25600/60000 (43%)]\tLoss: 102.103134\n",
      "Train Epoch: 40 [38400/60000 (64%)]\tLoss: 93.865730\n",
      "Train Epoch: 40 [51200/60000 (85%)]\tLoss: 105.179077\n",
      "Train Epoch: 41 [0/60000 (0%)]\tLoss: 100.982895\n",
      "Train Epoch: 41 [12800/60000 (21%)]\tLoss: 101.977707\n",
      "Train Epoch: 41 [25600/60000 (43%)]\tLoss: 102.243271\n",
      "Train Epoch: 41 [38400/60000 (64%)]\tLoss: 101.888214\n",
      "Train Epoch: 41 [51200/60000 (85%)]\tLoss: 103.057228\n",
      "Train Epoch: 42 [0/60000 (0%)]\tLoss: 102.800217\n",
      "Train Epoch: 42 [12800/60000 (21%)]\tLoss: 102.360962\n",
      "Train Epoch: 42 [25600/60000 (43%)]\tLoss: 98.047958\n",
      "Train Epoch: 42 [38400/60000 (64%)]\tLoss: 104.380325\n",
      "Train Epoch: 42 [51200/60000 (85%)]\tLoss: 101.676445\n",
      "Train Epoch: 43 [0/60000 (0%)]\tLoss: 100.490875\n",
      "Train Epoch: 43 [12800/60000 (21%)]\tLoss: 99.322685\n",
      "Train Epoch: 43 [25600/60000 (43%)]\tLoss: 99.517090\n",
      "Train Epoch: 43 [38400/60000 (64%)]\tLoss: 101.405396\n",
      "Train Epoch: 43 [51200/60000 (85%)]\tLoss: 102.353867\n",
      "Train Epoch: 44 [0/60000 (0%)]\tLoss: 99.505142\n",
      "Train Epoch: 44 [12800/60000 (21%)]\tLoss: 101.920883\n",
      "Train Epoch: 44 [25600/60000 (43%)]\tLoss: 102.357590\n",
      "Train Epoch: 44 [38400/60000 (64%)]\tLoss: 102.261505\n",
      "Train Epoch: 44 [51200/60000 (85%)]\tLoss: 102.244415\n",
      "Train Epoch: 45 [0/60000 (0%)]\tLoss: 102.328094\n",
      "Train Epoch: 45 [12800/60000 (21%)]\tLoss: 103.007652\n",
      "Train Epoch: 45 [25600/60000 (43%)]\tLoss: 103.911964\n",
      "Train Epoch: 45 [38400/60000 (64%)]\tLoss: 98.719040\n",
      "Train Epoch: 45 [51200/60000 (85%)]\tLoss: 104.782707\n",
      "Train Epoch: 46 [0/60000 (0%)]\tLoss: 101.423080\n",
      "Train Epoch: 46 [12800/60000 (21%)]\tLoss: 104.625137\n",
      "Train Epoch: 46 [25600/60000 (43%)]\tLoss: 101.360756\n",
      "Train Epoch: 46 [38400/60000 (64%)]\tLoss: 102.793427\n",
      "Train Epoch: 46 [51200/60000 (85%)]\tLoss: 104.677246\n",
      "Train Epoch: 47 [0/60000 (0%)]\tLoss: 99.825058\n",
      "Train Epoch: 47 [12800/60000 (21%)]\tLoss: 102.542892\n",
      "Train Epoch: 47 [25600/60000 (43%)]\tLoss: 100.851006\n",
      "Train Epoch: 47 [38400/60000 (64%)]\tLoss: 102.489677\n",
      "Train Epoch: 47 [51200/60000 (85%)]\tLoss: 101.983276\n",
      "Train Epoch: 48 [0/60000 (0%)]\tLoss: 100.036430\n",
      "Train Epoch: 48 [12800/60000 (21%)]\tLoss: 99.679039\n",
      "Train Epoch: 48 [25600/60000 (43%)]\tLoss: 101.518158\n",
      "Train Epoch: 48 [38400/60000 (64%)]\tLoss: 101.379028\n",
      "Train Epoch: 48 [51200/60000 (85%)]\tLoss: 103.348068\n",
      "Train Epoch: 49 [0/60000 (0%)]\tLoss: 97.630508\n",
      "Train Epoch: 49 [12800/60000 (21%)]\tLoss: 104.533501\n",
      "Train Epoch: 49 [25600/60000 (43%)]\tLoss: 99.768440\n",
      "Train Epoch: 49 [38400/60000 (64%)]\tLoss: 102.189354\n",
      "Train Epoch: 49 [51200/60000 (85%)]\tLoss: 101.429382\n",
      "Train Epoch: 50 [0/60000 (0%)]\tLoss: 99.920479\n",
      "Train Epoch: 50 [12800/60000 (21%)]\tLoss: 99.153496\n",
      "Train Epoch: 50 [25600/60000 (43%)]\tLoss: 100.171036\n",
      "Train Epoch: 50 [38400/60000 (64%)]\tLoss: 101.639099\n",
      "Train Epoch: 50 [51200/60000 (85%)]\tLoss: 104.755150\n"
     ]
    }
   ],
   "source": [
    "# Train the VAE for several epochs\n",
    "for epoch in range(1, 51):\n",
    "    train(model, train_loader, optimizer, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "test_set = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "test_loader = DataLoader(test_set, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss: 12893.700\n",
      "Standard deviation of loss: 1305.604\n"
     ]
    }
   ],
   "source": [
    "# assume that the trained VAE model is named 'vae_model'\n",
    "# and the new dataset is loaded into a DataLoader named 'new_data_loader'\n",
    "\n",
    "model.eval()\n",
    "device = torch.device(\"cpu\")\n",
    "reconstruction_loss = nn.MSELoss(reduction='none')\n",
    "\n",
    "total_loss = []\n",
    "for batch_idx, (data, _) in enumerate(test_loader):\n",
    "    data = data.to(device)\n",
    "    recon_batch, mu, logvar = model(data)\n",
    "    \n",
    "    # compute reconstruction loss\n",
    "    recon_loss =  loss_function(recon_batch, data, mu, logvar)\n",
    "    \n",
    "\n",
    "    loss = (recon_loss).item()\n",
    "    total_loss.append(loss)\n",
    "\n",
    "mean_loss = np.mean(total_loss)\n",
    "std_loss = np.std(total_loss)\n",
    "\n",
    "print(\"Mean loss: {:.3f}\".format(mean_loss))\n",
    "print(\"Standard deviation of loss: {:.3f}\".format(std_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss: 1237.653\n"
     ]
    }
   ],
   "source": [
    "# Evaluating on single image\n",
    "\n",
    "\n",
    "model.eval()\n",
    "image,_ = test_set.__getitem__(43)\n",
    "with torch.no_grad():\n",
    "    image = image.to(device)\n",
    "    recon_image, mu, logvar = model(image.unsqueeze(0))\n",
    "    \n",
    "    # compute reconstruction loss\n",
    "    \n",
    "    recon_loss = loss_function(recon_batch, data, mu, logvar)\n",
    "\n",
    "    \n",
    "print(\"Total loss: {:.3f}\".format(recon_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# convert the tensors to numpy arrays and reshape them into images\n",
    "image = np.reshape(image.numpy(), (28, 28))\n",
    "recon_image = np.reshape(recon_image.numpy(), (28, 28))\n",
    "\n",
    "# display the original image and the reconstructed image side-by-side\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(8, 4))\n",
    "axes[0].imshow(image, cmap='gray')\n",
    "axes[0].set_title(\"Original\")\n",
    "axes[1].imshow(recon_image, cmap='gray')\n",
    "axes[1].set_title(\"Reconstructed\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Image recognition.\n",
    "- Image recognition is obtained by returning the probability using softmax layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 400)\n",
    "        self.fc21 = nn.Linear(400, 10)\n",
    "        self.fc22 = nn.Linear(400, 10)\n",
    "        self.fc3 = nn.Linear(20, 400)\n",
    "        self.fc4 = nn.Linear(128, 10)  # Classifier\n",
    "        self.fc5 = nn.Linear(400, 784)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps * std + mu\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = F.relu(self.fc3(z))\n",
    "        return torch.sigmoid(self.fc5(h3))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x.view(-1, 784))\n",
    "        print(np.shape(mu), np.shape(logvar))\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        print(np.shape(self.fc4(z.T)))\n",
    "        #recon_x = self.decode(z)\n",
    "        class_probs = F.log_softmax(self.fc4(z), dim=1)\n",
    "        return class_probs, mu, logvar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the loss function and optimizer\n",
    "vae_model = VAE()\n",
    "recon_loss = nn.BCELoss(reduction='sum') # for reconstrction\n",
    "class_loss = nn.CrossEntropyLoss() # for classification loss\n",
    "optimizer = optim.Adam(vae_model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 10]) torch.Size([128, 10])\n",
      "torch.Size([10, 10])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (128x10 and 128x10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_60770/4155570765.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mrecon_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogvar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvae_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mrecon_loss_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrecon_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecon_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m784\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mclass_loss_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclass_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecon_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch_dl/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_60770/645468446.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;31m#recon_x = self.decode(z)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mclass_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mclass_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogvar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch_dl/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch_dl/lib/python3.8/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (128x10 and 128x10)"
     ]
    }
   ],
   "source": [
    "# train the VAE\n",
    "vae_model.train()\n",
    "for epoch in range(10):\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = vae_model(data)\n",
    "        recon_loss_val = recon_loss(recon_batch, data.view(-1, 784))\n",
    "        class_loss_val = class_loss(recon_batch, target)\n",
    "        loss = recon_loss_val + class_loss_val\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader),\n",
    "                loss.item() / len(data)))\n",
    "\n",
    "# evaluate the VAE\n",
    "vae_model.eval()\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        recon_batch, mu, logvar = vae_model(data)\n",
    "        test_loss += recon_loss(recon_batch, data.view(-1, 784)).item()\n",
    "        pred = recon_batch.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "test_loss /= len(test_loader.dataset)\n",
    "accuracy = 100. * correct / len(test_loader.dataset)\n",
    "print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n",
    "    test_loss, correct, len(test_loader.dataset), accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
