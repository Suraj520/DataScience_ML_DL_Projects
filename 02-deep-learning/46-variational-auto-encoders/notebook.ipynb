{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### About\n",
    "> VAE\n",
    "- Variational Autoencoder (VAE) is a type of artificial neural network used for unsupervised learning that can generate new data based on the input data. It is an extension of the basic autoencoder (AE) architecture that is used to learn a compressed representation of the input data.\n",
    "- The key difference between VAEs and AEs is that VAEs are probabilistic models that learn the probability distribution of the input data, while AEs learn the deterministic mapping between the input and the output. VAEs also have an encoder-decoder architecture, but they are trained to optimize a lower bound on the likelihood of the data given a prior distribution, typically a Gaussian distribution, in the latent space. The resulting compressed representation is not only learned to be useful for reconstruction but also to generate new data points in the latent space.\n",
    "- The main use case of VAEs is in generative modeling, where they can be used to generate new data points that are similar to the training data. This is particularly useful in domains where data is scarce, and it can be challenging to collect large amounts of labeled data for supervised learning.\n",
    "- The KL divergence is often used as a regularization term in variational autoencoders (VAEs) to encourage the learned latent space to follow a specific prior distribution, typically a Gaussian distribution. In this case, the KL divergence measures the distance between the learned latent distribution and the desired prior distribution.\n",
    "- Kullback-Leibler (KL) divergence is a measure of the difference between two probability distributions. It is often used in information theory and statistics to compare a model distribution to an actual distribution. KL divergence measures the amount of information lost when one distribution is used to approximate another distribution.\n",
    "- The VAE architecture takes an input image, maps it to a lower-dimensional latent space, samples a latent variable from a learned distribution in that space, and then maps the sampled variable back to the original input space to generate a reconstructed image. The output of the VAE is not just the reconstructed image but also the mean and variance of the learned latent distribution, which are used to compute the KL divergence term in the loss function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
