{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#### About \n",
        "\n",
        "- LAPGAN is an abbreviation for \"Laplacian Pyramid Generative Adversarial Networks.\" It is a generative adversarial network (GAN) used to generate high-resolution images.\n",
        "\n",
        "- The main idea behind LAPGAN is to generate images in a step-by-step manner using a Laplacian pyramid. A Laplacian pyramid is a multiresolution image representation in which each level corresponds to a different scale of the image. The generator network in LAPGAN is trained to produce images at each level of the Laplacian pyramid, beginning with the coarsest and gradually increasing the resolution.\n",
        "\n",
        "- LAPGAN uses a GAN with a modified loss function to generate an image at a given level of the Laplacian pyramid. This encourages the generated image to have the same Laplacian pyramid."
      ],
      "metadata": {
        "id": "34C1kDbSFTeG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#importing modules\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as transforms\n",
        "from torch.autograd import Variable\n",
        "import torchvision.utils as vutils\n",
        "import torchvision"
      ],
      "metadata": {
        "id": "oDhhUYeTFUDf"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load FashionMNIST dataset\n",
        "batch_size=2048\n",
        "dataset = dset.FashionMNIST(root='./data', download=True,\n",
        "                            transform=transforms.Compose([\n",
        "                                transforms.Resize(64),\n",
        "                                transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.5,), (0.5,))\n",
        "                            ]))\n",
        "\n",
        "# Create dataloader\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
        "                                         shuffle=True, num_workers=2)"
      ],
      "metadata": {
        "id": "BQeolbH9Oaem"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define generator network, discriminator and laplacian network\n",
        "# Define generator model\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "        \n",
        "        self.main = nn.Sequential(\n",
        "            # Initial layer\n",
        "            nn.ConvTranspose2d(100, 512, 4, 1, 0, bias=False),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(True),\n",
        "            \n",
        "            # Layer 2\n",
        "            nn.ConvTranspose2d(512, 256, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(True),\n",
        "            \n",
        "            # Layer 3\n",
        "            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(True),\n",
        "            \n",
        "            # Layer 4\n",
        "            nn.ConvTranspose2d(128, 64, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            # Layer 5\n",
        "            nn.ConvTranspose2d(64, 1, 4, 2, 1, bias=False),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        output = self.main(input)\n",
        "        #print(\"Here2\",output.shape)\n",
        "        return output\n",
        "\n",
        "# Define discriminator model\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=64, kernel_size=3, stride=2, padding=1)\n",
        "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=2, padding=1)\n",
        "        self.conv3 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=2, padding=1)\n",
        "        self.conv4 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=2, padding=1)\n",
        "        self.fc = nn.Linear(512*4*4, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = nn.functional.leaky_relu(self.conv1(x), negative_slope=0.2)\n",
        "        #print(x.shape)\n",
        "        x = nn.functional.leaky_relu(self.conv2(x), negative_slope=0.2)\n",
        "        #print(x.shape)\n",
        "        x = nn.functional.leaky_relu(self.conv3(x), negative_slope=0.2)\n",
        "        #print(x.shape)\n",
        "        x = nn.functional.leaky_relu(self.conv4(x), negative_slope=0.2)\n",
        "        #print(x.shape)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        #print(x.shape)\n",
        "        x = torch.sigmoid(self.fc(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "# Define LaplacianPyramid class\n",
        "class LaplacianPyramid(nn.Module):\n",
        "    def __init__(self, num_levels):\n",
        "        super(LaplacianPyramid, self).__init__()\n",
        "        self.num_levels = num_levels\n",
        "        self.downsample = nn.AvgPool2d(kernel_size=2, stride=2, count_include_pad=False)\n",
        "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # Create list to hold pyramid levels\n",
        "        pyramid = []\n",
        "        # Add original image to pyramid\n",
        "        pyramid.append(x)\n",
        "        # Generate additional levels\n",
        "        for i in range(self.num_levels - 1):\n",
        "            x = self.downsample(x)\n",
        "            pyramid.append(x)\n",
        "        # Reconstruct original image and store residuals\n",
        "        for i in range(self.num_levels - 1, 0, -1):\n",
        "            x_up = self.upsample(pyramid[i])\n",
        "            x_down = pyramid[i - 1]\n",
        "            h, w = x_down.size()[2:]\n",
        "            x_up = x_up[:, :, :h, :w]\n",
        "            diff = x_down - x_up\n",
        "            pyramid[i - 1] = diff\n",
        "        return pyramid       \n"
      ],
      "metadata": {
        "id": "q9DnUgK1FaPK"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_pyramid_levels=3\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# Define fixed noise for testing generator during training\n",
        "fixed_noise = torch.randn(64, 100, 1, 1)\n",
        "generator = Generator().to(device)\n",
        "discriminator = Discriminator().to(device)\n",
        "pyramid = LaplacianPyramid(num_pyramid_levels).to(device)\n",
        "fixed_noise = fixed_noise.to(device)"
      ],
      "metadata": {
        "id": "kGQb3CYjF87v"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#loss function and optimizers\n",
        "# Define loss function and optimizers\n",
        "learning_rate=0.0002\n",
        "criterion = nn.BCELoss()\n",
        "optimizer_g = optim.Adam(generator.parameters(), lr=learning_rate, betas=(0.5, 0.999))\n",
        "optimizer_d = optim.Adam(discriminator.parameters(), lr=learning_rate, betas=(0.5, 0.999))\n",
        "num_epochs=50\n"
      ],
      "metadata": {
        "id": "qQ-2sAUTGERH"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf generated_images\n",
        "!mkdir generated_images\n",
        "!rm -rf discriminator_epoch*.*\n",
        "!rm -rf generator_epoch*.*"
      ],
      "metadata": {
        "id": "pzqhfXCsOrcW"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g0MfuhibdpAH"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(num_epochs):\n",
        "    for i, data in enumerate(dataloader, 0):\n",
        "        # Update discriminator\n",
        "        discriminator.zero_grad()\n",
        "        real, _ = data\n",
        "        #print(real.shape)\n",
        "        batch_size = real.size(0)\n",
        "        real = real.to(device)\n",
        "        input_real = Variable(real)\n",
        "        target_real = Variable(torch.ones(batch_size))\n",
        "  \n",
        "        target_real = target_real.to(device)\n",
        "        #print(input_real.shape)\n",
        "        output_real = discriminator(input_real)\n",
        "        output_real = output_real.squeeze(1)\n",
        "        error_real = criterion(output_real, target_real)\n",
        "        error_real.backward()\n",
        "\n",
        "        noise = torch.randn(batch_size, 100, 1, 1)\n",
        "        \n",
        "        noise = noise.to(device)\n",
        "        input_fake = Variable(generator(noise).data)\n",
        "        target_fake = Variable(torch.zeros(batch_size))\n",
        "        \n",
        "        target_fake = target_fake.to(device)\n",
        "        #print(\"Here\",input_fake.shape)\n",
        "        output_fake = discriminator(input_fake)\n",
        "        output_fake = output_fake.squeeze(1)\n",
        "        error_fake = criterion(output_fake, target_fake)\n",
        "        error_fake.backward()\n",
        "\n",
        "        optimizer_d.step()\n",
        "\n",
        "        # Update generator\n",
        "        generator.zero_grad()\n",
        "        noise = torch.randn(batch_size, 100, 1, 1)\n",
        "        \n",
        "        noise = noise.to(device)\n",
        "        input_fake = generator(noise)\n",
        "        output_fake = discriminator(input_fake)\n",
        "        target_real = Variable(torch.ones(batch_size))\n",
        "        \n",
        "        target_real = target_real.to(device)\n",
        "        pyramid_fake = pyramid(input_fake)\n",
        "        pyramid_real = pyramid(input_real)\n",
        "        loss = 0\n",
        "        for j in range(num_pyramid_levels):\n",
        "            loss += criterion(torch.sigmoid(pyramid_fake[j]), pyramid_real[j].detach())\n",
        "        loss.backward()\n",
        "        optimizer_g.step()\n",
        "\n",
        "        # Print loss and save generated images periodically\n",
        "        if i % 10 == 0:\n",
        "            print('[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f' % (epoch, num_epochs, i, len(dataloader), error_real + error_fake, loss))\n",
        "            #save models\n",
        "            torch.save(generator.state_dict(), f\"generator_epoch_{epoch}.pth\")\n",
        "            torch.save(discriminator.state_dict(), f\"discriminator_epoch_{epoch}.pth\")\n",
        "\n",
        "            # Generate images from fixed noise\n",
        "            generator.eval()\n",
        "            with torch.no_grad():\n",
        "                fixed_fake = generator(fixed_noise)\n",
        "                pyramid_fake = pyramid(fixed_fake)\n",
        "                pyramid_real = pyramid(input_real[:64])\n",
        "            generator.train()\n",
        "            # Save images\n",
        "            vutils.save_image(fixed_fake.data[:64], 'generated_images/fake_samples_epoch_%03d.png' % epoch, normalize=True)\n",
        "            vutils.save_image(input_real[:64], 'generated_images/real_samples.png', normalize=True)\n",
        "            for j in range(num_pyramid_levels):\n",
        "                vutils.save_image(pyramid_real[j].data[:64], 'generated_images/real_pyramid_%d.png' % j, normalize=True)\n",
        "                vutils.save_image(pyramid_fake[j].data[:64], 'generated_images/fake_pyramid_%d_epoch_%03d.png' % (j, epoch), normalize=True)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hElmEMsCGvvQ",
        "outputId": "26f8e2c8-9f99-4571-8ba7-b2aa705e0f4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0/50][0/30] Loss_D: 0.0000 Loss_G: 0.5850\n",
            "[0/50][10/30] Loss_D: 0.0000 Loss_G: 0.5945\n",
            "[0/50][20/30] Loss_D: 0.0000 Loss_G: 0.5953\n",
            "[1/50][0/30] Loss_D: 0.0000 Loss_G: 0.6010\n",
            "[1/50][10/30] Loss_D: 0.0000 Loss_G: 0.5928\n",
            "[1/50][20/30] Loss_D: 0.0000 Loss_G: 0.5630\n",
            "[2/50][0/30] Loss_D: 0.0000 Loss_G: 0.5766\n",
            "[2/50][10/30] Loss_D: 0.0000 Loss_G: 0.5546\n",
            "[2/50][20/30] Loss_D: 0.0000 Loss_G: 0.5508\n",
            "[3/50][0/30] Loss_D: 0.0000 Loss_G: 0.5693\n",
            "[3/50][10/30] Loss_D: 0.0000 Loss_G: 0.5523\n",
            "[3/50][20/30] Loss_D: 0.0000 Loss_G: 0.5680\n",
            "[4/50][0/30] Loss_D: 0.0000 Loss_G: 0.5520\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mg-RqKfyOweE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}