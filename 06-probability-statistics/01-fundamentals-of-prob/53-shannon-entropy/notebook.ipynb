{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### About\n",
    "> Shannon Entropy\n",
    "\n",
    "Shannon entropy is a measure of the uncertainty or unpredictability of information content. It was introduced by Claude Shannon in 1948 in his paper \"A Mathematical Theory of Communication.\" The Shannon entropy of a message is calculated based on the probability of each possible symbol in the message, and it provides an upper bound on the average length of a lossless compression algorithm that can be used to represent the message.\n",
    "\n",
    "The Shannon entropy of a discrete random variable X with n possible outcomes and probability distribution p(X) is given by:\n",
    "\n",
    "H(X) = - âˆ‘ p(X) log2 p(X)\n",
    "\n",
    "where log2 is the base-2 logarithm.\n",
    "\n",
    "Use cases\n",
    "\n",
    "Information theory: Shannon entropy is a fundamental concept in information theory, and it is used to study the properties of communication channels and encoding schemes.\n",
    "\n",
    "Cryptography: Shannon entropy is used to measure the strength of encryption keys and to generate random numbers.\n",
    "\n",
    "Machine learning: Shannon entropy can be used as a measure of the purity of a node in a decision tree or the amount of information gained by splitting a node."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example\n",
    "\n",
    "Suppose we have a sequence of symbols: \"ABACADABRA\". We can calculate the Shannon entropy of this sequence as follows:\n",
    "\n",
    "Count the number of occurrences of each symbol:\n",
    "A: 5\n",
    "B: 2\n",
    "C: 1\n",
    "D: 1\n",
    "R: 2\n",
    "\n",
    "Calculate the probability of each symbol by dividing the count by the total number of symbols:\n",
    "P(A) = 5/11 = 0.45\n",
    "P(B) = 2/11 = 0.18\n",
    "P(C) = 1/11 = 0.09\n",
    "P(D) = 1/11 = 0.09\n",
    "P(R) = 2/11 = 0.18\n",
    "\n",
    "Calculate the Shannon entropy as the sum of the products of each symbol's probability and its logarithm (base 2):\n",
    "H(X) = - (0.45log2(0.45) + 0.18log2(0.18) + 0.09log2(0.09) + 0.09log2(0.09) + 0.18*log2(0.18))\n",
    "= 2.334\n",
    "\n",
    "Therefore, the Shannon entropy of the sequence \"ABACADABRA\" is 2.334 bits per symbol. This means that on average, it takes at least 2.334 bits to represent each symbol in the sequence using a lossless compression algorithm."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
