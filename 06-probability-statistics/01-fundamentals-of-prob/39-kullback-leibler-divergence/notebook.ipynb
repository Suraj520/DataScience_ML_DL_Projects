{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### About\n",
    "> Kullback Leibler Divergence\n",
    "\n",
    "Kullback-Leibler (KL) divergence is a measure of how different two probability distributions are from each other. It measures the amount of information lost when approximating one probability distribution with another.\n",
    "\n",
    "The KL divergence between two probability distributions P and Q is defined as:\n",
    "\n",
    "$D_{KL}(P||Q) = \\sum_{i} P(i) \\log\\frac{P(i)}{Q(i)}$\n",
    "\n",
    "The KL divergence is non-negative, and it is equal to zero if and only if P and Q are identical.\n",
    "\n",
    "Use cases -\n",
    "\n",
    "1. Model selection: KL divergence can be used to compare the performance of different models. For example, if we have two models that predict the same output, we can use KL divergence to measure the difference between the distributions of the predicted values.\n",
    "\n",
    "2. Feature selection: KL divergence can be used to measure the information gain of adding a new feature to a model. For example, if we have a classification problem and we want to add a new feature to our model, we can use KL divergence to measure the difference between the class distributions with and without the new feature.\n",
    "\n",
    "3. Optimization: KL divergence can be used as a loss function in optimization problems. For example, in some unsupervised learning problems, we want to find a distribution that is similar to the data distribution. We can use KL divergence to measure the difference between the data distribution and the model distribution and minimize it using gradient descent.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have two probability distributions P and Q represented by the following arrays:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "P = np.array([0.3, 0.2, 0.5])\n",
    "Q = np.array([0.4, 0.1, 0.5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05232481437645474\n"
     ]
    }
   ],
   "source": [
    "def kl_divergence(P, Q):\n",
    "    return np.sum(P * np.log(P / Q))\n",
    "\n",
    "print(kl_divergence(P, Q))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KL divergence between P and Q is 0.05232481437645474, which indicates that the two distributions are very similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
